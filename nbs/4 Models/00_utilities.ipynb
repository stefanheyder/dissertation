{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities for the models of this thesis\n",
    "> comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp models.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from isssm.typing import PGSSM\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.cm as cm\n",
    "import jax.numpy as jnp\n",
    "from jaxtyping import Float, Array\n",
    "import fastcore.test as fct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def __zero_to_nan(arr, eps=1e-10):\n",
    "    return jnp.where(jnp.abs(arr) < eps, jnp.nan, arr)\n",
    "\n",
    "\n",
    "def visualize_pgssm(pgssm: PGSSM):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3)\n",
    "    cmap = cm.get_cmap(\"viridis\")\n",
    "\n",
    "    A, B, D, Sigma = pgssm.A[0], pgssm.B[0], pgssm.D[0], pgssm.Sigma[0]\n",
    "    max = jnp.max(jnp.array([A.max(), B.max(), D.max()]))\n",
    "    min = jnp.min(jnp.array([A.min(), B.min(), D.min()]))\n",
    "\n",
    "    normalizer = Normalize(min, max)\n",
    "    im = cm.ScalarMappable(norm=normalizer)\n",
    "    axes[0].imshow(__zero_to_nan(A), cmap=cmap, norm=normalizer)\n",
    "    axes[0].set_title(\"A\")\n",
    "    axes[1].imshow(__zero_to_nan(B), cmap=cmap, norm=normalizer)\n",
    "    axes[1].set_title(\"B\")\n",
    "    axes[2].imshow(__zero_to_nan(D), cmap=cmap, norm=normalizer)\n",
    "    axes[2].set_title(\"D\")\n",
    "\n",
    "    fig.colorbar(im, ax=axes.ravel().tolist())\n",
    "    plt.show()\n",
    "\n",
    "    plt.imshow(__zero_to_nan(Sigma))\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $p \\in \\mathbf R^{k}_{>0}$ with $\\sum_{i = 1}^k p_{i} = 1$, let $\\log q_i = \\log \\frac{p_{i}}{p_{k}}$ for $i = 1, \\dots, k -1$. Then \n",
    "$$\n",
    "    p_{k} = \\frac{1}{1 + \\sum_{i = 1}^{k-1}q_{i}},\n",
    "$$\n",
    "so \n",
    "$$\n",
    "    p_{i} = q_{i} p_{k} = \\frac{q_{i}}{1 + \\sum_{i = 1}^{k-1}q_{i}}.\n",
    "\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def to_log_probs(log_ratios: Float[Array, \"k-1\"]) -> Float[Array, \"k\"]:\n",
    "    exp_q = jnp.exp(log_ratios)\n",
    "    p_n_delay = 1 / (1 + exp_q.sum(axis=-1, keepdims=True))\n",
    "    log_p = jnp.log(jnp.concatenate([exp_q * p_n_delay, p_n_delay], axis=-1))\n",
    "    return log_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "fct.test_close(to_log_probs(jnp.zeros(4)), jnp.log(jnp.ones(5) / 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monkey-Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import isssm.importance_sampling\n",
    "\n",
    "\n",
    "def patch_la():\n",
    "    isssm.importance_sampling.log_weights = print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "patch_la()\n",
    "from isssm.importance_sampling import log_weights\n",
    "\n",
    "log_weights(\"10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from isssm.util import MVN_degenerate as MVN\n",
    "\n",
    "def log_weights_t_full_deps(\n",
    "    s_t: Float[Array, \"p\"],  # signal\n",
    "    y_t: Float[Array, \"p\"],  # observation\n",
    "    xi_t: Float[Array, \"p\"],  # parameters\n",
    "    dist,  # observation distribution\n",
    "    z_t: Float[Array, \"p\"],  # synthetic observation\n",
    "    Omega_t: Float[Array, \"p p\"],  # synthetic observation covariance, assumed diagonal\n",
    ") -> Float:  # single log weight\n",
    "    \"\"\"Log weight for a single time point.\"\"\"\n",
    "    p_ys = dist(s_t, xi_t).log_prob(y_t).sum()\n",
    "\n",
    "    # omega_t = jnp.sqrt(jnp.diag(Omega_t))\n",
    "    # g_zs = MVN_diag(s_t, omega_t).log_prob(z_t).sum()\n",
    "    g_zs = jnp.nansum(MVN(s_t, Omega_t).log_prob(z_t))\n",
    "\n",
    "    return p_ys - g_zs\n",
    "\n",
    "\n",
    "def log_weights_full_deps(\n",
    "    s: Float[Array, \"n+1 p\"],  # signals\n",
    "    y: Float[Array, \"n+1 p\"],  # observations\n",
    "    dist,  # observation distribution\n",
    "    xi: Float[Array, \"n+1 p\"],  # observation parameters\n",
    "    z: Float[Array, \"n+1 p\"],  # synthetic observations\n",
    "    Omega: Float[Array, \"n+1 p p\"],  # synthetic observation covariances:\n",
    ") -> Float:  # log weights\n",
    "    \"\"\"Log weights for all time points\"\"\"\n",
    "    p_ys = dist(s, xi).log_prob(y).sum()\n",
    "\n",
    "    # avoid triangular solve problems\n",
    "    # omega = jnp.sqrt(vmap(jnp.diag)(Omega))\n",
    "    # g_zs = MVN_diag(s, omega).log_prob(z).sum()\n",
    "    g_zs = jnp.nansum(MVN(s, Omega).log_prob(z))\n",
    "\n",
    "    return p_ys - g_zs\n",
    "\n",
    "def patch_log_weights_full_deps():\n",
    "    isssm.importance_sampling.log_weights = log_weights_full_deps\n",
    "    isssm.importance_sampling.log_weights_t = log_weights_t_full_deps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# LA monkey patch\n",
    "from isssm.kalman import kalman, smoothed_signals\n",
    "from isssm.typing import GLSSM, GLSSMProposal, ConvergenceInformation\n",
    "from isssm.laplace_approximation import default_link, vvmap, vdiag\n",
    "from jax.scipy.optimize import minimize\n",
    "from functools import partial\n",
    "from isssm.util import converged\n",
    "from jax import jacfwd, hessian, jacrev\n",
    "from jax.lax import while_loop\n",
    "import isssm.laplace_approximation\n",
    "\n",
    "\n",
    "def _initial_guess_full_deps(xi_t, y_t, dist, link=default_link):\n",
    "    result = minimize(\n",
    "        lambda s_ti: -dist(s_ti, xi_t).log_prob(y_t).sum(),\n",
    "        jnp.concatenate((jnp.atleast_1d(link(y_t.sum())), jnp.zeros(3))),\n",
    "        method=\"BFGS\",\n",
    "    )\n",
    "    return jnp.squeeze(result.x)\n",
    "\n",
    "\n",
    "def laplace_approximation_full_deps(\n",
    "    y: Float[Array, \"n+1 p\"],  # observation\n",
    "    model: PGSSM,\n",
    "    n_iter: int,  # number of iterations\n",
    "    log_lik=None,  # log likelihood function\n",
    "    d_log_lik=None,  # derivative of log likelihood function\n",
    "    dd_log_lik=None,  # second derivative of log likelihood function\n",
    "    eps: Float = 1e-5,  # precision of iterations\n",
    "    link=default_link,  # default link to use in initial guess\n",
    ") -> tuple[GLSSMProposal, ConvergenceInformation]:\n",
    "    u, A, D, Sigma0, Sigma, v, B, dist, xi = model\n",
    "    np1, p, m = B.shape\n",
    "\n",
    "    # s_init = vmap(partial(_initial_guess_full_deps, dist=dist, link=link))(xi, y)\n",
    "    # missing values have zero obs. -> 0.\n",
    "    s_init = jnp.hstack(\n",
    "        (jnp.log(y.sum(axis=1, keepdims=True) + 1.0), jnp.zeros((np1, 3)))\n",
    "    )\n",
    "\n",
    "    def default_log_lik(\n",
    "        s_t: Float[Array, \"p\"], xi_t: Float[Array, \"p\"], y_t: Float[Array, \"p'\"]\n",
    "    ):\n",
    "        return dist(s_t, xi_t).log_prob(y_t).sum()\n",
    "\n",
    "    if log_lik is None:\n",
    "        log_lik = default_log_lik\n",
    "\n",
    "    if d_log_lik is None:\n",
    "        d_log_lik = jacfwd(log_lik, argnums=0)\n",
    "    if dd_log_lik is None:\n",
    "        dd_log_lik = jacrev(d_log_lik, argnums=0)\n",
    "\n",
    "    vd_log_lik = jit(vmap(d_log_lik))\n",
    "    vdd_log_lik = jit(vmap(dd_log_lik))\n",
    "\n",
    "    def _break(val):\n",
    "        _, i, z, Omega, z_old, Omega_old = val\n",
    "\n",
    "        z_converged = jnp.logical_and(converged(z, z_old, eps), i > 0)\n",
    "        Omega_converged = jnp.logical_and(converged(Omega, Omega_old, eps), i > 0)\n",
    "        all_converged = jnp.logical_and(z_converged, Omega_converged)\n",
    "\n",
    "        iteration_limit_reached = i >= n_iter\n",
    "\n",
    "        return jnp.logical_or(all_converged, iteration_limit_reached)\n",
    "\n",
    "    def _iteration(val):\n",
    "        s, i, z_old, Omega_old, _, _ = val\n",
    "\n",
    "        grad = vd_log_lik(s, xi, y)\n",
    "        Gamma = -vdd_log_lik(s, xi, y)\n",
    "        # pinv for missing values: if Gamma is 0, we want Omega to be 0 as well\n",
    "        # requries exact derivatives, not numerical ones!\n",
    "        Omega = jnp.linalg.pinv(Gamma, hermitian=True)\n",
    "\n",
    "        # z = s + jnp.linalg.solve(Gamma, grad[..., None])[..., 0]\n",
    "        z = s + (Omega @ grad[..., None])[..., 0]\n",
    "        approx_glssm = GLSSM(u, A, D, Sigma0, Sigma, v, B, Omega)\n",
    "\n",
    "        filtered = kalman(z, approx_glssm)\n",
    "        s_new = smoothed_signals(filtered, z, approx_glssm)\n",
    "\n",
    "        return s_new, i + 1, z, Omega, z_old, Omega_old\n",
    "\n",
    "    empty_z = jnp.empty_like(s_init)\n",
    "    empty_Omega = jnp.empty((np1, p, p))\n",
    "    init = (s_init, 0, empty_z, empty_Omega, empty_z, empty_Omega)\n",
    "\n",
    "    _keep_going = lambda *args: jnp.logical_not(_break(*args))\n",
    "    _, n_iters, z, Omega, z_old, Omega_old = while_loop(_keep_going, _iteration, init)\n",
    "\n",
    "    final_proposal = GLSSMProposal(u, A, D, Sigma0, Sigma, v, B, Omega, z)\n",
    "    delta_z = jnp.max(jnp.abs(z - z_old))\n",
    "    delta_Omega = jnp.max(jnp.abs(Omega - Omega_old))\n",
    "    information = ConvergenceInformation(\n",
    "        converged=jnp.logical_and(\n",
    "            converged(z, z_old, eps), converged(Omega, Omega_old, eps)\n",
    "        ),\n",
    "        n_iter=n_iters,\n",
    "        delta=jnp.max(jnp.array([delta_z, delta_Omega])),\n",
    "    )\n",
    "    return final_proposal, information\n",
    "\n",
    "\n",
    "def patch_la_full_deps():\n",
    "    isssm.laplace_approximation._initial_guess = _initial_guess_full_deps\n",
    "    isssm.laplace_approximation.laplace_approximation = laplace_approximation_full_deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# MEIS monkey patch\n",
    "from jax import jit, vmap\n",
    "import jax.random as jrn\n",
    "from jaxtyping import PRNGKeyArray\n",
    "from isssm.importance_sampling import normalize_weights, log_weights_t\n",
    "from isssm.kalman import simulation_smoother\n",
    "import isssm.modified_efficient_importance_sampling\n",
    "\n",
    "\n",
    "def triu(entries: Float[Array, \" q\"], p: int):\n",
    "    (q,) = entries.shape\n",
    "\n",
    "    if 2 * q != p * (p + 1):\n",
    "        raise ValueError(\n",
    "            f\"entries does not have the correct shape, expected {int(p * (p - 1) /2)} entries in the second dimension but got {q}\"\n",
    "        )\n",
    "\n",
    "    mat = jnp.zeros((p, p))\n",
    "    mat = mat.at[jnp.triu_indices(p)].set(entries)\n",
    "\n",
    "    return mat\n",
    "\n",
    "\n",
    "def tril(entries: Float[Array, \" q\"], p: int):\n",
    "    (q,) = entries.shape\n",
    "\n",
    "    if 2 * q != p * (p + 1):\n",
    "        raise ValueError(\n",
    "            f\"entries does not have the correct shape, expected {int(p * (p - 1) /2)} entries in the second dimension but got {q}\"\n",
    "        )\n",
    "\n",
    "    mat = jnp.zeros((p, p))\n",
    "    mat = mat.at[jnp.tril_indices(p)].set(entries)\n",
    "\n",
    "    return mat\n",
    "\n",
    "\n",
    "@jit\n",
    "def optimal_parameters(\n",
    "    signal: Float[Array, \"N p\"], weights: Float[Array, \"N\"], log_p: Float[Array, \"N\"]\n",
    "):\n",
    "    N, p = signal.shape\n",
    "    ones = jnp.ones_like(weights)[:, None]\n",
    "\n",
    "    second_moment = vmap(jnp.outer)(signal, signal)\n",
    "    # double tranpose, because [:, jnp.tril_indices(p)] is not supported (returns (N, 2, p(p-1)/2, 4) instead of (N, p(p-1)/2)\n",
    "    lower_tri_second_moment = second_moment.transpose((1, 2, 0))[jnp.tril_indices(p)].T\n",
    "\n",
    "    # w_inner_prod = lambda a, b: jnp.einsum(\"i,ij,ik->jk\", weights, a, b)\n",
    "    # X_T_W_X = jnp.block(\n",
    "    #    [\n",
    "    #        [\n",
    "    #            w_inner_prod(ones, ones),\n",
    "    #            w_inner_prod(ones, signal),\n",
    "    #            w_inner_prod(ones, -0.5 * lower_tri_second_moment),\n",
    "    #        ],\n",
    "    #        [\n",
    "    #            w_inner_prod(signal, ones),\n",
    "    #            w_inner_prod(signal, signal),\n",
    "    #            w_inner_prod(signal, -0.5 * lower_tri_second_moment),\n",
    "    #        ],\n",
    "    #        [\n",
    "    #            w_inner_prod(-0.5 * lower_tri_second_moment, ones),\n",
    "    #            w_inner_prod(-0.5 * lower_tri_second_moment, signal),\n",
    "    #            w_inner_prod(\n",
    "    #                -0.5 * lower_tri_second_moment, -0.5 * lower_tri_second_moment\n",
    "    #            ),\n",
    "    #        ],\n",
    "    #    ]\n",
    "    # )\n",
    "    # X_T_W_y = jnp.concatenate(\n",
    "    #    [\n",
    "    #        w_inner_prod(ones, log_p[:, None]),\n",
    "    #        w_inner_prod(signal, log_p[:, None]),\n",
    "    #        w_inner_prod(-0.5 * lower_tri_second_moment, log_p[:, None]),\n",
    "    #    ]\n",
    "    # )\n",
    "\n",
    "    design = jnp.hstack([ones, signal, -0.5 * lower_tri_second_moment])\n",
    "    lhs = log_p\n",
    "\n",
    "    sqrt_weights = jnp.sqrt(weights)\n",
    "\n",
    "    design_W = design * sqrt_weights[:, None]\n",
    "    lhs_W = lhs * sqrt_weights\n",
    "\n",
    "    # if lhs_W is 0, this returns 0 even if design matrix does not have full rank (numpy promises solution with least l2-norm)\n",
    "    beta, *_ = jnp.linalg.lstsq(design_W, lhs_W)\n",
    "    return beta\n",
    "    # beta = jnp.linalg.solve(X_T_W_X, X_T_W_y[:, 0])\n",
    "    # return beta\n",
    "\n",
    "\n",
    "def MEIS_full_deps(\n",
    "    y: Float[Array, \"n+1 p\"],  # observations\n",
    "    model: PGSSM,  # model\n",
    "    z_init: Float[Array, \"n+1 p\"],  # initial z estimate\n",
    "    Omega_init: Float[Array, \"n+1 p p\"],  # initial Omega estimate\n",
    "    n_iter: int,  # number of iterations\n",
    "    N: int,  # number of samples\n",
    "    key: PRNGKeyArray,  # random key\n",
    "    eps: Float = 1e-5,  # convergence threshold\n",
    "):\n",
    "    z, Omega = z_init, Omega_init\n",
    "\n",
    "    np1, p, m = model.B.shape\n",
    "\n",
    "    key, crn_key = jrn.split(key)\n",
    "\n",
    "    v_norm_w = vmap(normalize_weights)\n",
    "    dist = model.dist\n",
    "    lw_t = vmap(\n",
    "        vmap(lambda s, y, xi, z, Omega: log_weights_t(s, y, xi, dist, z, Omega)),\n",
    "        (0, None, None, None, None),\n",
    "    )\n",
    "\n",
    "    def _break(val):\n",
    "        i, z, Omega, z_old, Omega_old = val\n",
    "\n",
    "        # in first iteration we don't have old values, converged is True for NaNs\n",
    "        z_converged = jnp.logical_and(converged(z, z_old, eps), i > 0)\n",
    "        Omega_converged = jnp.logical_and(converged(Omega, Omega_old, eps), i > 0)\n",
    "        iteration_limit_reached = i >= n_iter\n",
    "\n",
    "        return jnp.logical_or(\n",
    "            jnp.logical_and(z_converged, Omega_converged), iteration_limit_reached\n",
    "        )\n",
    "\n",
    "    def _iteration(val):\n",
    "        i, z, Omega, _, _ = val\n",
    "        glssm_approx = GLSSM(\n",
    "            model.u,\n",
    "            model.A,\n",
    "            model.D,\n",
    "            model.Sigma0,\n",
    "            model.Sigma,\n",
    "            model.v,\n",
    "            model.B,\n",
    "            Omega,\n",
    "        )\n",
    "        sim_signal = simulation_smoother(glssm_approx, z, N, crn_key)\n",
    "\n",
    "        log_weights = lw_t(sim_signal, y, model.xi, z, Omega)\n",
    "        log_p = dist(sim_signal, model.xi).log_prob(y).sum(axis=-1)\n",
    "        wls_estimate = vmap(optimal_parameters, (1, 1, 1), 0)(\n",
    "            sim_signal, v_norm_w(log_weights), log_p\n",
    "        )\n",
    "\n",
    "        a = wls_estimate[:, 0]\n",
    "        b = wls_estimate[:, 1 : (p + 1)]\n",
    "        c = wls_estimate[:, (p + 1) :]\n",
    "\n",
    "        Omega_inv_new = vmap(tril, (0, None))(c, p)\n",
    "        Omega_inv_new = 1 / 2 * (Omega_inv_new + Omega_inv_new.transpose((0, 2, 1)))\n",
    "        # pinv for missing cases\n",
    "        Omega_new = jnp.linalg.pinv(Omega_inv_new, hermitian=True)\n",
    "        z_new = (Omega_new @ b[..., None])[..., 0]\n",
    "\n",
    "        return i + 1, z_new, Omega_new, z, Omega\n",
    "\n",
    "    _keep_going = lambda *args: jnp.logical_not(_break(*args))\n",
    "\n",
    "    init = (0, z_init, Omega_init, jnp.empty_like(z_init), jnp.empty_like(Omega_init))\n",
    "    n_iters, z, Omega, z_old, Omega_old = while_loop(_keep_going, _iteration, init)\n",
    "\n",
    "    proposal = GLSSMProposal(\n",
    "        model.u, model.A, model.D, model.Sigma0, model.Sigma, model.v, model.B, Omega, z\n",
    "    )\n",
    "\n",
    "    delta_z = jnp.max(jnp.abs(z - z_old))\n",
    "    delta_Omega = jnp.max(jnp.abs(Omega - Omega_old))\n",
    "    information = ConvergenceInformation(\n",
    "        converged=jnp.logical_and(\n",
    "            converged(z, z_old, eps), converged(Omega, Omega_old, eps)\n",
    "        ),\n",
    "        n_iter=n_iters,\n",
    "        delta=jnp.max(jnp.array([delta_z, delta_Omega])),\n",
    "    )\n",
    "\n",
    "    return proposal, information\n",
    "\n",
    "\n",
    "def patch_meis_full_deps():\n",
    "    isssm.modified_efficient_importance_sampling.modified_efficient_importance_sampling = MEIS_full_deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# monkey patch gnll to handle nans\n",
    "vmm = vmap(jnp.matmul, (0,0))\n",
    "from isssm.util import MVN_degenerate as MVN\n",
    "import isssm.estimation\n",
    "@jit\n",
    "def gnll_full_deps(\n",
    "    y: Float[Array, \"n+1 p\"],  # observations $y_t$\n",
    "    x_pred: Float[Array, \"n+1 m\"],  # predicted states $\\hat X_{t+1\\bar t}$\n",
    "    Xi_pred: Float[Array, \"n+1 m m\"],  # predicted state covariances $\\Xi_{t+1\\bar t}$\n",
    "    B: Float[Array, \"n+1 p m\"],  # state observation matrices $B_{t}$\n",
    "    Omega: Float[Array, \"n+1 p p\"],  # observation covariances $\\Omega_{t}$\n",
    ") -> Float:  # gaussian negative log-likelihood\n",
    "    \"\"\"Gaussian negative log-likelihood\"\"\"\n",
    "    y_pred = vmm(B, x_pred)\n",
    "    Psi_pred = vmm(vmm(B, Xi_pred), jnp.transpose(B, (0, 2, 1))) + Omega\n",
    "\n",
    "    return jnp.nansum(-MVN(y_pred, Psi_pred).log_prob(y))\n",
    "\n",
    "def patch_gnll_full_deps():\n",
    "    isssm.estimation.gnll = gnll_full_deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_full_deps():\n",
    "    patch_la_full_deps()\n",
    "    patch_meis_full_deps()\n",
    "    patch_log_weights_full_deps()\n",
    "    patch_gnll_full_deps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/research/lib/python3.10/site-packages/nbdev/export.py:73: UserWarning: Notebook '/Users/stefan/workspace/work/phd/thesis/nbs/4 Models/4.3 Nowcasting hospitalizations/10_model.ipynb' uses `#|export` without `#|default_exp` cell.\n",
      "Note nbdev2 no longer supports nbdev1 syntax. Run `nbdev_migrate` to upgrade.\n",
      "See https://nbdev.fast.ai/getting_started.html for more information.\n",
      "  warn(f\"Notebook '{nbname}' uses `#|export` without `#|default_exp` cell.\\n\"\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
