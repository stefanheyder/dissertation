{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true    # Skips in tests\n",
    "skip_showdoc: true # Skips in docs builds\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp patch.full_deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import jax.numpy as jnp\n",
    "from jaxtyping import Float, Array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monkey patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# log weights monkey patch\n",
    "# nansum to account for missing values\n",
    "# explicitly assumes that s_t = z_t!\n",
    "\n",
    "from isssm.util import MVN_degenerate as MVN\n",
    "import isssm.importance_sampling\n",
    "\n",
    "\n",
    "def log_weights_t(\n",
    "    s_t: Float[Array, \"p\"],  # signal\n",
    "    y_t: Float[Array, \"p\"],  # observation\n",
    "    xi_t: Float[Array, \"p\"],  # parameters\n",
    "    dist,  # observation distribution\n",
    "    z_t: Float[Array, \"p\"],  # synthetic observation\n",
    "    Omega_t: Float[Array, \"p p\"],  # synthetic observation covariance, assumed diagonal\n",
    ") -> Float:  # single log weight\n",
    "    \"\"\"Log weight for a single time point.\"\"\"\n",
    "    p_ys = dist(s_t, xi_t).log_prob(y_t).sum()\n",
    "\n",
    "    # omega_t = jnp.sqrt(jnp.diag(Omega_t))\n",
    "    # g_zs = MVN_diag(s_t, omega_t).log_prob(z_t).sum()\n",
    "    g_zs = jnp.nansum(MVN(s_t, Omega_t).log_prob(z_t))\n",
    "\n",
    "    return p_ys - g_zs\n",
    "\n",
    "\n",
    "def log_weights(\n",
    "    s: Float[Array, \"n+1 p\"],  # signals\n",
    "    y: Float[Array, \"n+1 p\"],  # observations\n",
    "    dist,  # observation distribution\n",
    "    xi: Float[Array, \"n+1 p\"],  # observation parameters\n",
    "    z: Float[Array, \"n+1 p\"],  # synthetic observations\n",
    "    Omega: Float[Array, \"n+1 p p\"],  # synthetic observation covariances:\n",
    ") -> Float:  # log weights\n",
    "    \"\"\"Log weights for all time points\"\"\"\n",
    "    p_ys = dist(s, xi).log_prob(y).sum()\n",
    "\n",
    "    # avoid triangular solve problems\n",
    "    # omega = jnp.sqrt(vmap(jnp.diag)(Omega))\n",
    "    # g_zs = MVN_diag(s, omega).log_prob(z).sum()\n",
    "    g_zs = jnp.nansum(MVN(s, Omega).log_prob(z))\n",
    "\n",
    "    return p_ys - g_zs\n",
    "\n",
    "\n",
    "isssm.importance_sampling.log_weights = log_weights\n",
    "isssm.importance_sampling.log_weights_t = log_weights_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# LA monkey patch\n",
    "from isssm.kalman import kalman, smoothed_signals\n",
    "from isssm.typing import GLSSM, GLSSMProposal, ConvergenceInformation, PGSSM\n",
    "from isssm.laplace_approximation import default_link, vvmap, vdiag\n",
    "from jax.scipy.optimize import minimize\n",
    "from functools import partial\n",
    "from isssm.util import converged\n",
    "from jax import jacfwd, hessian, jacrev, vmap, jit\n",
    "from jax.lax import while_loop\n",
    "import isssm.laplace_approximation\n",
    "from isssm.laplace_approximation import _initial_guess, default_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgsssm_to_glssm(model: PGSSM, Omega: Float[Array, \"n+1 p p\"]) -> GLSSM:\n",
    "    return GLSSM(\n",
    "        u=model.u,\n",
    "        A=model.A,\n",
    "        D=model.D,\n",
    "        Sigma0=model.Sigma0,\n",
    "        Sigma=model.Sigma,\n",
    "        v=model.v,\n",
    "        B=model.B,\n",
    "        Omega=Omega,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isssm.typing import to_glssm\n",
    "\n",
    "def clip_negative_evals(proposal: GLSSMProposal) -> GLSSMProposal:\n",
    "    z = proposal.z\n",
    "    Omega = proposal.Omega\n",
    "\n",
    "    glssm = to_glssm(proposal)\n",
    "    filtered = kalman(z, glssm)\n",
    "    s = smoothed_signals(filtered, z, glssm)\n",
    "    # clip eigenvalues of final Omega to ensure PSD\n",
    "    evals, evecs = jnp.linalg.eigh(Omega)\n",
    "    if (evals > 0).all():\n",
    "        return proposal\n",
    "\n",
    "    evals_clipped = jnp.where(evals < 1e-8, 0.0, evals)\n",
    "    Omega_new = evecs @ vmap(jnp.diag)(evals_clipped) @ jnp.transpose(evecs, (0, 2,1))\n",
    "\n",
    "    # adjust z s.t. (z -s) lies in span of cOmega, project to span of cOmega, then substract\n",
    "    cOmega = jnp.linalg.cholesky(Omega_new)\n",
    "    # projection matrix on im(cOmega ) is cOmega @ jnp.linalg.pinv(cOmega), but use more stable lstsq\n",
    "    z_new = s + (cOmega @ jnp.linalg.lstsq(cOmega, z- s)[..., None])[...,0]\n",
    "\n",
    "    new_proposal = GLSSMProposal(\n",
    "        **proposal,\n",
    "        z=z_new,\n",
    "        Omega=Omega,\n",
    "    )\n",
    "\n",
    "    return new_proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def default_link(y):\n",
    "    return jnp.log(jnp.clip(y, 0.05))\n",
    "\n",
    "def laplace_approximation(\n",
    "    y: Float[Array, \"n+1 p\"],  # observation\n",
    "    model: PGSSM,\n",
    "    n_iter: int,  # number of iterations\n",
    "    log_lik=None,  # log likelihood function\n",
    "    d_log_lik=None,  # derivative of log likelihood function\n",
    "    dd_log_lik=None,  # second derivative of log likelihood function\n",
    "    eps: Float = 1e-5,  # precision of iterations\n",
    "    link=default_link,  # default link to use in initial guess\n",
    ") -> tuple[GLSSMProposal, ConvergenceInformation]:\n",
    "    u, A, D, Sigma0, Sigma, v, B, dist, xi = model\n",
    "    np1, p, m = B.shape\n",
    "\n",
    "    s_init = vmap(partial(_initial_guess, dist=dist, link=link))(xi, y)\n",
    "    # s_init = jnp.zeros((np1, p))  # initial guess for the signal\n",
    "    # missing values have zero obs. -> 0.\n",
    "    # s_init = jnp.hstack(\n",
    "    #    (jnp.log(y.sum(axis=1, keepdims=True) + 1.0), jnp.zeros((np1, 3)))\n",
    "    # )\n",
    "\n",
    "    def default_log_lik(\n",
    "        s_t: Float[Array, \"p\"], xi_t: Float[Array, \"p\"], y_t: Float[Array, \"p'\"]\n",
    "    ):\n",
    "        return dist(s_t, xi_t).log_prob(y_t).sum()\n",
    "\n",
    "    if log_lik is None:\n",
    "        log_lik = default_log_lik\n",
    "\n",
    "    if d_log_lik is None:\n",
    "        d_log_lik = jacfwd(log_lik, argnums=0)\n",
    "    if dd_log_lik is None:\n",
    "        dd_log_lik = jacrev(d_log_lik, argnums=0)\n",
    "\n",
    "    vd_log_lik = jit(vmap(d_log_lik))\n",
    "    vdd_log_lik = jit(vmap(dd_log_lik))\n",
    "\n",
    "    def _break(val):\n",
    "        _, i, z, Omega, z_old, Omega_old = val\n",
    "\n",
    "        z_converged = jnp.logical_and(converged(z, z_old, eps), i > 0)\n",
    "        Omega_converged = jnp.logical_and(converged(Omega, Omega_old, eps), i > 0)\n",
    "        all_converged = jnp.logical_and(z_converged, Omega_converged)\n",
    "\n",
    "        iteration_limit_reached = i >= n_iter\n",
    "\n",
    "        return jnp.logical_or(all_converged, iteration_limit_reached)\n",
    "\n",
    "    def _iteration(val):\n",
    "        s, i, z_old, Omega_old, _, _ = val\n",
    "\n",
    "        grad = vd_log_lik(s, xi, y)\n",
    "        Gamma = -vdd_log_lik(s, xi, y)\n",
    "        # pinv for missing values: if Gamma is 0, we want Omega to be 0 as well\n",
    "        # requries exact derivatives, not numerical ones!\n",
    "        Omega = jnp.linalg.pinv(Gamma, hermitian=True, rcond=1e-10)\n",
    "\n",
    "        # z = s + jnp.linalg.solve(Gamma, grad[..., None])[..., 0]\n",
    "        z = s + (Omega @ grad[..., None])[..., 0]\n",
    "        approx_glssm = GLSSM(u, A, D, Sigma0, Sigma, v, B, Omega)\n",
    "\n",
    "        filtered = kalman(z, approx_glssm)\n",
    "        s_new = smoothed_signals(filtered, z, approx_glssm)\n",
    "\n",
    "        return s_new, i + 1, z, Omega, z_old, Omega_old\n",
    "\n",
    "    empty_z = jnp.empty_like(s_init)\n",
    "    empty_Omega = jnp.empty((np1, p, p))\n",
    "    init = (s_init, 0, empty_z, empty_Omega, empty_z, empty_Omega)\n",
    "\n",
    "    _keep_going = lambda *args: jnp.logical_not(_break(*args))\n",
    "    s, n_iters, z, Omega, z_old, Omega_old = while_loop(_keep_going, _iteration, init)\n",
    "\n",
    "    delta_z = jnp.max(jnp.abs(z - z_old))\n",
    "    delta_Omega = jnp.max(jnp.abs(Omega - Omega_old))\n",
    "    information = ConvergenceInformation(\n",
    "        converged=jnp.logical_and(\n",
    "            converged(z, z_old, eps), converged(Omega, Omega_old, eps)\n",
    "        ),\n",
    "        n_iter=n_iters,\n",
    "        delta=jnp.max(jnp.array([delta_z, delta_Omega])),\n",
    "    )\n",
    "\n",
    "    final_proposal = GLSSMProposal(u, A, D, Sigma0, Sigma, v, B, Omega, z)\n",
    "\n",
    "    return final_proposal, information\n",
    "\n",
    "\n",
    "isssm.laplace_approximation.default_link = default_link\n",
    "isssm.laplace_approximation._initial_guess = _initial_guess\n",
    "isssm.laplace_approximation.laplace_approximation = laplace_approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# MEIS monkey patch\n",
    "import jax.random as jrn\n",
    "from jaxtyping import PRNGKeyArray\n",
    "from isssm.importance_sampling import normalize_weights, log_weights_t\n",
    "from isssm.kalman import simulation_smoother\n",
    "import isssm.modified_efficient_importance_sampling\n",
    "\n",
    "\n",
    "def triu(entries: Float[Array, \" q\"], p: int):\n",
    "    (q,) = entries.shape\n",
    "\n",
    "    if 2 * q != p * (p + 1):\n",
    "        raise ValueError(\n",
    "            f\"entries does not have the correct shape, expected {int(p * (p - 1) /2)} entries in the second dimension but got {q}\"\n",
    "        )\n",
    "\n",
    "    mat = jnp.zeros((p, p))\n",
    "    mat = mat.at[jnp.triu_indices(p)].set(entries)\n",
    "\n",
    "    return mat\n",
    "\n",
    "\n",
    "def tril(entries: Float[Array, \" q\"], p: int):\n",
    "    (q,) = entries.shape\n",
    "\n",
    "    if 2 * q != p * (p + 1):\n",
    "        raise ValueError(\n",
    "            f\"entries does not have the correct shape, expected {int(p * (p - 1) /2)} entries in the second dimension but got {q}\"\n",
    "        )\n",
    "\n",
    "    mat = jnp.zeros((p, p))\n",
    "    mat = mat.at[jnp.tril_indices(p)].set(entries)\n",
    "\n",
    "    return mat\n",
    "\n",
    "\n",
    "@jit\n",
    "def optimal_parameters(\n",
    "    signal: Float[Array, \"N p\"], weights: Float[Array, \"N\"], log_p: Float[Array, \"N\"]\n",
    "):\n",
    "    N, p = signal.shape\n",
    "    ones = jnp.ones_like(weights)[:, None]\n",
    "\n",
    "    second_moment = vmap(jnp.outer)(signal, signal)\n",
    "    # double tranpose, because [:, jnp.tril_indices(p)] is not supported (returns (N, 2, p(p-1)/2, 4) instead of (N, p(p-1)/2)\n",
    "    lower_tri_second_moment = second_moment.transpose((1, 2, 0))[jnp.tril_indices(p)].T\n",
    "\n",
    "    # w_inner_prod = lambda a, b: jnp.einsum(\"i,ij,ik->jk\", weights, a, b)\n",
    "    # X_T_W_X = jnp.block(\n",
    "    #    [\n",
    "    #        [\n",
    "    #            w_inner_prod(ones, ones),\n",
    "    #            w_inner_prod(ones, signal),\n",
    "    #            w_inner_prod(ones, -0.5 * lower_tri_second_moment),\n",
    "    #        ],\n",
    "    #        [\n",
    "    #            w_inner_prod(signal, ones),\n",
    "    #            w_inner_prod(signal, signal),\n",
    "    #            w_inner_prod(signal, -0.5 * lower_tri_second_moment),\n",
    "    #        ],\n",
    "    #        [\n",
    "    #            w_inner_prod(-0.5 * lower_tri_second_moment, ones),\n",
    "    #            w_inner_prod(-0.5 * lower_tri_second_moment, signal),\n",
    "    #            w_inner_prod(\n",
    "    #                -0.5 * lower_tri_second_moment, -0.5 * lower_tri_second_moment\n",
    "    #            ),\n",
    "    #        ],\n",
    "    #    ]\n",
    "    # )\n",
    "    # X_T_W_y = jnp.concatenate(\n",
    "    #    [\n",
    "    #        w_inner_prod(ones, log_p[:, None]),\n",
    "    #        w_inner_prod(signal, log_p[:, None]),\n",
    "    #        w_inner_prod(-0.5 * lower_tri_second_moment, log_p[:, None]),\n",
    "    #    ]\n",
    "    # )\n",
    "\n",
    "    design = jnp.hstack([ones, signal, -0.5 * lower_tri_second_moment])\n",
    "    lhs = log_p\n",
    "\n",
    "    sqrt_weights = jnp.sqrt(weights)\n",
    "\n",
    "    design_W = design * sqrt_weights[:, None]\n",
    "    lhs_W = lhs * sqrt_weights\n",
    "\n",
    "    # if lhs_W is 0, this returns 0 even if design matrix does not have full rank (numpy promises solution with least l2-norm)\n",
    "    beta, *_ = jnp.linalg.lstsq(design_W, lhs_W)\n",
    "    return beta\n",
    "    # beta = jnp.linalg.solve(X_T_W_X, X_T_W_y[:, 0])\n",
    "    # return beta\n",
    "\n",
    "\n",
    "def modified_efficient_importance_sampling(\n",
    "    y: Float[Array, \"n+1 p\"],  # observations\n",
    "    model: PGSSM,  # model\n",
    "    z_init: Float[Array, \"n+1 p\"],  # initial z estimate\n",
    "    Omega_init: Float[Array, \"n+1 p p\"],  # initial Omega estimate\n",
    "    n_iter: int,  # number of iterations\n",
    "    N: int,  # number of samples\n",
    "    key: PRNGKeyArray,  # random key\n",
    "    eps: Float = 1e-5,  # convergence threshold\n",
    "):\n",
    "    z, Omega = z_init, Omega_init\n",
    "\n",
    "    np1, p, m = model.B.shape\n",
    "\n",
    "    key, crn_key = jrn.split(key)\n",
    "\n",
    "    v_norm_w = vmap(normalize_weights)\n",
    "    dist = model.dist\n",
    "    lw_t = vmap(\n",
    "        vmap(lambda s, y, xi, z, Omega: log_weights_t(s, y, xi, dist, z, Omega)),\n",
    "        (0, None, None, None, None),\n",
    "    )\n",
    "\n",
    "    def _break(val):\n",
    "        i, z, Omega, z_old, Omega_old = val\n",
    "\n",
    "        # in first iteration we don't have old values, converged is True for NaNs\n",
    "        z_converged = jnp.logical_and(converged(z, z_old, eps), i > 0)\n",
    "        Omega_converged = jnp.logical_and(converged(Omega, Omega_old, eps), i > 0)\n",
    "        iteration_limit_reached = i >= n_iter\n",
    "\n",
    "        return jnp.logical_or(\n",
    "            jnp.logical_and(z_converged, Omega_converged), iteration_limit_reached\n",
    "        )\n",
    "\n",
    "    def _iteration(val):\n",
    "        i, z, Omega, _, _ = val\n",
    "        glssm_approx = GLSSM(\n",
    "            model.u,\n",
    "            model.A,\n",
    "            model.D,\n",
    "            model.Sigma0,\n",
    "            model.Sigma,\n",
    "            model.v,\n",
    "            model.B,\n",
    "            Omega,\n",
    "        )\n",
    "        sim_signal = simulation_smoother(glssm_approx, z, N, crn_key)\n",
    "\n",
    "        log_weights = lw_t(sim_signal, y, model.xi, z, Omega)\n",
    "        log_p = dist(sim_signal, model.xi).log_prob(y).sum(axis=-1)\n",
    "        wls_estimate = vmap(optimal_parameters, (1, 1, 1), 0)(\n",
    "            sim_signal, v_norm_w(log_weights), log_p\n",
    "        )\n",
    "\n",
    "        a = wls_estimate[:, 0]\n",
    "        b = wls_estimate[:, 1 : (p + 1)]\n",
    "        c = wls_estimate[:, (p + 1) :]\n",
    "\n",
    "        Omega_inv_new = vmap(tril, (0, None))(c, p)\n",
    "        Omega_inv_new = 1 / 2 * (Omega_inv_new + Omega_inv_new.transpose((0, 2, 1)))\n",
    "        # pinv for missing cases\n",
    "        Omega_new = jnp.linalg.pinv(Omega_inv_new, hermitian=True)\n",
    "        z_new = (Omega_new @ b[..., None])[..., 0]\n",
    "\n",
    "        return i + 1, z_new, Omega_new, z, Omega\n",
    "\n",
    "    _keep_going = lambda *args: jnp.logical_not(_break(*args))\n",
    "\n",
    "    init = (0, z_init, Omega_init, jnp.empty_like(z_init), jnp.empty_like(Omega_init))\n",
    "    n_iters, z, Omega, z_old, Omega_old = while_loop(_keep_going, _iteration, init)\n",
    "\n",
    "    proposal = GLSSMProposal(\n",
    "        model.u, model.A, model.D, model.Sigma0, model.Sigma, model.v, model.B, Omega, z\n",
    "    )\n",
    "\n",
    "    delta_z = jnp.max(jnp.abs(z - z_old))\n",
    "    delta_Omega = jnp.max(jnp.abs(Omega - Omega_old))\n",
    "    information = ConvergenceInformation(\n",
    "        converged=jnp.logical_and(\n",
    "            converged(z, z_old, eps), converged(Omega, Omega_old, eps)\n",
    "        ),\n",
    "        n_iter=n_iters,\n",
    "        delta=jnp.max(jnp.array([delta_z, delta_Omega])),\n",
    "    )\n",
    "\n",
    "    return proposal, information\n",
    "\n",
    "\n",
    "isssm.modified_efficient_importance_sampling.modified_efficient_importance_sampling = (\n",
    "    modified_efficient_importance_sampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# monkey patch gnll to handle nans\n",
    "vmm = vmap(jnp.matmul, (0,0))\n",
    "from isssm.util import MVN_degenerate as MVN\n",
    "import isssm.estimation\n",
    "@jit\n",
    "def gnll(\n",
    "    y: Float[Array, \"n+1 p\"],  # observations $y_t$\n",
    "    x_pred: Float[Array, \"n+1 m\"],  # predicted states $\\hat X_{t+1\\bar t}$\n",
    "    Xi_pred: Float[Array, \"n+1 m m\"],  # predicted state covariances $\\Xi_{t+1\\bar t}$\n",
    "    B: Float[Array, \"n+1 p m\"],  # state observation matrices $B_{t}$\n",
    "    Omega: Float[Array, \"n+1 p p\"],  # observation covariances $\\Omega_{t}$\n",
    ") -> Float:  # gaussian negative log-likelihood\n",
    "    \"\"\"Gaussian negative log-likelihood\"\"\"\n",
    "    y_pred = vmm(B, x_pred)\n",
    "    Psi_pred = vmm(vmm(B, Xi_pred), jnp.transpose(B, (0, 2, 1))) + Omega\n",
    "\n",
    "    return jnp.nansum(-MVN(y_pred, Psi_pred).log_prob(y))\n",
    "\n",
    "isssm.estimation.gnll = gnll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
