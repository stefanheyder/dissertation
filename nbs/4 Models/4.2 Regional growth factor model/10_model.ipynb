{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true    # Skips in tests\n",
    "skip_showdoc: true # Skips in docs builds\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp models.regional_growth_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy as jsp\n",
    "from pyprojroot.here import here\n",
    "import jax\n",
    "from jax import vmap, jit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import jax.random as jrn\n",
    "\n",
    "from isssm.typing import PGSSM, GLSSMState\n",
    "from jaxtyping import Array, Float\n",
    "\n",
    "from tensorflow_probability.substrates.jax.distributions import (\n",
    "    NegativeBinomial as NBinom,\n",
    ")\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monkey patches (account for missing observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NBinom sampler is slow (https://github.com/tensorflow/probability/issues/1843)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def _call_sample_n_patched(self, sample_shape, seed, **kwargs):\n",
    "    return self._sample_n(sample_shape, seed)\n",
    "\n",
    "\n",
    "def _sample_n_patched(self, n, seed=None):\n",
    "    # Here we use the fact that if:\n",
    "    # lam ~ Gamma(concentration=total_count, rate=(1-probs)/probs)\n",
    "    # then X ~ Poisson(lam) is Negative Binomially distributed.\n",
    "    logits = self._logits_parameter_no_checks()\n",
    "    gamma_seed, poisson_seed = jrn.split(seed)\n",
    "    r = self.total_count\n",
    "    mu = jnp.exp(logits) * r\n",
    "    gamma_sample = mu / r * jrn.gamma(gamma_seed, r, (mu / r).shape)\n",
    "    return jrn.poisson(poisson_seed, gamma_sample)\n",
    "\n",
    "\n",
    "NBinom._sample_n = _sample_n_patched\n",
    "NBinom._call_sample_n = _call_sample_n_patched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def account_for_nans(model: PGSSM, y, missing_indices) -> tuple[PGSSM, Float]:\n",
    "    # only works for Poisson!\n",
    "    # missing_indices = jnp.isnan(y)\n",
    "\n",
    "    y_missing = jnp.nan_to_num(y, nan=0.0)\n",
    "\n",
    "    v = model.v.at[missing_indices].set(0.0)\n",
    "    B = model.B.at[missing_indices].set(0.0)\n",
    "\n",
    "    xi_missing = jnp.ones_like(y).at[missing_indices].set(0.0)[..., None]\n",
    "    xi = jnp.concatenate([xi_missing, model.xi], -1)\n",
    "\n",
    "    def missing_dist(s, xi):\n",
    "        xi_missing, old_xi = xi[..., 0], xi[..., 1:]\n",
    "        old_dist = model.dist(s, old_xi)\n",
    "        # if xi == 0. the logits becomes -inf, so dist is dirac_0\n",
    "        return NBinom(\n",
    "            old_dist.total_count, logits=old_dist.logits + jnp.log(xi_missing)\n",
    "        )\n",
    "\n",
    "    model_missing = PGSSM(\n",
    "        u=model.u,\n",
    "        A=model.A,\n",
    "        D=model.D,\n",
    "        Sigma0=model.Sigma0,\n",
    "        Sigma=model.Sigma,\n",
    "        v=v,\n",
    "        B=B,\n",
    "        dist=missing_dist,\n",
    "        xi=xi,\n",
    "    )\n",
    "\n",
    "    return model_missing, y_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# log weights monkey patch\n",
    "# nansum to account for missing values\n",
    "# explicitly assumes that s_t = z_t!\n",
    "\n",
    "from isssm.util import MVN_degenerate as MVN\n",
    "import isssm.importance_sampling\n",
    "\n",
    "\n",
    "def log_weights_t(\n",
    "    s_t: Float[Array, \"p\"],  # signal\n",
    "    y_t: Float[Array, \"p\"],  # observation\n",
    "    xi_t: Float[Array, \"p\"],  # parameters\n",
    "    dist,  # observation distribution\n",
    "    z_t: Float[Array, \"p\"],  # synthetic observation\n",
    "    Omega_t: Float[Array, \"p p\"],  # synthetic observation covariance, assumed diagonal\n",
    ") -> Float:  # single log weight\n",
    "    \"\"\"Log weight for a single time point.\"\"\"\n",
    "    p_ys = dist(s_t, xi_t).log_prob(y_t).sum()\n",
    "\n",
    "    # omega_t = jnp.sqrt(jnp.diag(Omega_t))\n",
    "    # g_zs = MVN_diag(s_t, omega_t).log_prob(z_t).sum()\n",
    "    g_zs = jnp.sum(\n",
    "        jnp.nan_to_num(MVN(s_t, Omega_t).log_prob(z_t), nan=0.0, neginf=0.0, posinf=0.0)\n",
    "    )\n",
    "\n",
    "    return p_ys - g_zs\n",
    "\n",
    "\n",
    "def log_weights(\n",
    "    s: Float[Array, \"n+1 p\"],  # signals\n",
    "    y: Float[Array, \"n+1 p\"],  # observations\n",
    "    dist,  # observation distribution\n",
    "    xi: Float[Array, \"n+1 p\"],  # observation parameters\n",
    "    z: Float[Array, \"n+1 p\"],  # synthetic observations\n",
    "    Omega: Float[Array, \"n+1 p p\"],  # synthetic observation covariances:\n",
    ") -> Float:  # log weights\n",
    "    \"\"\"Log weights for all time points\"\"\"\n",
    "    p_ys = dist(s, xi).log_prob(y).sum()\n",
    "\n",
    "    # avoid triangular solve problems\n",
    "    # omega = jnp.sqrt(vmap(jnp.diag)(Omega))\n",
    "    # g_zs = MVN_diag(s, omega).log_prob(z).sum()\n",
    "    # g_zs = jnp.nansum(MVN(s, Omega).log_prob(z))\n",
    "    g_zs = jnp.sum(\n",
    "        jnp.nan_to_num(MVN(s, Omega).log_prob(z), nan=0.0, neginf=0.0, posinf=0.0)\n",
    "    )\n",
    "\n",
    "    return p_ys - g_zs\n",
    "\n",
    "\n",
    "isssm.importance_sampling.log_weights = log_weights\n",
    "isssm.importance_sampling.log_weights_t = log_weights_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# LA monkey patch\n",
    "from isssm.kalman import kalman, smoothed_signals\n",
    "from isssm.typing import GLSSM, GLSSMProposal, ConvergenceInformation\n",
    "from isssm.laplace_approximation import default_link, vvmap, vdiag\n",
    "from jax.scipy.optimize import minimize\n",
    "from functools import partial\n",
    "from isssm.util import converged\n",
    "from jax import jacfwd, hessian, jacrev\n",
    "from jax.lax import while_loop\n",
    "import isssm.laplace_approximation\n",
    "\n",
    "\n",
    "def _initial_guess(xi_t, y_t, dist, link=jnp.log):\n",
    "    result = minimize(\n",
    "        lambda s_ti: -dist(s_ti, xi_t).log_prob(y_t).sum(),\n",
    "        jnp.concatenate((jnp.atleast_1d(link(y_t.sum())), jnp.zeros(3))),\n",
    "        method=\"BFGS\",\n",
    "    )\n",
    "    return jnp.squeeze(result.x)\n",
    "\n",
    "\n",
    "def laplace_approximation(\n",
    "    y: Float[Array, \"n+1 p\"],  # observation\n",
    "    model: PGSSM,\n",
    "    n_iter: int,  # number of iterations\n",
    "    log_lik=None,  # log likelihood function\n",
    "    d_log_lik=None,  # derivative of log likelihood function\n",
    "    dd_log_lik=None,  # second derivative of log likelihood function\n",
    "    eps: Float = 1e-5,  # precision of iterations\n",
    "    link=default_link,  # default link to use in initial guess\n",
    ") -> tuple[GLSSMProposal, ConvergenceInformation]:\n",
    "    u, A, D, Sigma0, Sigma, v, B, dist, xi = model\n",
    "    np1, p, m = B.shape\n",
    "\n",
    "    # s_init = vmap(partial(_initial_guess, dist=dist, link=link))(xi, y)\n",
    "    # missing values have zero obs. -> 0.\n",
    "    # s_init = jnp.hstack(\n",
    "    #    (jnp.log(y.sum(axis=1, keepdims=True) + 1.0), jnp.zeros((np1, 3)))\n",
    "    # )\n",
    "    s_init = jnp.nan_to_num(jnp.log(y / model.xi[..., -1]), nan=0.0, neginf=0.0)\n",
    "\n",
    "    def default_log_lik(s_ti, xi_ti, y_ti):\n",
    "        return jnp.nansum(dist(s_ti, xi_ti).log_prob(y_ti))\n",
    "\n",
    "    if log_lik is None:\n",
    "        log_lik = default_log_lik\n",
    "\n",
    "    if d_log_lik is None:\n",
    "        d_log_lik = jacfwd(log_lik, argnums=0)\n",
    "    if dd_log_lik is None:\n",
    "        dd_log_lik = jacrev(d_log_lik, argnums=0)\n",
    "\n",
    "    vd_log_lik = jit(vvmap(d_log_lik))\n",
    "    vdd_log_lik = jit(vvmap(dd_log_lik))\n",
    "\n",
    "    def _break(val):\n",
    "        _, i, z, Omega, z_old, Omega_old = val\n",
    "\n",
    "        z_converged = jnp.logical_and(converged(z, z_old, eps), i > 0)\n",
    "        Omega_converged = jnp.logical_and(converged(Omega, Omega_old, eps), i > 0)\n",
    "        all_converged = jnp.logical_and(z_converged, Omega_converged)\n",
    "\n",
    "        iteration_limit_reached = i >= n_iter\n",
    "\n",
    "        return jnp.logical_or(all_converged, iteration_limit_reached)\n",
    "\n",
    "    def _iteration(val):\n",
    "        s, i, z_old, Omega_old, _, _ = val\n",
    "\n",
    "        grad = vd_log_lik(s, xi, y)\n",
    "        Gamma = -vdd_log_lik(s, xi, y)\n",
    "        # assume hessian is diagonal\n",
    "        Gamma_inv = jnp.nan_to_num(1.0 / Gamma, neginf=0.0, posinf=0.0)\n",
    "        Omega = vdiag(Gamma_inv)\n",
    "\n",
    "        z = s + grad * Gamma_inv\n",
    "        approx_glssm = GLSSM(u, A, D, Sigma0, Sigma, v, B, Omega)\n",
    "\n",
    "        filtered = kalman(z, approx_glssm)\n",
    "        s_new = smoothed_signals(filtered, z, approx_glssm)\n",
    "\n",
    "        return s_new, i + 1, z, Omega, z_old, Omega_old\n",
    "\n",
    "    empty_z = jnp.empty_like(s_init)\n",
    "    empty_Omega = jnp.empty((np1, p, p))\n",
    "    init = (s_init, 0, empty_z, empty_Omega, empty_z, empty_Omega)\n",
    "\n",
    "    _keep_going = lambda *args: jnp.logical_not(_break(*args))\n",
    "    _, n_iters, z, Omega, z_old, Omega_old = while_loop(_keep_going, _iteration, init)\n",
    "\n",
    "    final_proposal = GLSSMProposal(u, A, D, Sigma0, Sigma, v, B, Omega, z)\n",
    "    delta_z = jnp.max(jnp.abs(z - z_old))\n",
    "    delta_Omega = jnp.max(jnp.abs(Omega - Omega_old))\n",
    "    information = ConvergenceInformation(\n",
    "        converged=jnp.logical_and(\n",
    "            converged(z, z_old, eps), converged(Omega, Omega_old, eps)\n",
    "        ),\n",
    "        n_iter=n_iters,\n",
    "        delta=jnp.max(jnp.array([delta_z, delta_Omega])),\n",
    "    )\n",
    "    return final_proposal, information\n",
    "\n",
    "\n",
    "isssm.laplace_approximation._initial_guess = _initial_guess\n",
    "isssm.laplace_approximation.laplace_approximation = laplace_approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# MEIS monkey patch\n",
    "import jax.random as jrn\n",
    "from jaxtyping import PRNGKeyArray\n",
    "from isssm.importance_sampling import normalize_weights, log_weights_t\n",
    "from isssm.kalman import simulation_smoother\n",
    "import isssm.modified_efficient_importance_sampling\n",
    "\n",
    "optimal_parameters = isssm.modified_efficient_importance_sampling.optimal_parameters\n",
    "\n",
    "\n",
    "def modified_efficient_importance_sampling(\n",
    "    y: Float[Array, \"n+1 p\"],  # observations\n",
    "    model: PGSSM,  # model\n",
    "    z_init: Float[Array, \"n+1 p\"],  # initial z estimate\n",
    "    Omega_init: Float[Array, \"n+1 p p\"],  # initial Omega estimate\n",
    "    n_iter: int,  # number of iterations\n",
    "    N: int,  # number of samples\n",
    "    key: PRNGKeyArray,  # random key\n",
    "    eps: Float = 1e-5,  # convergence threshold\n",
    ") -> tuple[GLSSMProposal, ConvergenceInformation]:\n",
    "    z, Omega = z_init, Omega_init\n",
    "\n",
    "    np1, p, m = model.B.shape\n",
    "\n",
    "    key, crn_key = jrn.split(key)\n",
    "\n",
    "    v_norm_w = vmap(normalize_weights)\n",
    "    dist = model.dist\n",
    "    lw_t = vmap(\n",
    "        vmap(lambda s, y, xi, z, Omega: log_weights_t(s, y, xi, dist, z, Omega)),\n",
    "        (0, None, None, None, None),\n",
    "    )\n",
    "\n",
    "    def _break(val):\n",
    "        i, z, Omega, z_old, Omega_old = val\n",
    "\n",
    "        # in first iteration we don't have old values, converged is True for NaNs\n",
    "        z_converged = jnp.logical_and(converged(z, z_old, eps), i > 0)\n",
    "        Omega_converged = jnp.logical_and(converged(Omega, Omega_old, eps), i > 0)\n",
    "        iteration_limit_reached = i >= n_iter\n",
    "\n",
    "        return jnp.logical_or(\n",
    "            jnp.logical_and(z_converged, Omega_converged), iteration_limit_reached\n",
    "        )\n",
    "\n",
    "    def _iteration(val):\n",
    "        i, z, Omega, _, _ = val\n",
    "        glssm_approx = GLSSM(\n",
    "            model.u,\n",
    "            model.A,\n",
    "            model.D,\n",
    "            model.Sigma0,\n",
    "            model.Sigma,\n",
    "            model.v,\n",
    "            model.B,\n",
    "            Omega,\n",
    "        )\n",
    "        sim_signal = simulation_smoother(glssm_approx, z, N, crn_key)\n",
    "\n",
    "        log_weights = lw_t(sim_signal, y, model.xi, z, Omega)\n",
    "        log_p = dist(sim_signal, model.xi).log_prob(y).sum(axis=-1)\n",
    "        wls_estimate = vmap(optimal_parameters, (1, 1, 1), 0)(\n",
    "            sim_signal, v_norm_w(log_weights), log_p\n",
    "        )\n",
    "\n",
    "        a = wls_estimate[:, 0]\n",
    "        b = wls_estimate[:, 1 : (p + 1)]\n",
    "        c = wls_estimate[:, (p + 1) :]\n",
    "        c_inv = jnp.nan_to_num(1 / c, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        z_new = b * c_inv\n",
    "        Omega_new = vmap(jnp.diag)(c_inv)\n",
    "\n",
    "        return i + 1, z_new, Omega_new, z, Omega\n",
    "\n",
    "    _keep_going = lambda *args: jnp.logical_not(_break(*args))\n",
    "\n",
    "    n_iters, z, Omega, z_old, Omega_old = while_loop(\n",
    "        _keep_going,\n",
    "        _iteration,\n",
    "        (0, z_init, Omega_init, jnp.empty_like(z_init), jnp.empty_like(Omega_init)),\n",
    "    )\n",
    "\n",
    "    proposal = GLSSMProposal(\n",
    "        u=model.u,\n",
    "        A=model.A,\n",
    "        D=model.D,\n",
    "        Sigma0=model.Sigma0,\n",
    "        Sigma=model.Sigma,\n",
    "        v=model.v,\n",
    "        B=model.B,\n",
    "        Omega=Omega,\n",
    "        z=z,\n",
    "    )\n",
    "\n",
    "    delta_z = jnp.max(jnp.abs(z - z_old))\n",
    "    delta_Omega = jnp.max(jnp.abs(Omega - Omega_old))\n",
    "    information = ConvergenceInformation(\n",
    "        converged=jnp.logical_and(\n",
    "            converged(z, z_old, eps), converged(Omega, Omega_old, eps)\n",
    "        ),\n",
    "        n_iter=n_iters,\n",
    "        delta=jnp.max(jnp.array([delta_z, delta_Omega])),\n",
    "    )\n",
    "\n",
    "    return proposal, information\n",
    "\n",
    "\n",
    "isssm.modified_efficient_importance_sampling.modified_efficient_importance_sampling = (\n",
    "    modified_efficient_importance_sampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# monkey patch gnll to handle nans\n",
    "vmm = vmap(jnp.matmul, (0, 0))\n",
    "from isssm.util import MVN_degenerate as MVN\n",
    "import isssm.estimation\n",
    "\n",
    "\n",
    "def gnll(\n",
    "    y: Float[Array, \"n+1 p\"],  # observations $y_t$\n",
    "    x_pred: Float[Array, \"n+1 m\"],  # predicted states $\\hat X_{t+1\\bar t}$\n",
    "    Xi_pred: Float[Array, \"n+1 m m\"],  # predicted state covariances $\\Xi_{t+1\\bar t}$\n",
    "    B: Float[Array, \"n+1 p m\"],  # state observation matrices $B_{t}$\n",
    "    Omega: Float[Array, \"n+1 p p\"],  # observation covariances $\\Omega_{t}$\n",
    ") -> Float:  # gaussian negative log-likelihood\n",
    "    \"\"\"Gaussian negative log-likelihood\"\"\"\n",
    "    y_pred = vmm(B, x_pred)\n",
    "    Psi_pred = vmm(vmm(B, Xi_pred), jnp.transpose(B, (0, 2, 1))) + Omega\n",
    "\n",
    "    return jnp.sum(\n",
    "        jnp.nan_to_num(\n",
    "            -MVN(y_pred, Psi_pred).log_prob(y), nan=0.0, posinf=0.0, neginf=0.0\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "isssm.estimation.gnll = gnll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "key = jrn.PRNGKey(452342342345)\n",
    "n_iterations = 20\n",
    "N_mle = 1000\n",
    "N_meis = 2000\n",
    "N_posterior = 10000\n",
    "\n",
    "# same as in FCH\n",
    "percentiles_of_interest = jnp.array(\n",
    "    [0.01, 0.025, *(0.05 * jnp.arange(1, 20)), 0.975, 0.99]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams[\"figure.figsize\"] = (20, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## States\n",
    "Let $\\bar r_t = \\log \\bar \\rho_t$ be the average growth factor (on the log scale) across all counties and $u_t^c$ be the deviation of county $c$'s log-growth factor for $\\bar r_t$. \n",
    "We model $\\bar r_t$ following a random walk and $u^\\cdot_t$ following an $\\text{VAR}(1)$ process with transition matrix $\\alpha I$ and spatial correlation $\\Omega$.\n",
    "\n",
    "Thus we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\bar r_{t + 1} &= \\bar r_{t} + \\varepsilon_{t + 1}^\\rho \\\\\n",
    "u^c_{t + 1} &= \\alpha u^c_t + \\varepsilon_{t + 1}^c\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "with $\\text{Var}(\\varepsilon_{t + 1}^\\rho) = \\sigma^2_\\rho$ and $\\text{Cov}(\\varepsilon_{t + 1}) = \\Omega$. \n",
    "The covariance matrix of the stationary distribution is $\\Sigma = \\frac {1} {1 - \\alpha^2} \\Omega$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def _state_model(r0, u0, alpha, s2_rho, Omega, n) -> GLSSMState:\n",
    "    (K,) = u0.shape\n",
    "    A = jnp.broadcast_to(\n",
    "        jsp.linalg.block_diag(1, alpha * jnp.eye(K)), (n, K + 1, K + 1)\n",
    "    )\n",
    "    # Sigma0 = jsp.linalg.block_diag(s2_rho, 1 / (1 - alpha**2) * Omega)\n",
    "    Sigma0 = jnp.eye(K + 1)\n",
    "    Sigma = jsp.linalg.block_diag(s2_rho * jnp.eye(1), Omega)\n",
    "    Sigma = jnp.broadcast_to(Sigma, (n, K + 1, K + 1))\n",
    "\n",
    "    x0 = jnp.concatenate([r0, u0])\n",
    "    D = jnp.broadcast_to(jnp.eye(K + 1), (n, K + 1, K + 1))\n",
    "    u = jnp.zeros((n + 1, K + 1))\n",
    "    u = u.at[0].set(x0)\n",
    "\n",
    "    return GLSSMState(u, A, D, Sigma0, Sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "The log growth factor in a region at time $t$, $r^c_t$, is given by the mean log growth factor $\\bar r_t$ and the per-region deviation $u^c_t$.\n",
    "Conditional on the log growth factors and past cases, cases are Negative Binomially distributed with shared overdispersion parameter $r$\n",
    "$$\n",
    "\\begin{align*}\n",
    "r^c_t &= \\bar r_t + u^c_{t} \\\\\n",
    "\\lambda_t^c &= \\exp(r^c_t) \\sum_{d}p_{c,d} I_t^d \\\\\n",
    "I^c_{t + 1} | I^c_{t}, \\rho_t, u^c_t &\\sim \\text{NegBinom}(\\underbrace{\\lambda_t^c}_{\\text{mean}}, \\underbrace{r}_{\\text{overdispersion}})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where the variance of the negative binomial distribution is $\\mu + \\frac{\\mu^2}{r}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from tensorflow_probability.substrates.jax.distributions import Poisson\n",
    "\n",
    "\n",
    "def _observation_model(obs: Float[Array, \"n+2 K\"], P: Float[Array, \"K K\"], r: Float):\n",
    "\n",
    "    np2, p = obs.shape\n",
    "    np1 = np2 - 1\n",
    "\n",
    "    delayed_obs = obs[:-1]\n",
    "    cases_adjusted = vmap(jnp.matmul, (None, 0))(P.T, delayed_obs)\n",
    "\n",
    "    xi = jnp.concatenate(\n",
    "        (jnp.full((np1, p, 1), r), cases_adjusted[:, :, None]), axis=-1\n",
    "    )\n",
    "    # xi = cases_adjusted\n",
    "\n",
    "    def dist_obs_nb(signal, xi):\n",
    "        r, sum_I = jnp.moveaxis(xi, -1, 0)\n",
    "        return NBinom(r, logits=signal + jnp.log(sum_I) - jnp.log(r))\n",
    "\n",
    "    def dist_obs_pois(signal, xi):\n",
    "        sum_I = xi\n",
    "        return Poisson(log_rate=signal + jnp.log(sum_I))\n",
    "\n",
    "    B = jnp.hstack([jnp.ones((p, 1)), jnp.eye(p)])\n",
    "    B = jnp.broadcast_to(B, (np1, p, p + 1))\n",
    "    v = jnp.zeros((np1, p))\n",
    "\n",
    "    return v, B, dist_obs_nb, xi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Correlations\n",
    "\n",
    "Suppose we have in county $c$ $S^c$ many new infections generated, which may be attributed to the same county or another one, $c'$ say.\n",
    "Let $p_{c,c'}$ be the fraction of cases generated in county $c'$ (instead of in county $c$) and $p_{c,c} = 1 - \\sum_{c' \\neq c} p_{c,c'}$.\n",
    "\n",
    "\n",
    "Let $P = \\left(p_{c,c'}\\right)_{c = 1, \\dots, K, c' = 1,\\dots, K}$.\n",
    "\n",
    "Let $\\tilde S^c = \\sum p_{c,d} S^d$ be the number of cases generated in county $c$.\n",
    "\n",
    "Then we are interested in \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{Cov}(\\tilde S^c, \\tilde S^{c'}) = \\underbrace{\\sigma^2}_{\\text{Var}(S^c)} (\\sum_{d} p_{c,d} p_{c',d}) = \\sigma^2_{\\text{spat}} (P P^T)_{c, c'}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To obtain $p_{c,c'}$ we use data on commuters $q_{c,c'}$ the fraction of socially insured employees that have their center of life in county $c$ but are registered to work in county $c'$. \n",
    "\n",
    "To account for non-working inhabitants (elderly, kids, ...) we introduce a constant $C \\geq 1$ s.t. \n",
    "$$\n",
    "p_{c,c'} = \\bar q + (1 - \\bar q)\\frac{\\mathbf 1 _{c \\neq c'} q_{c,c'}}{ \\sum_{d \\neq c} q_{c,d} + C q_{c,c}},\n",
    "$$\n",
    "i.e. we blow up the proportion of \"stay at home\" by a constant $C$ (that is the same for all counties) and add a constant \"socket\" of travel $\\bar q$ between the counties.\n",
    "\n",
    "\n",
    "Finally we choose\n",
    "\n",
    "$$\n",
    "\\Omega = \\sigma^2_{\\text{spat}}PP^T %+ \\sigma^2_{\\text{nugget}} I\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def _P(C, q, n_ij, n_tot) -> Float[Array, \"K K\"]:\n",
    "    p, _ = n_ij.shape\n",
    "    m_ij = n_ij + jnp.diag(C * n_tot - n_ij.sum(axis=1))\n",
    "    normalize_rows = lambda x: x / x.sum(axis=1).reshape((-1, 1))\n",
    "    return jnp.full((p, p), q / p) + (1 - q) * normalize_rows(m_ij)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Parameters\n",
    "\n",
    "$$\n",
    "\\theta = \\left( \\text{logit}(\\alpha), \\log \\sigma^2_r, \\log \\sigma^2_{\\text{spat}}, C, \\log \\mu \\right)%\\log \\sigma^2_{\\text{nugget}}, \\log \\mu \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def growth_factor_model(theta, aux) -> PGSSM:\n",
    "\n",
    "    logit_alpha, log_s2_r, log_s2_spat, logit_q, log_Cm1, log_r = theta\n",
    "    # logit_alpha, log_s2_r, log_s2_spat, logit_q, log_Cm1 = theta\n",
    "    obs, n_ij, n_tot = aux\n",
    "    C = jnp.exp(log_Cm1) + 1\n",
    "\n",
    "    np2, _ = obs.shape\n",
    "    np1 = np2 - 1\n",
    "    (K,) = n_tot.shape\n",
    "\n",
    "    m = K + 1\n",
    "    p = K\n",
    "    l = K + 1\n",
    "\n",
    "    alpha = jsp.special.expit(logit_alpha)\n",
    "    s2_rho = jnp.exp(log_s2_r)\n",
    "    s2_spat = jnp.exp(log_s2_spat)\n",
    "    r = jnp.exp(log_r)\n",
    "    # r = None\n",
    "    q = jsp.special.expit(logit_q)\n",
    "\n",
    "    P = _P(C, q, n_ij, n_tot)\n",
    "    state = _state_model(\n",
    "        jnp.zeros(1), jnp.zeros(K), alpha, s2_rho, s2_spat * P.T @ P, np1 - 1\n",
    "    )\n",
    "    obs = _observation_model(obs, P, r)\n",
    "\n",
    "    return PGSSM(*state, *obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "n_tot = jnp.array(\n",
    "    pd.read_csv(here() / \"data/processed/home_totals.csv\").iloc[:, 2].to_numpy()\n",
    ")\n",
    "n_ij = (\n",
    "    jnp.array(\n",
    "        pd.read_csv(here() / \"data/processed/incommuters.csv\").iloc[:, -1].to_numpy()\n",
    "    )\n",
    "    .reshape((400, 400))\n",
    "    .T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(jnp.log(n_ij))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = _P(1.0, 0.5, n_ij, n_tot)\n",
    "plt.imshow(P)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "df_weekly_cases = pd.read_csv(here() / \"data/processed/RKI_county_weekly.csv\").pivot(\n",
    "    index=\"date\", columns=\"ags\", values=\"cases\"\n",
    ")\n",
    "\n",
    "\n",
    "cases_full = jnp.asarray(df_weekly_cases.to_numpy(), dtype=jnp.float64)\n",
    "dates_full = df_weekly_cases.index.to_numpy()\n",
    "ags_full = df_weekly_cases.columns.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def make_aux(date, cases_full, n_ij, n_tot, np1):\n",
    "    iloc_of_date_in_index = jnp.where(dates_full == date)[0][0]\n",
    "    cases = cases_full[iloc_of_date_in_index : iloc_of_date_in_index + np1 + 1]\n",
    "    return cases, n_ij, n_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import jax.scipy as jsp\n",
    "\n",
    "\n",
    "def par_to_theta(params):\n",
    "    alpha, s2_r, s2_spat, q, C, r = params\n",
    "    return jnp.array(\n",
    "        [\n",
    "            jsp.special.logit(alpha),\n",
    "            jnp.log(s2_r),\n",
    "            jnp.log(s2_spat),\n",
    "            jsp.special.logit(q),\n",
    "            jnp.log(C - 1),\n",
    "            jnp.log(r),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def theta_to_par(theta):\n",
    "    alpha, s2_r, s2_spat, q, Cm1, r = theta\n",
    "    return jnp.array(\n",
    "        [\n",
    "            jsp.special.expit(alpha),\n",
    "            jnp.exp(s2_r),\n",
    "            jnp.exp(s2_spat),\n",
    "            jsp.special.expit(q),\n",
    "            jnp.exp(Cm1) + 1,\n",
    "            jnp.exp(r),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_manual = jnp.array(\n",
    "    [\n",
    "        0.5,\n",
    "        0.01**2,\n",
    "        0.01**2,\n",
    "        0.5,\n",
    "        2,\n",
    "        100.0,\n",
    "    ]\n",
    ")\n",
    "theta_manual = par_to_theta(par_manual)\n",
    "np1 = 10\n",
    "# aux = (cases_full[: np1 + 1], n_ij, n_tot)\n",
    "dates = dates_full[14 + 1 : 14 + np1 + 1]\n",
    "aux = make_aux(dates[0], cases_full, n_ij, n_tot, np1)\n",
    "y = aux[0][1:]\n",
    "y_nan = y.at[-1].set(jnp.nan)\n",
    "missing_inds = jnp.isnan(y_nan)\n",
    "_, y_miss = account_for_nans(\n",
    "    growth_factor_model(theta_manual, aux), y_nan, missing_inds\n",
    ")\n",
    "_model_miss = lambda theta, aux: account_for_nans(\n",
    "    growth_factor_model(theta, aux), y_nan, missing_inds\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dates, y_nan.sum(axis=-1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssm4epi.models.util import visualize_pgssm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "theta_manual = jnp.array([0.0, -2.063e00, -5.355e00, -4.511e-01, -5.711e-01, 7.932e-01])\n",
    "model = growth_factor_model(theta_manual, aux)\n",
    "\n",
    "visualize_pgssm(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isssm.estimation import initial_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "theta0_result = initial_theta(\n",
    "    y_miss, _model_miss, theta_manual, aux, n_iterations, options={\"maxiter\": 10}\n",
    ")\n",
    "theta0 = theta0_result.x\n",
    "theta0_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_to_par(theta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model = _model_miss(theta0, aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isssm.laplace_approximation import laplace_approximation as LA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "proposal_la, info_la = LA(y_miss, fitted_model, n_iterations)\n",
    "info_la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isssm.importance_sampling import pgssm_importance_sampling, ess_pct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "key, subkey = jrn.split(key)\n",
    "samples, lw = pgssm_importance_sampling(\n",
    "    y_miss,\n",
    "    fitted_model,\n",
    "    proposal_la.z,\n",
    "    proposal_la.Omega,\n",
    "    N_mle,\n",
    "    subkey,\n",
    ")\n",
    "\n",
    "ess_pct(lw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isssm.modified_efficient_importance_sampling import (\n",
    "    modified_efficient_importance_sampling as MEIS,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "key, subkey = jrn.split(key)\n",
    "proposal_meis, info_meis = MEIS(\n",
    "    y_miss, fitted_model, proposal_la.z, proposal_la.Omega, 10, int(1e3), subkey\n",
    ")\n",
    "info_meis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isssm.importance_sampling import pgssm_importance_sampling, ess_pct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "key, subkey = jrn.split(key)\n",
    "samples, lw = pgssm_importance_sampling(\n",
    "    y_miss,\n",
    "    fitted_model,\n",
    "    proposal_meis.z,\n",
    "    proposal_meis.Omega,\n",
    "    N_mle,\n",
    "    subkey,\n",
    ")\n",
    "\n",
    "ess_pct(lw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isssm.importance_sampling import mc_integration, prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "key, subkey = jrn.split(key)\n",
    "\n",
    "preds = prediction(\n",
    "    lambda x, s, y: jnp.concatenate([y[-1].sum()[None], y[-1]]),\n",
    "    y_miss,\n",
    "    proposal_la,\n",
    "    fitted_model,\n",
    "    1000,\n",
    "    subkey,\n",
    "    percentiles_of_interest,\n",
    "    growth_factor_model(theta0, aux),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dates_full[1 : np1 + 1], y_nan.sum(axis=-1))\n",
    "plt.scatter(dates_full[np1], preds[0][0], color=\"red\")\n",
    "plt.scatter(dates_full[np1], preds[2][1, 0], color=\"grey\")\n",
    "plt.scatter(dates_full[np1], preds[2][-2, 0], color=\"grey\")\n",
    "plt.scatter(dates_full[np1], y[-1].sum(), color=\"green\", marker=\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find index of Gütersloh / neighboring Warendorf\n",
    "jnp.where(ags_full == 5754), jnp.where(ags_full == 5570)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jrn.split(key)\n",
    "county_index = jrn.randint(subkey, (1,), 0, 400)[0]\n",
    "# Gütersloh\n",
    "# county_index = 98\n",
    "\n",
    "\n",
    "plt.title(f\"ags = {ags_full[county_index]}\")\n",
    "plt.plot(dates_full[1 : np1 + 1], y_nan[:, county_index])\n",
    "plt.scatter(dates_full[np1], preds[0][county_index + 1], color=\"red\")\n",
    "plt.scatter(dates_full[np1], preds[2][1, county_index + 1], color=\"grey\")\n",
    "plt.scatter(dates_full[np1], preds[2][-2, county_index + 1], color=\"grey\")\n",
    "plt.scatter(dates_full[np1], y[-1, county_index], color=\"green\", marker=\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
