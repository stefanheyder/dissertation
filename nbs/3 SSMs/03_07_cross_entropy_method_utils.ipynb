{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp ce_method\n",
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy as jsp\n",
    "import jax.random as jrn\n",
    "#from isssm.typing import MarkovProcessCholeskyComponents\n",
    "from jax import vmap, jit\n",
    "from isssm.importance_sampling import ess_pct\n",
    "from isssm.pgssm import log_prob as log_prob_joint\n",
    "import tensorflow_probability.substrates.jax.distributions as tfd\n",
    "from jaxtyping import Float, Array, PRNGKeyArray\n",
    "from typing import Tuple\n",
    "from isssm.importance_sampling import normalize_weights\n",
    "from isssm.util import converged\n",
    "from jax.lax import while_loop, fori_loop, scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide \n",
    "import jax\n",
    "from tensorflow_probability.substrates.jax.distributions import MultivariateNormalFullCovariance as MVN\n",
    "import fastcore.test as fct\n",
    "from isssm.pgssm import simulate_pgssm, nb_pgssm_running_example\n",
    "from isssm.laplace_approximation import laplace_approximation\n",
    "from isssm.importance_sampling import pgssm_importance_sampling\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Entropy method \n",
    "> See also the corresponding [section in my thesis](https://stefanheyder.github.io/dissertation/thesis.pdf#nameddest=subsection.3.6.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [!CAUTION]\n",
    "> this module is still under construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross entropy method [@Rubinstein1997Optimization;@Rubinstein2004CrossEntropy] is a method for determining good importance sampling proposals. Given a parametric family of proposals $g_\\theta(x)$ and target $p(x)$, the Cross-Entropy method aims at choosing $\\theta$ such that the Cross Entropy \n",
    "$$\n",
    "\\mathcal H_{\\text{CE}} \\left( p \\middle|\\middle| g_{\\theta} \\right) = \\int p(x) \\log g_{\\theta}(x) \\mathrm d x\n",
    "$$\n",
    "is maximized. This is equivalent to minimizing the Kullback Leibler divergence between $p$ and $g_\\theta$. As $H_\\text{CE}$ is not analytically available, it is approximated by importance sampling itself, usually with a suitable proposal $g_{\\hat\\theta_0}$. Then the approximate optimization problem is solved, yielding $\\hat \\theta_1$. These steps are then iterated until convergence, using common random numbers to ensure convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the Cross-Entropy method with a Gaussian proposal $g_\\theta$, we see that the optimal $\\theta$ only depends on the first and second order moments of $p$, indeed the optimal Gaussian is the one that matches these moments. Unfortunately this approach is not feasible for the models we consider in this package as the dimensionality ($n \\cdot m$) is likely too high to act on the joint distribution directly - matching means is feasible, but simulating from the distribution and evaluating the likelihood is infeasible. However, we can exploit the Markov structure of our models:\n",
    "\n",
    "For the class of state space models treated in this package, it can be shown that the smoothing distribution, the target of our inference, $p(x|y)$, is again a Markov process, see Chapter 5 in [@Chopin2020Introduction], so it makes sense to approximate this distribution with a Gaussian Markov process.\n",
    "For such a Gaussian Markov process it is known that the precision matrix $P$ of the joint distribution is sparse, indeed it is a block-tri-diagonal matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## computation of Cholesky factor $L$ of $P$\n",
    "[@Schafer2021Sparse] show that the centered Gaussian Markov process (more generally any Gaussian distribution with given sparsity) which minimizes the Kullback-Leibler divergence to a centered Gaussian with specified covariance matrix has an analytically and numerically tractable Cholesky decomposition that can be computed fast. Using this Cholesky decomposition we are able to evaluate the likelihood and simulate from this distribution.\n",
    "\n",
    "Consider states $X_t$ consisting of $m$ states $X_{t,1}, \\dots, X_{t, m}$. The $i$-th $v_{i}$ column of the KL-optimal Cholesky decomposition corresponding to $X_t$ can, by [@Schafer2021Sparse], be obtained by \n",
    "$$\n",
    "v_{i} = \\frac{1}{\\lambda_{i}}\\text{Cov} \\left(X_{t,i}, \\dots, X_{t,m}, X_{t+1, 1}, \\dots, X_{t+1, m} \\right)^{-1} e_{1}\n",
    "$$\n",
    "where $\\lambda_i$ is the square root of the first entry of the matrix vector product on the right-hand side and $e_{1}$ is the first unit vector. If $t=n$, then it suffices to replace above covariance with $\\text{Cov}\\left(X_{t,i}, \\dots, X_{t,m}\\right)$. In our setup we replace the exact covariance by an importance sampling estimate with weights $w^i$, $i = 1, \\dots, n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def transition_precision_root(cov):\n",
    "    def _iter(carry, input):\n",
    "        ei, = input\n",
    "        i, cov = carry\n",
    "\n",
    "        v = jnp.linalg.solve(cov, ei)\n",
    "        # as jitting only works when shapes are constant\n",
    "        # instead of submatrices with changing shapes, we replace\n",
    "        # entries of cov succesively with identity matrix,\n",
    "        # then, solving for ei is equvalent to soliving the i x i submatrix for e1\n",
    "        cov = cov.at[:,i].set(ei).at[i,:].set(ei)\n",
    "        return (i + 1, cov), v\n",
    "    \n",
    "    l, _ = cov.shape\n",
    "    m = int(l/2)\n",
    "    _, LT = scan(_iter, (0, cov), (jnp.eye(2 * m)[:m],))\n",
    "    L = LT.T\n",
    "\n",
    "    lam = jnp.sqrt(jnp.diag(L))\n",
    "    L = L / lam[None]\n",
    "\n",
    "    return L\n",
    "\n",
    "def final_precision_root(cov):\n",
    "    def _iter(carry, input):\n",
    "        ei, = input\n",
    "        i, cov = carry\n",
    "\n",
    "        v = jnp.linalg.solve(cov, ei)\n",
    "        # same trick as for transition_precision_root\n",
    "        cov = cov.at[:,i].set(ei).at[i,:].set(ei)\n",
    "        return (i + 1, cov), v\n",
    "    \n",
    "    m, _ = cov.shape\n",
    "    _, LT = scan(_iter, (0, cov), (jnp.eye(m),))\n",
    "\n",
    "    lam = jnp.sqrt(jnp.diag(LT))\n",
    "    L = LT.T / lam[None]\n",
    "    return L\n",
    "\n",
    "\n",
    "def ce_cholesky_block(\n",
    "    x: Float[Array, \"N m\"],  # samples of $X_t$\n",
    "    x_next: Float[Array, \"N m\"],  # samples of $X_{t+1}$\n",
    "    weights: Float[Array, \"N\"],  # $w$, need not be normalized\n",
    ") -> Float[Array, \"2*m m\"]:  # Cholesky factor\n",
    "    \"\"\"Calculate the columns and section of rows of the Cholesky factor of $P$ corresponding to $X_t$, $t < n$.\"\"\"\n",
    "\n",
    "    joint_x = jnp.concatenate([x, x_next], axis=1)\n",
    "    weights = weights / weights.sum()\n",
    "\n",
    "    #joint_mean = jnp.sum(joint_x * weights[:, None], axis=0)\n",
    "    #cov = jnp.atleast_2d(\n",
    "    #    jnp.sum(\n",
    "    #        (joint_x[:, :, None] @ joint_x[:, None, :]) * weights[:, None, None], axis=0\n",
    "    #    )\n",
    "    #    - joint_mean[:, None] @ joint_mean[None, :]\n",
    "    #)\n",
    "    cov = jnp.atleast_2d(jnp.cov(joint_x, aweights=weights, rowvar=False))\n",
    "\n",
    "    return transition_precision_root(cov)\n",
    "\n",
    "\n",
    "def ce_cholesky_last(\n",
    "    x: Float[Array, \"N m\"],  # samples of $X_n$\n",
    "    weights: Float[Array, \"N\"],  # $w$, need not be normalized\n",
    ") -> Float[Array, \"m m\"]:  # Cholesky factor\n",
    "    \"\"\"Calculate the Cholesky factor of $P$ corresponding to $X_n$.\"\"\"\n",
    "    _, m = x.shape\n",
    "    weights = weights / weights.sum()\n",
    "\n",
    "    #mean = jnp.sum(x * weights[:, None], axis=0)\n",
    "\n",
    "    #cov = jnp.atleast_2d(\n",
    "    #    jnp.sum((x[:, :, None] @ x[:, None, :]) * weights[:, None, None])\n",
    "    #    - mean @ mean.T\n",
    "    #)\n",
    "    cov = jnp.atleast_2d(jnp.cov(x, aweights=weights, rowvar=False))\n",
    "\n",
    "    return final_precision_root(cov)\n",
    "\n",
    "\n",
    "def cholesky_components(\n",
    "    samples: Float[Array, \"N n m\"],  # samples of $X_1, \\ldots, X_n$\n",
    "    weights: Float[Array, \"N\"],  # $w$, need not be normalized\n",
    ") :  # block diagonal and off-diagonal components\n",
    "    \"\"\"calculate all components of the Cholesky factor of $P$\"\"\"\n",
    "    current = samples[:, :-1]\n",
    "    next = samples[:, 1:]\n",
    "\n",
    "    diag, off_diag = jnp.split(\n",
    "        vmap(ce_cholesky_block, (1, 1, None))(current, next, weights), 2, 1\n",
    "    )\n",
    "    last_diag = ce_cholesky_last(samples[:, -1], weights)\n",
    "    full_diag = jnp.concatenate([diag, last_diag[None, :]], axis=0)\n",
    "\n",
    "    return (full_diag, off_diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## running example: AR(1) process\n",
    "\n",
    "we'll use the simples non-degenerate Gaussian Markov process as an example, an AR(1) process. We start this process in its stationary distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1/2\n",
    "s2 = 1.\n",
    "\n",
    "n = 10\n",
    "# stationary\n",
    "tau2 = s2 / (1 - a**2)\n",
    "\n",
    "cov = jnp.array([[\n",
    "    tau2 * a**(jnp.abs(i - j))\n",
    "    for j in range(n)\n",
    "] for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100000\n",
    "initial_samples = MVN(jnp.zeros(n), cov).sample((N,), seed=jrn.PRNGKey(5324523423))[...,None]\n",
    "initial_weights = jnp.ones(N)\n",
    "initial_samples.shape\n",
    "\n",
    "root_diag, root_off_diag = cholesky_components(initial_samples, initial_weights)\n",
    "P = jnp.linalg.inv(cov)\n",
    "L = jnp.diag(jnp.concatenate(root_diag)[:,0]) + jnp.diag(jnp.concatenate(root_off_diag)[:,0], -1)\n",
    "plt.title(\"$L$ has lower bi-diagonal structure\")\n",
    "plt.imshow(L)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"difference between $L L^T$ and $P$\")\n",
    "plt.imshow(L @ L.T - P)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fct.test_close(L @ L.T, P, .05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simulation using $L$\n",
    "\n",
    "As $LL^T = P = \\Sigma^{-1}$, $L^{-T}L^{-1} = \\Sigma$ so to simulate $X \\sim \\mathcal N(0, \\Sigma)$ it suffices to simulate a standard normal $Z \\sim \\mathcal N(0, I)$ and solve $L^T X = Z$. Luckily, $L^T$ is an upper block diagonal matrix which solving this system of equations straightforward. To this end we partition $X = (X_1^{T}, \\dots, X_n^{T})^{T}$ and $Z = (Z_1^{T}, \\dots, Z_n^{T})^{T}$ and $L = \\text{block-diag} \\left( L_{1,1}, \\dots, L_{n,n} \\right) + \\text{lower-block-off-diag} \\left( L_{2,1}, \\dots, L_{n, n-1} \\right)$. Starting with $X_n$, we solve\n",
    "$$\n",
    "L_{n,n}^T X_{n} = Z_{n}\n",
    "$$\n",
    "by using the fact that $L_{n,n}^T$ is an upper triangular matrix. We then iteratively solve for $i = n-1, \\dots, 1$\n",
    "$$\n",
    "L_{i,i}^TX_{i} + L_{i+1,i}^T X_{i + 1} = Z_{i},\n",
    "$$\n",
    "so\n",
    "$$\n",
    "X_{i} = L_{i,i}^{-T} \\left( Z_{i} - L_{i +1, i}^{T} X_{i + 1} \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "vsolve_t = vmap(jsp.linalg.solve_triangular, (None, 0))\n",
    "vmm = vmap(jnp.matmul, (None, 0))\n",
    "\n",
    "def simulate_backwards(z_t, x_next, diag_tt, off_diag_t_tp1):\n",
    "    return jsp.linalg.solve_triangular(diag_tt.T, z_t - off_diag_t_tp1 @ x_next)\n",
    "\n",
    "\n",
    "def simulate(\n",
    "    full_diag: Float[Array, \"n m m\"],  # block diagonals of $L$\n",
    "    off_diag: Float[Array, \"n-1 m m\"],  # off-diagonals of $L$\n",
    "    key: PRNGKeyArray,  # random key\n",
    "    N: int,  # number of samples\n",
    ") -> Float[Array, \"N n m\"]:  # $N$ samples of $X_1, \\ldots, X_n$\n",
    "    \"\"\"Simulate from Markov process with Cholesky factor $L$.\"\"\"\n",
    "    n, m, _ = full_diag.shape\n",
    "\n",
    "    vsimulate_backwards = vmap(\n",
    "        simulate_backwards,\n",
    "        (0, 0, None, None)\n",
    "    )\n",
    "\n",
    "    def _iteration(carry, input):\n",
    "        x, = carry\n",
    "        z, full_diag, off_diag = input\n",
    "\n",
    "        #new_x = vsolve_t(full_diag.T, z - vmm(off_diag, x))\n",
    "        new_x = vsimulate_backwards(z, x, full_diag, off_diag)\n",
    "\n",
    "        return (new_x,), new_x\n",
    "    \n",
    "    key, subkey = jrn.split(key)\n",
    "    extended_off_diag = jnp.concatenate([off_diag, jnp.zeros((1, m, m))], axis=0)\n",
    "    _, x = scan(\n",
    "        _iteration, \n",
    "        (jnp.zeros((N,m)),), \n",
    "        (jrn.normal(subkey, shape=(n, N, m)), full_diag[::-1], extended_off_diag[::-1])\n",
    "    )\n",
    "\n",
    "    x = x[::-1].transpose((1, 0, 2))\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## marginal distributions\n",
    "\n",
    "Rewriting the above equations used for simulation, we see that\n",
    "\n",
    "$$\n",
    "X_{i} = A_{i + 1} X_{i + 1} + \\varepsilon_{i}\n",
    "$$\n",
    "\n",
    "with $X_n \\sim \\mathcal N\\left(0, L_{n,n}^{-T}L_{n,n}^{-1}\\right)$, $A_{i + 1} = -L_{i,i}^{-T}L_{i +1, i}^{T}$ and $\\varepsilon_{i} \\sim \\mathcal N\\left(0, L_{i,i}^{-T}L_{i,i}^{-1}\\right).$\n",
    "\n",
    "From these recursions we can caluclate the marginal covariances of $X_i$ for all $i$ by iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def marginals(\n",
    "    mu: Float[Array, \"m\"],  # mean\n",
    "    full_diag: Float[Array, \"n m m\"],  # block diagonals of $L$\n",
    "    off_diag: Float[Array, \"n-1 m m\"],  # off-diagonals of $L$\n",
    "):\n",
    "    n, m, _ = full_diag.shape\n",
    "\n",
    "    Sigma = jnp.zeros((n, m, m))\n",
    "    P_n = full_diag[-1].T @ full_diag[-1]\n",
    "    Sigma = Sigma.at[-1].set(jnp.linalg.inv(P_n))\n",
    "\n",
    "    for i in reversed(range(n - 1)):\n",
    "        A_i = - jsp.linalg.solve_triangular(full_diag[i].T, off_diag[i].T)\n",
    "        Omega_i = jnp.linalg.inv(full_diag[i].T @ full_diag[i])\n",
    "\n",
    "        Sigma = Sigma.at[i].set(A_i.T @ Sigma[i + 1] @ A_i + Omega_i)\n",
    "\n",
    "    return mu, vmap(jnp.diag)(Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_true = jnp.linalg.cholesky(jnp.linalg.inv(cov)).T\n",
    "\n",
    "_,marginal_var = marginals(jnp.zeros(n), root_diag, root_off_diag)\n",
    "\n",
    "fct.test_close(marginal_var[0,0], jnp.diag(cov), .01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the density\n",
    "\n",
    "Similar to simulation, we can evaluate the log-density \n",
    "$$\n",
    "\\log p(x) = - \\frac{n\\cdot m}{2}  \\log(2  \\pi) - \\frac{1}{2} \\log \\det \\Sigma - \\frac 1 2 (x - \\mu)^{T} \\Sigma^{-1}(x-\\mu)\n",
    "$$\n",
    "efficiently using $L$.\n",
    "\n",
    "For the $\\log\\det \\Sigma$-term notice that $\\Sigma = L^{-T}L^{-1}$ results in $2 \\log\\det L = 2 \\sum_{i = 1}^n \\log\\det L_{i,i}$. These determinants are simply the product of the diagonal entries, as $L$ is a lower triangular matrix.\n",
    "\n",
    "The quadratic part can be rewritten as $\\lVert L^T(x-\\mu) \\rVert^{2}$ and, using the structure of $L^{T}$, this consists of terms of the form $L^T_{i,i} (x_i - \\mu_i) + L^T_{i+1, i}(x_{i + 1} - \\mu_{i + 1})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def log_prob(\n",
    "    x: Float[Array, \"n+1 m\"], # the location at which to evaluate the likelihood\n",
    "    full_diag: Float[Array, \"n+1 m m\"],# block diagonals of $L$\n",
    "    off_diag: Float[Array, \"n m m\"], # off-diagonals of $L$\n",
    "    mean: Float[Array, \"n+1 m\"], # mean of the process\n",
    ") -> Float: # log-likelihood\n",
    "    np1, m = x.shape\n",
    "\n",
    "    # append zeros to have the same shape as full_diag\n",
    "    extended_off_diag = jnp.concatenate([off_diag, jnp.zeros((m, m))[None]], axis=0)\n",
    "    centered = x - mean\n",
    "\n",
    "    # L is triangular\n",
    "    logdet = 2 * jnp.sum(jnp.log(vmap(jnp.diag)(full_diag)))\n",
    "\n",
    "    # exploit sparsity of L\n",
    "    extended_centered = jnp.concatenate([centered, jnp.zeros((1, m))], axis=0)\n",
    "    Lt_x = (\n",
    "        full_diag.transpose((0, 2, 1)) @ extended_centered[:-1,:,None]\n",
    "        + extended_off_diag.transpose((0, 2, 1)) @ extended_centered[1:,:,None]\n",
    "    )[:,:,0]\n",
    "    l2_norm = jnp.sum(Lt_x ** 2)\n",
    "\n",
    "    return -np1 * m / 2 * jnp.log(2 * jnp.pi) - 1 / 2 * logdet - 1 / 2 * l2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#log-det part\n",
    "actual = log_prob(jnp.zeros((10,1)), jnp.ones((10,1,1)), jnp.ones((9,1,1)), jnp.zeros((10,1)))\n",
    "expected = - 10/2 * jnp.log(2 * jnp.pi) \n",
    "fct.test_close(actual, expected)\n",
    "# l2 norm part\n",
    "actual = log_prob(jnp.ones((10,1)), jnp.ones((10,1,1)), jnp.ones((9,1,1)), jnp.zeros((10,1)))\n",
    "expected = - 10/2 * jnp.log(2 * jnp.pi) - 1/2 * (9 * 4 + 1)\n",
    "fct.test_close(actual, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def kl_optimal_rev_time_markov_process(samples):\n",
    "    N, n, m = samples.shape\n",
    "    current = samples[:,:-1]\n",
    "    next = samples[:,1:]\n",
    "\n",
    "    diag, off_diag = jnp.split(\n",
    "        vmap(ce_cholesky_block, (1, 1))(current, next),\n",
    "        2, 1\n",
    "    )\n",
    "    last_diag = ce_cholesky_last(samples[:,-1])\n",
    "    full_diag = jnp.concatenate([diag, last_diag[None,:]], axis=0)\n",
    "\n",
    "    long_eye = jnp.tile(jnp.eye(m), (n,1,1))\n",
    "    innov_chol = jsp.linalg.solve_triangular(full_diag.transpose((0,2,1)), long_eye, lower=False)\n",
    "    reverse_transitions = - innov_chol[:-1] @ jnp.linalg.solve(off_diag.transpose((0,2,1)), long_eye[:-1])\n",
    "\n",
    "    return innov_chol, reverse_transitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance Sampling\n",
    "\n",
    "For importance sampling with this model the log weights read \n",
    "$$\n",
    "\\log w(x) = \\log p(x|y) - \\log g(x) = \\log p(x,y) - \\log g(x) - \\log p(y),\n",
    "$$\n",
    "where we ignore the $\\log p(y)$ term to obtain autonormailsed importance sampling weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from isssm.typing import PGSSM\n",
    "def ce_log_weights(\n",
    "    x: Float[Array, \"n+1 m\"], # the sample\n",
    "    y: Float[Array, \"n+1 p\"], # the observations\n",
    "    full_diag: Float[Array, \"n+1 m m\"],# block diagonals of $L$\n",
    "    off_diag: Float[Array, \"n m m\"], # off-diagonals of $L$\n",
    "    mean: Float[Array, \"n+1 m\"], # mean of the process\n",
    "    model: PGSSM\n",
    ") -> Float: # log-weights\n",
    "    log_p = log_prob_joint(x, y, model)\n",
    "    log_g = log_prob(x, full_diag, off_diag, mean)\n",
    "\n",
    "    return log_p - log_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSM to Markov Model\n",
    "\n",
    "To inititialize the Cross-Entropy method, we will use the Laplace approximation, see [30_laplace_approximation.ipynb]. This approximates the true posterior by the posterior of a Gaussian state space model. To initiailze the Cross-entropy procedure, we determine the Cholesky root of this Gaussian posterior and use it as an initial value. To determine the diagonal and off-diagonal components of the Cholesky root, we calcultae the joint covariance matrix $\\text{Cov} \\left( X_t, X_{t + 1} | Y_1, \\dots, Y_n \\right)$ using the Kalman smoother and the FFBS, which results in \n",
    "$$\n",
    "\\text{Cov} \\left( X_t, X_{t + 1} | Y_1, \\dots, Y_n \\right) = \\begin{pmatrix} \n",
    "\\Xi_{t|n} & \\Xi_{t|t} A_t^T \\Xi_{t + 1|t}^{-1} \\Xi_{t + 1|n} \\\\\n",
    "\\left(\\Xi_{t|t} A_t^T \\Xi_{t + 1|t}^{-1} \\Xi_{t + 1|n} \\right)^T & \\Xi_{t + 1 | n}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Then the algorithm by [@Schafer2021Sparse] allows us efficiently determine the desired cholesky root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from jax.lax import cond\n",
    "from isssm.typing import GLSSM\n",
    "\n",
    "from isssm.kalman import kalman, smoother\n",
    "\n",
    "\n",
    "def joint_cov(Xi_smooth_t, Xi_smooth_tp1, Xi_filt_t, Xi_pred_tp1, A_t):\n",
    "    \"\"\"Joint covariance of conditional Markov process\"\"\"\n",
    "    off_diag = Xi_filt_t @ A_t.T @ jnp.linalg.solve(Xi_pred_tp1, Xi_smooth_tp1)\n",
    "    return jnp.block([[Xi_smooth_t, off_diag], [off_diag.T, Xi_smooth_tp1]])\n",
    "\n",
    "\n",
    "def forward_model_markov_process(y, model: GLSSM, time_reverse=True):\n",
    "    \"\"\"mean + Cholesky root components of precision matrix of smoothing distribution\"\"\"\n",
    "\n",
    "    x0, A, *_ = model\n",
    "\n",
    "    filtered = kalman(y, model)\n",
    "    x_filter, Xi_filter, x_pred, Xi_pred = filtered\n",
    "    x_smooth, Xi_smooth = smoother(filtered, A)\n",
    "\n",
    "    (m,) = x0.shape\n",
    "\n",
    "    # permute X_t and X_t+1\n",
    "    P = jnp.block([[jnp.zeros((m, m)), jnp.eye(m)], [jnp.eye(m), jnp.zeros((m, m))]])\n",
    "\n",
    "    covs = vmap(joint_cov)(\n",
    "        Xi_smooth[:-1], Xi_smooth[1:], Xi_filter[:-1], Xi_pred[1:], A\n",
    "    )\n",
    "\n",
    "    def forwards(x_smooth, covs, Xi_smooth):\n",
    "        return x_smooth[::-1], vmap(lambda cov: P @ cov @ P.T)(covs)[::-1], Xi_smooth[0]\n",
    "\n",
    "    def backwards(x_smooth, covs, Xi_smooth):\n",
    "        return x_smooth, covs, Xi_smooth[-1]\n",
    "\n",
    "    mu, covs, final_cov = cond(\n",
    "        time_reverse, backwards, forwards, x_smooth, covs, Xi_smooth\n",
    "    )\n",
    "\n",
    "    roots = vmap(transition_precision_root)(covs)\n",
    "\n",
    "    root_diag, root_off_diag = jnp.split(roots, 2, 1)\n",
    "    final_root = final_precision_root(final_cov)\n",
    "\n",
    "    full_diag = jnp.concatenate([root_diag, final_root[None, :]], axis=0)\n",
    "\n",
    "    return mu, (full_diag, root_off_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from isssm.kalman import FFBS\n",
    "from isssm.typing import GLSSM\n",
    "from isssm.models.glssm import ar1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "glssm_model = ar1(jnp.zeros(1), 1, 0.5, 10, 10)\n",
    "y = jnp.zeros((11,1))\n",
    "mu, (full_diag, root_off_diag) = forward_model_markov_process(y, glssm_model)\n",
    "\n",
    "N = int(1e7)\n",
    "samples = FFBS(y, glssm_model,  N, jrn.PRNGKey(1345))\n",
    "\n",
    "full_cov = jnp.cov(samples[:,:,0], rowvar=False)\n",
    "sim_L = jnp.linalg.cholesky(jnp.linalg.inv(full_cov))\n",
    "sim_full_diag = jnp.diag(sim_L)\n",
    "sim_off_diag = jnp.diag(sim_L, -1)\n",
    "\n",
    "fct.test_close(\n",
    "    sim_full_diag, full_diag[:,0,0],\n",
    "    1e-3\n",
    ")\n",
    "fct.test_close(\n",
    "    sim_off_diag, root_off_diag[:,0,0],\n",
    "    1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def ce_cholesky_precision(\n",
    "    y: Float[Array, \"n+1 p\"],  # observations\n",
    "    model: PGSSM, # the model\n",
    "    initial_mean: Float[Array, \"n+1 m\"],  # initial mean\n",
    "    initial_diag: Float[Array, \"n+1 m m\"],  # initial off_diag\n",
    "    initial_off_diag: Float[Array, \"n m m\"],  # initial off_diag\n",
    "    n_iter: int,  # number of iterations\n",
    "    N: int,  # number of samples\n",
    "    key: PRNGKeyArray,  # random key\n",
    "    eps: Float = 1e-5,  # convergence threshold\n",
    "):\n",
    "    key, subkey = jrn.split(key)\n",
    "    x0, A, Sigma, B, dist, xi = model\n",
    "\n",
    "    def _break(val):\n",
    "        i, diag, off_diag, mean, old_diag, old_off_diag, old_mean = val\n",
    "\n",
    "        diag_converged = converged(diag, old_diag, eps)\n",
    "        off_diag_converged = converged(off_diag, old_off_diag, eps)\n",
    "        mean_converged = converged(mean, old_mean, eps)\n",
    "\n",
    "        all_converged = jnp.logical_and(\n",
    "                diag_converged, jnp.logical_and(off_diag_converged, mean_converged)\n",
    "        )\n",
    "\n",
    "        is_first_iteration = i == 0\n",
    "        iteration_limit_reached = i >= n_iter\n",
    "\n",
    "        return jnp.logical_and(\n",
    "            jnp.logical_not(is_first_iteration),\n",
    "            jnp.logical_or(\n",
    "                all_converged,\n",
    "                iteration_limit_reached,\n",
    "            )\n",
    "        )\n",
    "    \n",
    "\n",
    "    def _iteration(val):\n",
    "        i, diag, off_diag, mean, _, _, _ = val\n",
    "\n",
    "        samples = simulate(diag, off_diag, subkey, N) + mean\n",
    "\n",
    "        log_weights = vmap(ce_log_weights, (0, *(None,) * 5))(\n",
    "            samples, y, diag, off_diag, mean, model\n",
    "        )\n",
    "\n",
    "        weights = normalize_weights(log_weights)\n",
    "        new_diag, new_off_diag = cholesky_components(samples, weights)\n",
    "        new_mean = jnp.sum(samples * weights[:, None, None], axis=0)\n",
    "\n",
    "        return i + 1, new_diag, new_off_diag, new_mean, diag, off_diag, mean\n",
    "\n",
    "    init = _iteration(\n",
    "        (0, initial_diag, initial_off_diag, initial_mean, initial_diag, initial_off_diag, initial_mean)\n",
    "    )\n",
    "\n",
    "    _keep_going = lambda x: jnp.logical_not(_break(x))\n",
    "    iterations, diag, off_diag, mean, *_ = while_loop(_keep_going, _iteration, init)\n",
    "\n",
    "    samples = simulate(diag, off_diag, subkey, N) + mean\n",
    "\n",
    "    log_weights = vmap(ce_log_weights, (0, *(None,) * 5))(\n",
    "        samples, y, diag, off_diag, mean, model\n",
    "    )\n",
    "\n",
    "    return (diag, off_diag, mean), (samples, log_weights), iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nb_pgssm_running_example()\n",
    "\n",
    "# simulate from model\n",
    "key = jrn.PRNGKey(51)\n",
    "key, subkey = jrn.split(key)\n",
    "N = 1\n",
    "(x_true, ), (y,) = simulate_pgssm(model, N, subkey)\n",
    "\n",
    "# laplace approximation & importance sampling\n",
    "s_init = jnp.log(y + 1.)\n",
    "x_smooth, z, Omega = laplace_approximation(y, model, s_init, 100)\n",
    "\n",
    "la_glssm = GLSSM(model.x0, model.A, model.Sigma, model.B, Omega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100000\n",
    "key, subkey = jrn.split(key)\n",
    "initial_samples, initial_log_weights = pgssm_importance_sampling(y, model, z, Omega, N, subkey)\n",
    "\n",
    "initial_weights = jnp.exp(initial_log_weights - jnp.max(initial_log_weights))\n",
    "ess_pct(initial_log_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_mean, (initial_diag, initial_off_diag) = forward_model_markov_process(z, la_glssm)\n",
    "\n",
    "key, subkey = jrn.split(key)\n",
    "\n",
    "components, (samples, log_weights), iterations =  ce_cholesky_precision(\n",
    "    y, model,\n",
    "    initial_mean, initial_diag, initial_off_diag,\n",
    "    10, N, subkey\n",
    ")\n",
    "\n",
    "ess_pct(log_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
