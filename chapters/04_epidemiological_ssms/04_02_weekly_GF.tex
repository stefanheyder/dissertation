\section{Regional growth factor model}%
\label{sec:regional_growth_factor_model}


Modeling the epidemics spread on a regional level allows us to differentiate between localized and global outbreaks, such as the one in June 2020, highlighted in \Cref{fig:cases_germany}. Additionally, regional level prediction and growth factors are of interest on their own, because \acrshortpl{npi} are enforced on the regional level. Moreover, having access to the spread on the regional level enables, e.g., regression of the growth rate against regional covariates, which in turn sheds light on which factors drive the epidemic.

Instead of modeling the number of cases per day and with delay as we did in \Cref{sec:model_reporting_delay}, we will now model the total number of cases reported within one week for every county in Germany. Here we assume that a sufficient time period has passed, i.e. several days, see \Cref{fig:reporting_delays_cases}, such that the total number of cases is known sufficiently well. This weekly approach has several advantages: First, aggregating over the weekly data gets rid of the weekday effect, at the expense of a lower time resolution. Second, if we are interested in a retrospective analysis, it is sensible to assume all cases have been reported already, so we can avoid modeling the reporting delays. 

However, modeling cases on the regional level comes with its own challenges, as we have to take care of accounting for the spatial spread, as well as an exchange between regions, cf. \Cref{sec:dessiderata}. 


% want regional level predictions / growth factors
% Toennies outbreak
% for small regions, have to consider exchange of cases as well

\subsection{Model}
Similar to the last section, we start by modeling the evolution of cases in time. We now have incidences $I_{t,r}$ reported for reporting date $t$ and region $r$, where there are a total of $R$ regions. For Germany we use the $R=400$ counties as of the Gebietsreform 2021, which united the free city Eisenach with the county of Wartburgkreis \citep{FreistaatThueringen2019Thueringer}. This has several implications for the raw data published by the \acrshort{rki}:
\begin{itemize}
    \item For data published before 30. June 2021, cases were reported separately for Eisenach and the Wartburgkreis. In pre-processing the data on the county level, we merge these two regions into a single one.
    \item The \acrshort{rki} reports cases for Berlin's districts separately. Again, we aggregate all of them into a single number of cases for Berlin. 
\end{itemize}

Again, we model the evolution of cases by 
\begin{align}
    \label{eq:log-growth-regional-model}
    \log I_{t + 1, r} \approx \log I_{t, r} + \log \rho_{t + 1, r}
\end{align}
where $\rho_{t+1, r}$ is the weekly growth factor in region $r$. Now we deviate from the previous model and model 
$$
    \log \rho_{t, r} = \overline{\log \rho}_{t} + u_{t, r},
$$
where $\overline{\log\rho}_{t}$ is the average growth rate and $u_{t,r}$ is the difference between the growth rate in region $r$ and the country wide average. 
We will model $u_{t,r}, r = 1, \dots, R$ to be jointly Gaussian, but correlated, which will enable us to model regional dependencies. To motivate our choice for the covariance structure, let us consider how cases are transferred between regions first.

As we are modeling cases on a regional level, we have to account for an exchange of cases as well. To illustrate our approach, suppose that we have for region $r$ $S^{r}$ many secondary cases generated where the primary case belongs to region $r$, but the secondary case may belong to another region $r'$. Here ``{}belonging to''{} signifies that the case is reported in that region, which means that the infectee has registered their center of living to be in this region. Denote by $p_{r,r'}$ the fraction of such cases and set $p_{r,r} = 1 - \sum_{r' \neq r} p_{r,r'}$. 

Under these assumptions, the newly reported cases in region $r$ are 
$$
    \tilde S^{r} = \sum_{r'=1}^{R} p_{r',r} S^{r'} = (P^{T}S)_{r}
$$
for $ P = \left( p_{r, r'} \right)_{r,r' = 1, \dots, R} \in \R^{R\times R}$. As $P \mathbf 1 = \mathbf 1$, $P$ is a stochastic matrix. Assuming now that $S^{r}, r= 1, \dots, R$ are random and i.i.d. with variance $\sigma^{2}_S$, we have 
$$
    \cov \left( \tilde S\right) = \cov \left( P^{T}S, P^{T}S \right) = \sigma^{2}_SP^{T}P.
$$

However, modeling the correlation of newly reported cases turns out to be difficult: the cases will surely be modeled by a Poisson or Negative Binomial distribution, so we would have to decide on a copula to introduce this dependency structure. While this is feasible in principle, we opt for an easier way. Instead of modeling correlated incidences $I_{t + 1, r}$, we model correlated growth rates $\log \rho_{t + 1, r}$, by taking $\cov \left( u_{t} \right)$ to be $\sigma^{2}_SP^{T}P$. By \Cref{eq:log-growth-regional-model}, conditional on $I_{t,r}$, this also captures regional correlation, without having to specify an involved joint distribution for the incidences. Note however, that this is just a convenient modeling choice whose aim is to replicate the covariance structure of the --- somewhat artificial --- case of having i.i.d. number of secondary cases across all regions.

As elaborated in \Cref{sec:dessiderata}, we want the regional effects $u_{t,r}$ to be both flexible, but also, in some sense, stable over time. Thus, it makes sense to model $u_{t}$ as a stationary process in time. The simplest, non-trivial, stationary process is a vector-autoregressive process 
$$
    u_{t + 1} = \alpha u_{t} + \varepsilon_{t + 1, u}
$$
where $\alpha \in (-1, 1)$ and $\varepsilon_{t + 1,u} \sim \mathcal N(0, \Gamma)$, where $\Gamma$ is a positive definite matrix. By the above discussion, we set $\Gamma = (1-\alpha^{2}) \sigma^{2}_{S}P^{T}P$ so that the stationary distribution of $u_{t}, t = 0, \dots, n$  is $\mathcal N(0, \sigma^{2}_SP^{T}P)$. 

To setup our \acrshort{ssm}, let $X_{t} = \left( \overline{\log \rho_{t + 1}}, u_{t, 1}, \dots, u_{t, R} \right)^{T} \in \R^{R + 1}$. For the observations, we let $Y_{t} = \left( I_{t, 1}, \dots, I_{t, R} \right)^{T}$, the number of cases observed in regions $1, \dots, R$ in the $t$-th week. 

We then model the number of cases at time $t + 1$ in region $r$, $I_{t+1, r}$ to follow a negative binomial distribution, conditional on the states $X_{t}$ to be
$$
    I_{t+1,r} | I_{t}, \overline{\log\rho}_{t}, u_{t,r} \sim \nbinom \left( \overline{\rho}_{t}\exp(u_{t,r})P^{T}I_t, r\right),
$$
conditionally independent. While the previous observations $I_{t}$ are now conditioned on as well, recall from our discussion in the beginning of \Cref{cha:state_space_models}, that this is not problematic.

To fully specify the model, we have to provide the transfer probabilities $p_{r,r'}$. For these, we use official data by Germany's federal employment agency on commuters. We use two datasets: the number of employees subject to social security contributions per county \citep{BundesagenturfuerArbeitStatistik2021Sozialversicherungspflichtig} and the number of commuters per county  \citep{BundesagenturfuerArbeitStatistik2024Pendleratlas}. We use the number of commuters per county from December 2022, which are no longer directly available on the homepage of the federal employment agency, but can be obtained through the Pendleratlas interface. In any case, these data are additionally available in the accompanying zenodo repository of this thesis \todo{link to it}. Notice that both datasets are dated after the Gebietsreform in June 2021. Counties are uniquely identified by the Allgemeiner Gemeinde Schlüssel (AGS), a five-digit code, which are also available in the \acrshort{rki} dataset. 

In this dataset a commuter is someone whose center of living (as registered with the German authorities) is a different county than the county of their workplace.
For two regions $r,r'$ we let $n_{r}$ be the total number of employees subject to social security contributions per county and $c_{r,r'}$ be the number of commuters from county $r$ to county $r'$, with $c_{r,r} = n_{r} - \sum_{r'\neq r} c_{r,r'}$.
From these data, we calculate $q_{r,r'} = \frac{c_{r,r'}}{n_{r}}$, the fraction of socially insured employees that have their center of life in region $r$, but are registered to work in region $r'$. 

As this is only a crude approximation to the actual exchange between regions, we let
$$
    p_{r,r'} \propto \bar q + (1- \bar q) \frac{q_{r,r'}}{\sum_{r'' \neq r} q_{r,r''} + C q_{r,r}} \phantom{......} r' \neq r
$$
where we interpret $\bar q$ as a constant socket of exchange between regions and $C \geq 1$ as an additional proportion of stay at home inhabitants that are not captured by $q_{r,r}$, e.g. elderly or children. The proportionality is owed to $\sum_{r' = 1}^R p_{r,r'} = 1$. Let us highlight some instances of $P$.  

If $\bar q = 0$ and $C = 1$, then $p_{r,r'} = q_{r,r'}$, which would distribute secondary cases according to the commuting given by the official data from Germany's federal employment agency. 

If $\bar q = 1$ or $\bar q \to 1$ with fixed $C$, $P = \frac{1}{R}J\in\R^{R\times R}$, where $J$ is the matrix with all entries equal to $1$. We can interpret this as choosing the region a secondary case is reported in as uniformly as random across all regions, regardless of the original region of the primary case. 

If $\bar q \in [0,1)$ and $C \to \infty$, then $P \to I\in\R^{R \times R}$, where $I$ is to identity matrix. In this setting all regions evolve independently of one another.
Thus, the model is flexible enough to account for these practically relevant scenarios, and interpolate between them.

Our final model is parameterized by
$$
    \theta = \left( \log \sigma^{2}_S, \operatorname{logit} \alpha, \log (C - 1), \logit \bar q, \log \sigma^{2}_{\overline{\log \rho}}, \log r\right),
$$
where we reparameterized to an unconstrained $\theta \in \R^{6}$. The model has a linear signal 
$$
    S_{t} = \left(\log \rho_{t} + u_{t,r}\right)_{r = 1, \dots, R},
$$
which makes inference fast, as the approximating \acrshort{glssm} in the \acrshort{la} and \acrshort{eis} method only requires $\mathcal O(n\,R)$ many parameters. Again, we use \acrshort{mle} to estimate $\theta$, using the methods from \Cref{sec:maximum_likelihood_estimation}.  \todo{describe setup/parameters}
% computational stuff /setup
%% N_samples, iterations, convergence criteria
%% how predictions are done: cap at n_pop per county

We aggregate the reported cases from the \acrshort{rki} on a weekly level. We orient ourselves at the ECDC's ForecastHub, whose guidelines read
\begin{quote}
    Forecast horizons should use the Epidemiological Week (EW) format, defined by the US CDC. Each week starts on Sunday and ends on Saturday. \footnote{\url{https://github.com/european-modelling-hubs/covid19-forecast-hub-europe/wiki/targets-and-horizons}}
\end{quote}
Thus, the observations $I_{t, r}$ $t=0, \dots, n$ and $r = 1, \dots, R$ consist of the number of reported cases in region $r$ that were reported from the $t$-th Sunday up until the $t$-the Saturday in the period of interest and for identification purposes we identify $t$ with the $t$-th Saturday in the period under consideration. 

\subsection{Results}
We will demonstrate the usefulness of our new model by applying it to two scenarios. First, recalling the local outbreak highlighted in \Cref{fig:cases_germany}, occurring in the middle of June 2022 \citep{Gunther2020SARSCoV2}, we show that using our model gives reasonable estimates of growth rates when incidences are low and local outbreaks can occur, which can then be used to give sensible forecasts, outperforming baseline models on the country as well as the regional level. We will use this as a \textbf{showcase} of the models capabilities, and comment on the estimated parameters, especially $P$, which accounts for cross-regional infections.
Second, we show that our model is able to give reasonable \textbf{one-week ahead forecasts} of \acrshort{c19} cases by comparing its predictive performance to that of real-time forecasts created in the ECDC's ForecastHub from April 2021 to January 2022.

\paragraph{showcase}
Similar to the last section, we start out with a motivational example on the capabilities of our model. For this, we use the local outbreak highlighted in \Cref{fig:cases_germany}, which occurred in the middle of June 2020 \citep{Gunther2020SARSCoV2}. We consider the time period from 25 April 2020\footnote{Recall that this weekly observation includes cases reported from 19 April 2020 to 25 April 2020.} up until 20 June 2020 as observations, consider the observation on 27 June 2020 to be missing, and fit the \acrshort{ssm} to this data. \Cref{fig:regional_showcase_prediction} shows the number of reported cases used to fit this model, aggregated to the whole of Germany. The local outbreak accounts for a total of $1\,413$ cases in the county of Gütersloh between 17 June 2020 and 23 June 2020 (see \citep[Figure 1]{Gunther2020SARSCoV2}), noticeable in this figure by the large jump between 13 June 2020 ($2324$ cases) and 20 June 2020 ($3872$ cases). 

\begin{figure}
    \resizebox{\textwidth}{!}{%
        \input{tikz/regional_showcase_prediction.tex}%
    }
    \caption{Incidences $I_{t,\cdot} = \sum_{r = 1}^R I_{t,r}$ in Germany in the period under consideration in the regional showcase model. We additionally show predictions (median, $50\%$ and $95\%$ prediction intervals) as shaded regions for three different approaches: the \acrshort{ssm} presented in this section and three negative binomial baselines (see text for details). The \acrshort{ssm} approach is the only approach with the actual incidences ending up within the $95\%$ prediction interval, the three baselines completely miss this target. Note that all four prediction intervals are located at 27 June 2020, but are shifted horizontally for better visibility.}
    \label{fig:regional_showcase_prediction}
\end{figure}

We fit the model using maximum likelihood estimation and present the estimated parameters in \Cref{tab:regional_showcase_theta}. 
The estimated auto-regressive parameter $\alpha$ is negative, which has can be interpreted as follows: when a region's log-growth factor is above the country-wide average at time $t$, it tends to decrease below the average in the following week. Similarly, regions with below-average growth factors tend to increase back toward the mean. This negative autocorrelation is sensible for the low-incidence setting we consider here, where large outbreaks are typically localized and temporary, rather than sustained.

The standard deviation of the average growth rate, $\sigma_{\overline{\log \rho}}$, is small compared to that of the spatial effect, $\sigma_{S}$. This is visible from \Cref{fig:regional_showcase_rho.tex} as well, where we show the predicted median growth factors (not on the log scale) for the model with a blue dashed line ($\overline{\log \rho}_{t}$) and boxplots for the median growth rates for each county (summarized by blue boxplots). Here we can see that the variation over time at the county level is much larger than that at the country level, which is also evident when examining $\log \rho_{t,r}$ directly (figures not shown).

\begin{table}
    \centering
    \input{tables/regional_showcase_theta.tex}
    \caption{Estimated parameters for the regional showcase model.}
    \label{tab:regional_showcase_theta}
\end{table}

The estimated proportion of stay at home inhabitants, $C \approx 2.4$, is remarkably close to the Germany-wide ratio of total inhabitants to employees subject to social security contributions, which is $2.5$—despite the model having no direct access to total population data. The estimated overdispersion parameter $r$ indicates significant overdispersion in the data: as explained in \Cref{ex:pois_negbinom}, the variance of a $\operatorname{NegBinom}$ distribution with mean $\mu$ and overdispersion parameter $r$ is $1 + \frac{\mu}{r}$ times larger than that of a Poisson distribution with the same mean. 

This overdispersion becomes practically significant when $\mu \geq r$, as this corresponds to at least doubling the variance compared to a Poisson model. For the date of 20 June 2020, this threshold was exceeded by 32 out of 400 counties, indicating that overdispersion was relevant for the larger counties during this period.

% comment on predictions and neg binom baselines
Having fitted the model, we can use it to obtain forecasts for the next observation, that on 27 June 2020, using the techniques detailed in \Cref{subsec:inference}. We show predictions (median, $50\%$ and $95\%$ prediction intervals) in \Cref{fig:regional_showcase_prediction}, indicated by the blue regions. As the predictions on the country level consist of those on the regional level, we additionally use the data on total inhabitants in a county, capping the number of predicted cases by the total inhabitants, that is we predict in region $r$ with samples
$$
    \min \left\{\log \rho^{i}_{T,r}I_{T-1,r}, \text{pop}_{r}\right\},
$$
where $i=1,\dots,N$, $T$ is the index for which we want to perform predictions and $\text{pop}_{r}$ is the total number of inhabitants in each region.

To assess the performance, we compare it to three simple baseline models. 

\begin{itemize}
    \item The \texttt{nbinom} model uses a $ \operatorname{NegBinom}$ distribution with mean $\mu = 6451.1$ set to the country wide growth factor from 13 June 2020 to 20 June 2020 ($\rho = 1.67$) times the total number of cases reported for Germany for 20 June 2020 ($I_{t,\cdot} = 3872$). The overdispersion parameter is estimated by fitting the model to past observations with maximum likelihood estimation, resulting in $r = 143.1$. The median and prediction intervals presented in the figure are given by the respective quantiles of this distribution.
    \item The \texttt{reg.nbinom} model uses the same strategy, but applied to observations on the county level, estimating an individual $r$ for each county. To take care of unrealisticly high growth factors, we cap all growth factors at $3$. Should the empirical growth factor be infinite or indeterminante due to the number of old cases being $0$, we set it to $1$.  The median and prediction intervals presented in the figure are obtained by simulating $10\,000$ samples from each county, independently, then taking empirical prediction intervals on the number of cases aggregated to the country level. We also apply the same capping strategy by the total number of inhabitants as for the \texttt{ssm} model.
    \item The \texttt{reg.nbinom no GL} model is the same as the \texttt{reg.nbinom} model, but with cases for the county of Gütersloh set to $0$. 
\end{itemize}

It is clear from \Cref{fig:regional_showcase_prediction} that the \texttt{ssm} approach outperforms the baseline models: it is the only model where the prediction interval contains, or is even close to the true observation. There are multiple reasons for this behavior. First, the simple \texttt{nbinom} overpredicts the number of total cases because the number of reported cases from the local outbreak in Gütersloh county dominate the country-wide development of cases. While the regional level \texttt{reg.nbinom} model is able to account for this, the empirical growth factor for Gütersloh is $\hat \rho_{T, \text{Gütersloh}} = 11.7$ with $856$ cases reported for 20 June 2020, which results in $\approx 10\,000$ cases predicted for that county alone. Recalling that we have capped the growth factor at $3$, the \texttt{reg.nbinom}  model would only predict $\approx 2\,600$ cases, which is still noticeable in the predictions. 
The best of the three baseline models --- in terms of predictive performance for this single observation --- is the \texttt{reg.nbinom no GL} model, where we have removed Gütersloh from the prediction completely. However, it still overpredicts by a relevant margin which we attribute to the fact that the same effect --- albeit less pronounced may be at work in other counties: due to small numbers of reported cases there are many large outliers in the empirical growth factors, see \Cref{fig:regional_showcase_rho.tex}. 

\begin{figure}
    \resizebox{\textwidth}{!}{%
        \input{tikz/regional_showcase_rho.tex}%
    }
    \caption{Growth factors for the showcase model estimated by the \acrshort{ssm} approach (blue boxplots and lines) and the empirical approach (red boxplots and lines). For the $400$ counties in Germany, we summarize the estimated growth factors with boxplots. The top of the y-axis is cutoff at $\rho = 4$ omitting the many outliers produced by both approaches, which is due to the overall small number of reported cases. The estimates of the \acrshort{ssm} approach tend to be more concentrated than those based on the empirical approach, as the \acrshort{ssm} approach regularizes the (log-)growth rates both in time an space. While the country level estimates (shown by dashed lines) mostly agree, they diverge at the end of the observational period, as the local outbreak \citep{Gunther2020SARSCoV2} has less of an impact in the \acrshort{ssm} approach.}
    \label{fig:regional_showcase_rho.tex}
\end{figure}

Another advantage of the \texttt{ssm} model are the regional and country level growth factors we obtain. In \Cref{fig:regional_showcase_rho.tex} we show median estimates of these growth factors, together with their empirical counterparts, i.e. the growth factors used in the \texttt{nbinom} and \texttt{reg.nbinom} baseline models (without the cap at $\rho = 3$ introduced there). 
In this figure, we observe that the country level estimates of the growth factor are smaller for the \acrshort{ssm} approach, compared to the empirical one. The reason for this is that $\overline{\log \rho}_{t}$ is not the logarithm of a country wide growth factor, but the mean (on the log-scale) of all regional growth factors, so by Jensen's inequality it has to be smaller than the country level growth factor. Its estimate is close to the median growth factor on the county level, indicated by the bar in the blue boxplots.  For the county level growth factors in this figure, we additionally observe less variation of growth factors across counties due to the regularization our model imposes.

Finally, we can interpret the entries in $P$, which informs both the redistribution of cases between regions, and the covariance of regional effects $u_{t}$. As $P$ is a $400\times 400$ matrix, we focus only on a small section of it: the counties in the federal state of Saxony. Saxony is interesting because it contains two large cities (``Leipzig, Stadt'' and ``Dresden, Stadt'') with commuters commuting from the surrounding area to these cities. In the fitted model, this results in a larger row-sum for the indices associated with these cities, see the right-hand side of \Cref{fig:P_matrix_saxony.tex}.

\begin{figure}
    \resizebox{\textwidth}{!}{%
        \input{tikz/P_matrix_saxony.tex}%
    }
    \caption{We show part of the estimated $P$ for the showcase example for Saxonian counties. On the left-hand side we present $p_{r,r'}$ for $r,r'$ in Saxony. As the diagonal elements are much larger than the off-diagonal ones, we only apply the color gradient to the off-diagonal elements. The right-hand shows the proportion of in-county infections $p_{r',r'}$ (blue bars) and out-of-county infections $\sum_{r \neq r'} p_{r,r'}$ (red bars). For the two big cities, ``Leipzig, Stadt'' and ``Dresden, Stadt'', we see a larger influx of cases, compared to the more rural regions, e.g. ``Görlitz''.} 
    \label{fig:P_matrix_saxony.tex}
\end{figure}

% results:
% WIS across all counties compare to poisson and nb baseline in that week (see talk at Karlsruhe)

\paragraph{one-week ahead forecasts}

In the last section we have seen that our regional \acrshort{ssm} provides accurate forecasts in a low incidence setting. We now want to investigate if that holds true in higher incidence settings.
To that end, we will compare the predictions made by our model to that of some models that participated in the \acrlong{gfch} \citep{Bracher2021Preregistered,Bracher2022National} and \acrlong{efch} \citep{Sherratt2023Predictive}, collaborative forecasting challenges for \acrshort{c19} cases, deaths and hospitalizations.
From 10 October 2020, the \gls{gfch} gathered weekly real-time forecasts for cases and deaths in Germany and Poland on the national and sub-national level. The participating teams were required to submit their forecast in a standardized format, providing selected quantiles from their predictive distributions. To evaluate the predictive performance of our model, we compare it to three models submitted to the Hub:

\begin{itemize}
    \item The \texttt{baseline} model takes the number of reported cases for this week as next weeks median, and adds uncertainty based on the past behavior of the epidemic.
    \item The \texttt{ensemble} model produces forecasts by averaging all submission to the \acrlong{gfch}, i.e. for each predictive quantile of one-week ahead cases, its predictive quantile is the average of all submissions for the same day, target and quantile, see \citep{Sherratt2023Predictive} for details.
    \item The \texttt{itww} model makes predictions based on a county-level reproduction number model, with forecasts based on repeated simulations from the renewal equation \Cref{eq:renewal-model}. See \citep{Heyder2022Regional} for details.
\end{itemize}

We chose to focus on models submitted to the \acrshort{gfch} instead of the \acrshort{efch} for two reasons. 
First, the \acrshort{efch} uses the JHU \acrshort{c19} data \citep{Dong2020} as truth, rather than the \acrshort{rki} data. The \acrshort{gfch} uses a dataset curated by the \acrshort{ecdc} instead, which is closer to the \acrshort{rki} as it is based on the TESSy reports from the \acrshort{rki}. Second, as the \acrshort{gfch} was explicitly gathering forecasts for Germany (and Poland), we expect the models submitted to it to create more accurate forecasts than the ones submitted to the \acrshort{efch}. 

% switch KIT -> ECDC
For the \texttt{ensemble} model we use a combination of the \texttt{KITCOVIDhub-median-baseline} model, submitted to the \acrshort{gfch} and the \texttt{EuroCOVIDhub-ensemble} model, submitted to the \acrshort{efch}: Starting with 10 April 2021, the \texttt{KITCOVIDhub-median-baseline} only aggregates subnational forecasts, not national ones. Instead the \texttt{EuroCOVIDhub-ensemble} is used in the \acrshort{gfch} from that date on, and so we do the same. The switch is indicated in \Cref{fig:regional_forecasts_comparison.tex} by a vertical line. 

To obtain the predictions for the \texttt{ssm} model, we aim at replicating the setup of the ForecastHubs as closely as possible. While the Hubs were active, forecasts could be submitted each Monday to predict the number of newly reported cases between the last Saturday and the next. For each forecast --- e.g. the first made for 17 October 2020 --- we use as observations the number of weekly reported cases that were available the Monday before --- e.g. on 12 October 2020 --- as reported by the \acrshort{rki} on that day. Of those, we use the last 9 weeks of weekly reported cases as observations and mark the last observation, that for the date to be forecasted, as missing. 

To fit the model, we use an abbreviated fitting and prediction procedure: we use only the initial guess of the parameter, $\hat\theta_{0}$ (\Cref{alg:mle}) which is obtained by at most $10$ iterations of a gradient descent procedure. Using this parameter, we obtain the \acrshort{la} (\Cref{alg:la}) of the posterior distribution and produce forecasts of the last, missing observation by using $1000$ samples from the \acrshort{la}, using the approach detailed in \Cref{subsec:inference}. The whole procedure can run in approximately 10 minutes for a single forecast on a consumer grade Macbook Air M3 with 16GB of RAM. 

To evaluate the quality of the forecasts, we use a the true value the \acrshort{rki} used throughout this thesis, i.e. the dataset published on 5 May 2023, see \Cref{subsec:incidence_and_death_data}. 

\begin{figure}
    \resizebox{\textwidth}{!}{%
        \input{tikz/regional_forecasts_comparison.tex}%
    }
    \caption{Comparison of the $67$ one-week ahead forecasts (showing median, 50\% and 95\% prediction intervals) produced by four different model: three from the ForecastHubs (\texttt{baseline}, \texttt{ensemble}, \texttt{itww}) and our novel \texttt{ssm} approach. Note the log-scale of the y-axis. The minor and major x-axis gridlines correspond to the forecasting dates. For the \texttt{ssm} approach, each forecast is obtained by a single run of the model, with the observations to be forecasts missing. The true reported cases after one week are indicated by a dashed line. Notice the log-scale of the $y$-axis. While the prediction intervals provided by the \texttt{baseline} and \texttt{itww} models occasionally miss the true reported cases, the \texttt{ensemble} and \texttt{ssm} approach provides more adequate coverage, see also \Cref{tab:regional_forecasts_combined_metrics} for a quantitative comparison of the four approaches.}

    \label{fig:regional_forecasts_comparison.tex}
\end{figure}

We show median, $50\%$ and $95\%$ prediction intervals of all four models in \Cref{fig:regional_forecasts_comparison.tex}. 
For the \texttt{baseline} model, we see that carrying forward the last observation results in over- or underpredctions in times of decline or growth. The \texttt{ensemble} model provides prediction intervals of medium size, and has close to nominal coverage (see \Cref{tab:regional_forecasts_combined_metrics}). The \texttt{itww} model has very narrow prediction intervals, which often miss the true reported cases completely, resulting in overall bad performance. 
Our novel \texttt{ssm} approach has much wider prediction intervals, but also provides quite precise point predictions. 

To make the comparison more rigorous, we evaluate the predictions made by the four models by several forecasting performance metrics, as shown in \Cref{tab:regional_forecasts_combined_metrics}. We use three different metrics: 

\begin{itemize}
    \item the \acrlong{mae}, where we take the median of the predictive distribution as a point forecast and average the absolute error between this point forecast and the \acrshort{rki} truth over all $67$ observations,
    \item the \acrlong{wis}\citep{Bracher2021Evaluating}, a proper scoring rule which approximates the continuous ranked probability score. It takes into account the whole predictive distribution, i.e. the centered prediction intervals provided in the forecasts. Lower values of WIS are better, and WIS can be decomposed into components measuring over- and underprediction as well as sharpness (length of predictive intervals). Finally, we also report
    \item the empirical coverage of $50\%$ and $95\%$ prediction intervals. 
\end{itemize}

Additionally, we report the decomposition of the \acrshort{wis} into its four components and the \acrshort{mae} and \acrshort{wis} relative to the baseline model, to quickly identify which of the three other models outperforms the baseline model. 

\begin{table}
    \centering
    \input{tables/regional_forecasts_combined_metrics.tex}
    \caption{Quantitative comparison of the four approaches to one-week ahead forecasts. All metrics are averaged across the $67$ one-week ahead forecasts made by each model. To be consistent with the \gls{wis} we use the median as the point forecast for the \gls{mae}. For the \gls{wis} we show additional the decomposition into four components and the relative performance, when compared to the \texttt{baseline}, of the \texttt{ensemble}, \texttt{itww} and \texttt{ssm}. }
    \label{tab:regional_forecasts_combined_metrics}
\end{table}

For the \acrshort{mae}, we see that the \texttt{ensemble} model obtains the best performance, followed closely by the \texttt{ssm} model, then the \texttt{baseline} model and finally, worse than the \texttt{baseline}, the \texttt{itww} model. While the \acrshort{mae} is dominated by the errors in periods of high incidence, we follow the argument of \citep[Section 5]{Bracher2021Evaluating}, which encourages the use of the \acrshort{mae} over relative performance measures, such as the mean absolute percentage error (MAPE), as using the \acrshort{mae} as a performance metric incentivizes models to report predictive medians. 
Additionally, the \acrshort{mae} is one of the components in the decomposition of the \acrshort{wis}, making the two metrics compatible with one another. 

For the \acrshort{wis}, we observe a similar ranking as for the \acrshort{mae}, with a slight difference: the \texttt{ensemble} model performs better relative to the baseline for \acrshort{wis} and the \texttt{ssm} model performs slightly worse. However, such comparisons should be treated carefully, as none of the measures reported here come with uncertainties attached and are based on only $67$ forecasts. 

As for the decomposition of the \acrshort{wis} into its four components, we see that for all models, except the \texttt{itww} model, substantial contributions come from the sharpness component, which penalizes large prediction intervals. For the \texttt{ssm} model, it is by far the largest component of the \acrshort{wis}, while for the \texttt{baseline} model underprediction (due to the long increase at the end of the forecasting period) and for the \texttt{ensemble} model overprediction are on a comparable level. 
The \texttt{itww} model has a high \acrshort{wis} largely due to overprediction and, to lesser extent, underprediction. 

For the empirical coverage of the prediction intervals, we observe similiar results: the best calibration of the $50\%$ and $95\%$ prediction intervals is achieved by the \texttt{ensemble} model, while the \texttt{baseline} model prodcues too narrow $50\%$ prediction intervals. The \texttt{itww} model produces too narrow forecasts, and the \texttt{ssm} model too wide ones. 

From this comparison we can speculate that the \texttt{ssm} model would benefit from slight rescaling of the prediction intervals, while the \texttt{itww} model could benefit from larger prediction intervals. 

As expected, the \texttt{ensemble} model outperforms all models --- this is consistent with the results from almost all such forecasting hubs, e.g. \citep{Bracher2021Comparison,Bracher2021Preregistered,Bracher2022National,Wolffram2023Collaborative,Sherratt2022Predictive,Ray2020Ensemble}. However, the \texttt{ssm} model is a close second, except for its somewhat large uncertainty intervals. In this analysis it should be mentioned that the model is in no way calibrated to perform accurate forecasts, i.e. we did not perform model selection (e.g. by Akaike's Information Criterion) or include predictive performance in the parameter fitting procedure.

\subsection{Discussion}

% model produces sensible reconstructions, allows for interpretation of parameters and produces well performing forecasts against simple regional baseline models as well as models
The \acrshort{ssm} we have introduced in this section allows making sensible reconstructions, interpretation of parameters and produces well performing forecasts for both low and high incidence scenarios. By including an exchange of cases between different regions, our regional model is able to handle scenarios of low incidences gracefully. The main strength of our novel model is its balance between model complexity necessary to account for the regional level effects and interpretability, while still retaining noteworthy predictive performance. By focusing on weekly aggregation of cases, we are able to ignore weekday effects, which is well suited for practical applications, such as the \gls{gfch}. The price we have to pay for this is the loss of daily effects, such as the daily growth factor and reporting delays we used in \Cref{sec:model_reporting_delay}. 

Let us reiterate that our forecasting setup aims at replicating the available information during the real-time forecasts performed for the \gls{gfch} as closely as possible, to make the evaluation of the predictive performance valid. Indeed, our model even uses less information than is available, as the weekly aggregation ignores all cases reported on Sunday and Monday. 

Our model also comes with some disadvantages: the dimension of $m=401$ states is quite high, which makes fitting the model more costly than the model presented in \Cref{sec:model_reporting_delay,sec:nowcasting_hospitalizations}, in fact so costly that we restrict ourselves to only $10$ time points. Estimating the MLE $\hat\theta$ with $N=10\,000$ samples for the showcase model takes 3 hours on a consumer grade Macbook Air M3, while obtaining the initial estimate of $\theta$, $\hat\theta_{0}$ is faster, taking only around $5$ minutes on the same computer. Let us note that for predictive performance in the one-week ahead forecasts, we have not found a practical difference between the two fitted models, so we opted for the cheaper $\hat\theta_{0}$. 

% advantages

% caveats

%% static P: does not allow for change over time, while possible will make model much more involved
%% maybe fine for the small time horizons considered here, but plots of parameters over the 67 prediction models shows change in C and q across time

%% model does not include age stratification, which makes predictions of downstream severe outcomes (hosptiliazaiton, death) harder, as they are largely age dependent

%% problem with the model: data used to inform model is probably not totally accurate (home office, different ways of commuting not captured)

%% comparison to real time forecasting not totally valid: hindsight bias, not preregistered 

% literature
% other approaches to regional predictions
\todo{consider Armbruster2024Networkbased, Armillotta2023Inference}

