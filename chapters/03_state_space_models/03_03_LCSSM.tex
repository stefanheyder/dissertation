\section{Partially Gaussian state space models}
\label{sec:logconcave_gaussian_state_space_models}

For the applications considered in this thesis the distribution of observations is never Gaussian --- see \Cref{sec:dessiderata} --- and  all we can hope for is that the data-generating mechanism is close enough to a Gaussian distribution that inferences made in a \acrshort{glssm} may carry over.
For epidemiological models, Gaussian distributions may be appropriate if incidences are high, e.g. during large outbreaks in a whole country. 
When case numbers are small, the discrete nature of incidences is better captured by a distribution on $\mathbf N_{0}$, and standard distributions used are the Poisson and negative binomial distributions, see e.g. \citep{Lloyd-Smith2005Superspreadinga}.
We thus want \acrshortpl{ssm} where observations are allowed to follow these non-Gaussian distributions. 

% argue for keeping states linear and Gaussian
Concerning the distribution of states, we keep the linear Gaussian assumption, i.e. \Cref{eq:glssm_states}. As demonstrated in \Cref{cha:analysis_of_selected_models}, using Gaussian states and transitions allows for flexible modeling of many epidemiological desiderata. Furthermore, keeping the states Gaussian will enable us to use \acrfull{eis} effectively, by constructing approximations via \acrshort{glssm} which possess the same state dynamics. Alternatively, t-distributed innovations or more general transition kernels could be employed and we refer the interested reader to \citep[Part II]{Durbin2012Time} for a selection of these models. The following definition is that of \citep{Koopman2019Modified}, which itself is an extension of earlier work of \citep{Shephard1994Partial}. \citep{Shephard1994Partial} considered only \acrshortpl{ssm} where, conditional on another Markov process $Z = (Z_{t})_{t = 0, \dots, n}$, model is a full \acrshort{glssm}, which allows for efficient inference if the conditional distribution $Z| (X, Y)$ is tractable. As their definition involves a conditional \acrshort{glssm}, the observations still take values in $\R^{p}$, not $\N^{p}$ as is necessary for our endeavors. Thus we opt for the definition presented in \citep{Koopman2019Modified}, where we replace the Gaussian observations (\Cref{eq:glssm_observations}) with arbitrary distributions.

\glsreset{pgssm}
\begin{definition}[\gls{pgssm}]
    A \acrfull{pgssm} is a joint distribution for $(X,Y)$ where states $X$ follow \Cref{eq:glssm_states}, i.e. 
    \begin{align*}
        X_{t + 1}  &= A_{t}X_{t} + u_{t} + \varepsilon_{t + 1} &  & t = 0, \dots, n - 1,
    \end{align*}
    with $X_{0} \sim \mathcal N(0, \Sigma_{0})$, $\varepsilon_{t} \sim \mathcal N (0, \Sigma_{t}), u_{t} \in \R^{m}$ for $t = 1, \dots, n$ and $X_{0}$, $(\varepsilon_{t})_{t = 1, \dots, n}$ jointly independent. 

    Furthermore, the observations $Y$ form a conditional Markov process, conditional on states $X$, where the conditional densities of observations, given states admit the following form
    $$
    p(y | x) = \prod_{t = 0}^n p(y_{t} | x_{t}, y_{t - 1}),
    $$
    with respect to the dominating measure $\bigotimes_{t = 0}^n\mu_{\mathcal Y}$.
    Here $p(y_{t} | x_{t}, y_{t -1})$ are allowed to take any arbitrary density\footnote{Recall that we have not specified $\mu_{\mathcal Y}$, so it is always possible to use $p = \mathbf 1 _{\mathcal Y}$, the constant function.}.
\end{definition}

It is straightforward to check that a \acrshort{pgssm} is indeed a \acrshort{ssm}. 

\begin{remark}
    Recalling \Cref{rem:dependence_Yt-1}, if our main interest lies in the conditional distribution $X|Y = y$ for a fixed set of observations $y$, it will suffice to consider models where 
    $$
    p(y | x) = \prod_{t = 0}^n p(y_{t} | x_{t})
    $$
    holds, and we will do so in the following to enhance readability. %At points where this distinction matters, e.g. \todo{add example}, we will give appropriate remarks.
\end{remark}

Both the Poisson and negative binomial distribution belong to the class of exponential family distributions. As such, their densities have a convenient structure, allowing only for a linear interaction between the natural parameter and the densities' argument. We refer to \citep{Brown1986Fundamentals} for a comprehensive treatment of exponential families and use their definitions throughout this section.

\begin{definition}[exponential family]
    Let $\mu$ be a $\sigma$-finite measure on $\R^{p}$ and denote by 
    $$\Psi = \left\{\psi \in \R^{p} : \int \exp \left( \psi^{T} y \right) \d\mu(y) < \infty\right\}$$
    the set of parameters $\psi$ such that the moment-generating function of $\mu$ is finite. 
    For every $\psi \in \Psi$ $$p_{\psi}(y) = Z(\psi)^{-1} \exp (\psi^{T} y)$$ defines a probability density with respect to the measure $\mu$, where $$Z(\psi) = \int \exp \left( \psi^{T} x \right) \d\mu(y)$$ is the normalizing constant. 
    We call both the densities $p_{\psi}$ and induced probability measures $$ \P_{\psi} (A) = \int_{A} p_{\psi}(y) \d \mu(y),$$ for measurable $A \subset \R^{p}$, a \textbf{standard exponential family}.

    Conversely, let $\P_{\psi}, \psi \in \Psi$ be a given parametric family of probability measures on some space $\mathcal Y$ that is absolutely continuous with respect to a common dominating measure $\mu$. Suppose there exist a reparametrization $\eta : \Psi \to \R^{p}$, a statistic $T: \mathcal Y \to \R^{p}$ and functions $Z: \Psi\to \R$, $h:\mathcal Y \to \R$, such that
    $$
        p_{\psi}(y) = \frac{\d \P_{\psi}}{\d \mu} = Z(\psi) h(y) \exp \left(\eta(\psi)^{T}T(y)\right),
    $$
    then we call $\left(\P_{\psi}\right)_{\psi \in \Psi} $ and $\left(p_{\psi}\right)_{ \psi \in \Psi}$ a \textbf{$p$-dimensional exponential family}. If $\eta(\psi) = \psi$ is the identity, we call $\psi$ the natural parameter. If $T(y) = y$, we call $y$ the natural observation. If $\psi$ is the natural parameter and $y$ the natural observation, we call $\left( \P_{\psi} \right)_{\psi \in \Psi}$ a \textbf{natural exponential family}. By reparametrization (in $\psi$) and sufficiency (in $y$) every $p$-dimensional exponential family can be written as an equivalent standard exponential family, see the elaborations in \citep[Chapter 1]{Brown1986Fundamentals}.
    % potentially more: natural parameter/statistic/observation, regular, full, minimal, convex support
\end{definition}

Exponential families have the attractive property that they are log-concave in their parameters. As such the Fisher-information is always positive semidefinite, which will be crucial in defining surrogate Gaussian models in \Cref{sec:gaussian_importance_sampling_for_state_space_models}.
\begin{lemma}[log-concavity of exponential family distributions]
    \label{lem:log-concavity}
    Let $\left(p_{\psi}\right)_{\psi \in \Psi}$ be a natural $p$-dimensional exponential family and $\Psi$ convex and open in $\R^{p}$. In this case $\psi \mapsto \log p_{\psi}(y)$ is concave for every $y \in \R^{p}$.
\end{lemma}

\begin{proof}
    As $\log p_{\psi}(y) = - \log Z(\psi) + \psi^{T} y$ it suffices to show that $\psi \mapsto \log Z(\psi)$ is convex. However, 
    $$\psi \mapsto \log Z(\psi) = \log \int \exp \left( \psi^{T}y \right) \mathrm d \mu(y)$$ is the cumulant generating function of the base measure $\mu$, which is convex \citep[p. 144f]{Billingsley1995Probabilitya}.
\end{proof}

Additionally, the moment generating function $\psi \mapsto Z(\psi)$ is smooth on the interior of $\Psi$ and allows to switch the order of integration and differentiation.
\begin{theorem}[{\citep[Theorem 2.2, Corollary 2.3]{Brown1986Fundamentals}}]
    \label{thm:logZsmooth}
    Let $\psi \in \operatorname{int} \Psi$ be an interior point. Then the moment generating function $Z: \Psi \to \R$ is infinitely often differentiable with derivatives 
    $$
        \frac{\partial^{\lvert \alpha \rvert}}{\partial^{\alpha}\psi} Z(\psi) = \int y^{\alpha} \exp \left( \psi^{T} y \right) \d \mu(y)
    $$
    for any multi-index $\alpha \in \N^{k}$.

    Additionally, the gradient of $\log Z$, $\nabla_{\psi} \log Z(\psi)$ is given by
    $$
        \nabla_{\psi} \log Z(\psi) = \E T(X),
    $$
    and the Hessian of $\log Z$, $H_{\psi} \log Z(\psi)$ by
    $$
        H_{\psi} \log Z(\psi) = \cov (T(X)),
    $$
    where $X \sim \P_{\psi}$.
    
\end{theorem}

\begin{example}[Poisson \& negative binomial distribution]
    \label{ex:pois_negbinom}
    Both the family of Poisson distributions, parameterized by rate $\lambda$ and the negative binomial distribution, parameterized by success probability $q$ with fixed overdispersion $r$ form an exponential family.

    The log-density of the Poisson distribution with rate $\lambda$, $\operatorname{Pois} (\lambda)$ w.r.t. the counting measure on $\N_{0}$ is 
    $$
    \log p_{\lambda} (x) = -\lambda + x\log \lambda - \log x!.
    $$
    Thus the Poisson distribution forms an exponential family with natural parameter $\log \lambda$, natural statistic $\id$ (the identity), base measure $h(x) = \frac{1}{x!}$ and moment-generating function function $Z(\lambda) = \exp \left( -\lambda \right)$. 

    The log-density of the negative binomial distribution with overdispersion parameter $r$ and success probability $p$ $\operatorname{NegBinom} \left( q, r \right)$ is 
    $$
    \log p_{q,r}(x) = \log \binom{x + r - 1}{x} + x \log (1 - q) + r \log q.
    $$
    For fixed $r$ these distributions form an exponential family with natural parameter $\log (1 - q )$, natural statistic $T = \id$, base measure $h(x) = \binom{x + r - 1}{x}$ and moment-generating function $Z(q) = r \log q$. 

    In this parametrization the mean of the $\operatorname{NegBinom}(q,r)$ distribution is $\mu = r \frac{1 - q}{q}$ and its variance is $r \frac{1 - q}{q^{2}}$. An alternative parametrization that will become useful \Cref{cha:analysis_of_selected_models} is that by the log mean $\xi = \log \mu$ and overdisperision $r$. As $q = \frac{r}{r + \mu}$, this parametrization has log-density
    $$
    \log p_{r,\xi}(x) = \log \binom{x + r - 1}{x} + x \xi  - (r + x) \log (\exp \xi + r) - r \log r,
    $$
    which does not form a natural exponential family. However, it retains the log-concavity of \Cref{lem:log-concavity}, as a quick calculation reveals that 
    $$
    \partial_{\xi^{2}}^{2} \log p_{r,\xi} (x) = -(r + x) \frac{r \exp (-\xi)}{(r \exp (-\xi) + 1)^{2}} < 0
    $$
    for all $x \in \N_{0}$. 
\end{example}


The models we study in \Cref{cha:analysis_of_selected_models} belong, for the most part, to the following subclass of \acrshort{pgssm} models.
\glsreset{egssm}
\begin{definition}[\gls{egssm}]
    \label{def:egssm}
    An \acrfull{egssm} is a \acrshort{pgssm} where the conditional distribution of $Y_{t}$ given $X_{t}$ comes from an exponential family with respect to a base measure $\mu_{t}$, i.e.
    $$
    p (y_{t}|x_{t}) = h_{t}(y_{t}) Z_{t}(x_{t}) \exp \left( \eta_{t}(x_{t})^{T} T_{t}(y_{t}) \right)
    $$
    for suitable functions $h_{t}, Z_{t}, \eta_{t}, T_{t}$. If $Y_{t}$ in the \acrshort{pgssm} is allowed to depend on the previous $Y_{t - 1}$, the functions $h_{t}, Z_{t}, \eta_{t}$ and $T_{t}$ may depend on $y_{t - 1}$. 

    If, additionally, matrices $B_{t} \in \R^{p \times m}$ exist, such that for the signal $S_{t} = B_{t}X_{t} \in \R^{p}$, $Y_{t}$ only depends on $X_{t}$ through $S_{t}$, i.e. it holds
    $$
    p(y_{t}|x_{t}) = \prod_{i = 1}^p h^{i}_{t}(y^{i}_{t})\, Z^{i}_{t} (s_{t})\, \exp \left( \eta^{i}_{t} (s^{i}_{t})\,T(y^{i}_{t}) \right),
    $$
    for functions $h_{t}^{i}: \R \to \R, Z^{i}_{t}: \R \to \R, \eta^{i}_{t}: \R\to\R, T: \R\to\R$, $i = 1, \dots p$, we say the \gls{egssm} has a \textbf{linear signal}, similar to the treatment in \citep[Part II]{Durbin2012Time}.
\end{definition}

\begin{remark}
    To simplify notation we will usually assume that the functions $h, Z$ and $T$ are the same for all $t$ (and $i$, if the \gls{egssm} has a linear signal) and drop in our notation the dependence of $h$, $Z$, and $T$ on $t$ (and $i$). Similarly, we assume that the base measure $\mu_t$ is the same for all $t$.
\end{remark}

From \Cref{lem:log-concavity}, we immediately obtain the following results \citep[Section 10.6.4]{Durbin2012Time}
\begin{lemma}[log-concavity of the smoothing distribution]
    Consider an \acrshort{egssm}, where $\eta_{t} = \id$ for all $t$.  Then $x \mapsto \log p(x|y)$ is concave for $\mu_{\mathcal Y}$-a.e. $y$. 
\end{lemma}
\begin{proof}
    We may write
    $$
    \log p(x|y) = \log p(y|x) + \log p(x) - \log p(y),
    $$
    where the last term does not depend on $x$. $\log p(x)$ is concave in $x$, as $p(x)$ is the joint density of a multivariate Gaussian distribution. Furthermore 
    $$
    \log p(y | x) = \sum_{t = 0}^n \log p(y_{t} | x_{t}, y_{t - 1}),
    $$
    which, by \Cref{lem:log-concavity} is concave in $x$. 
\end{proof}

Notice that the dependence of $Y_{t}$ on $Y_{t - 1}$ does not influence the statement of this lemma, as we are interested in properties of $x \mapsto p(x|y)$.

As in the previous chapter, after having observed $Y$, one is interested in the conditional distribution of states $X$, given $Y$. If the observations are not Gaussian, this is a difficult task as the distribution is not analytically tractable. Instead, approximations, e.g. the \gls{la}, which will exploit the log-concavity developed here or simulation-based inference, e.g. importance sampling (\Cref{sec:importance_sampling,sec:gaussian_importance_sampling_for_state_space_models}), sequential Monte Carlo \citep{Chopin2020Introduction} or MCMC-methods \citep{Brooks2011Handbook} are used. Similarly, fitting hyperparameters $\psi$ by maximum likelihood inference becomes more difficult as evaluating $\ell(\psi) = p(y) = \int p(x,y) \d x$ is not analytically available, thus requiring numerical or simulation methods for evaluation and gradient descent or EM-techniques for optimization, see \Cref{sec:maximum_likelihood_estimation}.

In this thesis, we will focus on importance sampling methods, which are the focus of the next section.