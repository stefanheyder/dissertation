\glsreset{eis}
\subsection{\texorpdfstring{\Acrfull{eis}}{Efficient importance sampling}}
\label{subsec:eis}
\gls{eis} \citep{Richard2007Efficient} provides an alternative to the \gls{cem}. Instead of minimizing the \gls{kld} between the target $\P$ and proposal $\G_{\psi}, \psi \in \Psi$, \gls{eis} aims at minimizing the variance of the logarithm of importance sampling weights. 
Our discussion of \citep{Chatterjee2018Sample}, \Cref{thm:chatterje2018Thm1}, especially \Cref{lem:bounded-log-variance}, suggests that this is worthwhile. 
Thus, \acrshort{eis} finds $\peis$ which is a feasible solution to the following optimization problem
\begin{align}
    \label{eq:eis_optim}
\min_{\psi \in \Psi} \text{Var}_{\P} \left[ \log w_{\psi} \right] = \min_{\psi \in \Psi} \P \left[ \log w_{\psi} - \P [\log w_{\psi}] \right]^{2},
\end{align}
where, as in the last section, $\log w_{\psi} = \log p - \log g_{\psi}$.

It is easy to see that we can replace the normalized weights in \Cref{eq:eis_optim} by unnormalized weights $\tilde w_{\psi} = \frac{p}{g_{\psi}}$, as the normalization constant cancels out, so no access to the normalized weights is required.
However, $\P [\log w_{\psi}] = \Dkl{\P}{\G_{\psi}}$ is usually intractable. This can be dealt with by introducing the nuisance parameter $\lambda = \P \left[ \log \tilde w_{\psi} \right]$, utilizing the fact that the mean is the minimizer of the squared distance functional with the minimum value equal to the variance, should it exist. Indeed,
$$
    \log w_{\psi} - \P [\log w_{\psi}] = \log \tilde w_{\psi} - \log \G_{\psi} [\tilde w_{\psi}] - \P \left[ \log \tilde w_{\psi} \right] + \log \G_{\psi}[\tilde w_{\psi}] = \log \tilde w_{\psi} - \P \left[ \log \tilde w_{\psi} \right],
$$
so
$$
    \min_{\psi \in \Psi} \P \left[ \log w_{\psi} - \P \left[ \log w_{\psi} \right] \right]^{2} = \min_{\psi \in \Psi, \lambda \in \R} \P \left[ \log \tilde w_{\psi} - \lambda \right]^{2},
$$
where $\psi\in \Psi$ is a minimizer of the left-hand side if, and only if, $(\psi, \lambda) \in \Psi \times \R$ with $\lambda = \P \left[ \log \tilde w_{\psi} \right]$ is a minimizer of the right-hand side. 

Similar to the \gls{cem} we restrict our in-depth analysis to natural exponential family proposals where $$\log g_{\psi}(x) = \psi^{T}T(x) - \log Z(\psi).$$ 
Letting $\log \tilde w_{\psi} = \log p - \psi^{T}(T - \P [T])$, we may rewrite \Cref{eq:eis_optim} as
\begin{align}
    \label{eq:eis_exponential_families}
    \min_{\psi \in \Psi, \lambda \in \R} \P \left[ \log p - \psi^{T}(T - \P [T]) - \lambda \right]^{2},
\end{align}
a weighted linear least squares problem. The rationale for centering $T$ under $\P$ will become apparent in \Cref{prop:eis_exponential_families}.
As we consider unnormalized weights $\tilde w$, we are additionally able to get rid of the potentially non-linear term $\log Z(\psi)$.
We note that this is a convex objective function in $(\psi, \lambda)$ which, similar to the \acrshort{cem}, will be very useful to derive asymptotics later on. For now, we begin with studying the existence and uniqueness of $\peis$ similar to \Cref{prop:cem_exponential_families}. While these results have, to the best of the authors knowledge, not been published anywhere, we believe them to be folk knowledge. 

\begin{lemma}[\acrshort{eis} for exponential families]
    \label{prop:eis_exponential_families}
    Let $\left( \G_{\psi} \right)_{\psi \in \Psi}$ form a k-dimensional natural exponential family with log-densities 
    $$
        \log g_{\psi}(x) = \psi^{T} T(x) - \log Z(\psi)
    $$
    for $\Psi \subseteq \R^{k}$. Suppose that $\log p, T \in L^{2}(\P)$. 

    If there is a $\peis \in \Psi$ with
    \begin{align}
        %\label{eq:peis-analytical}
        \cov_{\P} \left( T \right)\peis = \cov_{\P} \left(T, \log p \right),
    \end{align}
    it is a global minimizer of \Cref{eq:eis_optim}. If $\cov_{\P} \left( T \right)$ is non-singular,
    \begin{align}
        \label{eq:peis-analytical}
        \peis = \cov_{\P} (T) ^{-1} \cov _{\P} (T, \log p )
    \end{align}
    is the unique global minimizer.
\end{lemma}

\begin{proof}
    Under the proposed conditions, we may consider \Cref{eq:eis_exponential_families} instead, where the moment conditions on $\log p $ and $T$ ensure that the problem is well-posed, i.e. the target is finite for all $\psi \in \Psi$. 
    As $T$ is centered in \Cref{eq:eis_exponential_families}, the optimal $\peis$ is the \gls{blp} of $\log p$ given the sufficient statistics $T$ and $\lambda_{\eis}$ is $\P \left[ \log \tilde w_{\peis} \right]$.
    Thus $\peis$ is the $L^{2}$-projection of $\log p$ onto the span of $T - \P[T]$, which is given by any solution of
    $$
        \cov_{\P} (T) \peis = \cov_{\P} (T, \log p),
    $$
    i.e. $\peis$ as stated in the lemma. Furthermore, if $\cov_{\P} (T)$ is non-singular, the solution to this equation is unique.
\end{proof}


As the optimal $\peis$ depends on several unknown quantities, \gls{eis} proceeds like the \gls{cem} and employs importance sampling with a proposal $\G$, estimating $\peis$ by
$$
\left(\hat \lambda,\hat \psi_{\text{EIS}}\right) = \argmin_{\lambda,\psi} \P \left[ \log p - \psi^{T}(T - \P [T]) - \lambda \right]^{2},
$$
where $X^{1}, \dots, X^{N} \iid \G$, see also \Cref{rem:measures_in_is}. 
Again, if $\G_{\psi}, \psi \in \Psi$ form an exponential family with natural parameter $\psi$, this optimization problem turns into a weighted least squares problem. As $\P[T]$ is not known, we use log-weights $\log\tilde w_{\psi} = \log p - \psi^{T}T$, as the constant factor $\P[T]$ does not matter in the weights, see the discussion surrounding \Cref{eq:eis_exponential_families}, instead.
We can then estimate $\peis$ with the standard weighted least squares estimator
$$
\left( \hat\lambda', \hpeis \right) = \left(\mathbf X^{T}\mathbf W\mathbf X\right)^{-1}\mathbf X^{T}\mathbf W y%
$$
where the random design matrix $\mathbf X$\footnote{if $\mathbf X\mathbf W \mathbf X$ is not invertible, replace the inverse by the Moore-Penrose pseudoinverse} and diagonal weights matrix $\mathbf W$ are given by
\begin{align*}
\mathbf X &= \begin{pmatrix}
    1 & T(X^{1})^{T} \\
    \dots&\dots\\
    1 & T(X^{N})^{T} \\
\end{pmatrix}\\
\intertext{and}
\mathbf W &= \text{diag} \left( W^{1}, \dots, W^{N} \right),
\end{align*}
and the observations are 
\begin{align*}
y = \left( \log p(X^{1}), \dots, \log p(X^{N}) \right)^{T} \in \R^{N}.
\end{align*}

Alternatively, replacing $\P$ by $\hat\P_{N}$ in \Cref{eq:peis-analytical}, we obtain the equivalent formulation
\begin{align}
    \label{eq:hpeis-cov}
    \hpeis = \cov_{\hat\P_{N}} (T)^{-1} \cov_{\hat \P_{N}} \left( T, \log p \right),
\end{align}
as long as $\cov_{\hat \P_{N}} T$ is non-singular.

An attractive feature of \gls{eis} is that if the target $\P$ is a member of the exponential family of proposals, i.e. there is a $\psi_{\P}\in\Psi$ such that $\P = \G_{\psi_{\P}}$, then \gls{eis} finds the optimal $\peis = \psi_{\P}$ a.s. for a finite number of samples.

\begin{proposition}[Finite sample convergence of \gls{eis}]
    \label{prop:eis-finite-sample}
    Suppose $\G_{\psi}, \psi \in \Psi \subseteq \R^{k}$ for a natural exponential family w.r.t. Lebesgue measure, where the support of the sufficient statistic $\operatorname{supp} T$ is open in $\R^{k}$. 
    Furthermore let $\G$ be a probability measure on $\R^{m}$ that is equivalent to $\P$, i.e. $\G \ll \P$ and $\P \ll \G$. 

    If there is a $\psi_{\P} \in \Psi$ such that $\P = \G_{\psi_{\P}}$, then $\hpeis = \psi_{\P}$ a.s. for $N \geq k$. 
\end{proposition}

\begin{proof}
    As $\P$ stems from the same exponential family as $\G_{\psi}$, the pseudo-observations are $$\log p  = \psi_{\P}^T T - \log Z(\psi_{\P}).$$ Thus, $\cov_{\hat \P_{N}} \left( T, \log p  \right) = \cov_{\hat \P_{N}} \left( T \right)\psi_{\P}$. 
    If we can show that $\cov_{\hat\P_{N}} (T)$ is non-singular, \Cref{eq:hpeis-cov} implies that $\hpeis = \psi_{\P}$ a.s.. 

    Let $W^{i}$ be the $i$-th weight, $i = 1,\dots, N$ in the particle approximation $\hat \P_{N}$.
    By the assumed equivalence of $\G$ and $\P$, $W^{i}$ is non-zero $\G$-a.s., so $W^{i}(X^{i})T(X^{i}) \neq 0$ a.s. as well, as $T$ has open support.
    If $\cov_{\hat \P_{N}} (T)$ were singular, there would exist a $\psi \in \R^{k}$ such that
    $$
    \psi^{T} \cov_{\hat\P_{N}} (T) \psi = \cov_{\hat \P_{N}} \left( \psi^{T}T \right) = 0,
    $$
    implying that $W^{i} T(X^{i})$ lies in the orthogonal complement $\psi^{\perp}$, a subspace of dimension $k - 1$.
    As $W^{i} \neq 0$ a.s., $T(X^{i}) = \frac{1}{W^{i}} W^{i} T(X^{i}) \in \psi^{\perp}$.
    Thus, all $T(X^{i})$, $i = 1, \dots N$, would have to lie in a subspace of dimension $k - 1$, which is a contradiction to $T$ having open support if $N \geq k$.
\end{proof}

Note that if in the above proposition only $\G \gg \P$ holds, we obtain, by a similar argument, that $$\mathbb P \left( \hpeis = \psi_{\P} \right) \stackrel{N\to\infty}\longrightarrow 1.$$
Additionally, we then have to take care of the event $ \{ w(X) = 0 \}$, whose probability is now potentially positive.

We now turn to deriving asymptotics for $\hpeis$. As for the \acrshort{cem}, we start with proving that $\hpeis$ consistently estimates $\peis$.
For this we need to ensure that $\peis$ is the unique solution to \Cref{eq:eis_optim}, as otherwise, consistent estimators of $\peis$ cannot exist. 
As \Cref{eq:eis_exponential_families} is a linear least squares problem, the objective function is convex, and so we can apply \Cref{thm:haberman-consistent} and \Cref{prop:is-consistency}.

\begin{theorem}[consistency of $\hpeis$]
    \label{thm:eis-consistent}
    Let $\left( \G_{\psi} \right)_{\psi \in \Psi}$ form a $k$-dimensional natural exponential family with log-densities 
    $$
        \log g_{\psi}(x) = \psi^{T}T(x) - \log Z(\psi)
    $$
    for convex $\Psi \subseteq \R^{k}$. Let $\G \gg \P$ be a proposal and suppose that 
    \begin{enumerate}
        \item $\log p, T \in L^{2}(\P)$,
        \item $\cov_\P (T)$ is non-singular, and
        \item $\peis \in \operatorname{int} \Psi$ exists and is unique.
    \end{enumerate}
    
    Then 
    $$
        \hpeis \stackrel{N \to \infty}\longrightarrow \peis
    $$
    almost surely.
\end{theorem}

\begin{proof}
    We follow the same strategy as in the proof of \Cref{thm:cem-consistent}. Let 
    \begin{align*}
        b: \R^{p} \times \R^{k + 1} \to [-\infty, \infty)  && b(x,\psi') = \begin{cases}
            -\frac{1}{2} \left( \log p(x) - \psi^{T}(T(x) - \P [T]) - \lambda \right)^{2} & \psi \in \Psi \\
            -\infty &  \text{ else,}
        \end{cases}
    \end{align*}
    where $\psi' = (\psi, \lambda) \in \R^{k + 1}$. For fixed $x$ this function is concave, as its Hessian is negative semi-definite:
    $$
        H_{\psi'} b(x, \psi') = -\begin{pmatrix}
            T(x)T(x)^{T} & T(x) \\
            T(x)^{T} &  1
        \end{pmatrix} = -\begin{pmatrix} T(x)^{T} & 1 \end{pmatrix}^{T} \begin{pmatrix} T(x)^{T}& 1 \end{pmatrix},
    $$
    if $\psi \in \Psi$. Let $X^{1}, \dots, X^{N} \iid \P$ and let $\tilde \P _{N}$ be their empirical distribution. For $\psi \in \Psi, \lambda \in \R$ we have 
    $$
        \P \left[ b(\cdot, \psi') \right] = - \frac{1}{2} \P \left[ \left( \log p - \psi^{T}(T - \P[T]) - \lambda\right)^{2} \right] < \infty,
    $$
    as $\log p, T\in L^{2}(\P)$. Let us now check that conditions \ref{it:C1} - \ref{it:C3} are fulfilled. 

    \ref{it:C1} is fulfilled, as we assumed $\peis \in \operatorname{int} \Psi$. \ref{it:C2} holds, as $\peis$ is the unique global maximizer by \Cref{prop:eis_exponential_families}, as $\cov (T)$ is non-singular.
    \ref{it:C3} obviously holds. 

    Thus $\hpeis$ is strongly consistent if $\G = \P$. If $\G$ is different from $\P$, we can apply \Cref{prop:is-consistency}, where the existence of M-estimators is ensured by \Cref{eq:hpeis-cov}, using the Moore-Penrose inverse if $\cov_{\hat\P_{N}}(T)$ is singular. 
\end{proof}

As \Cref{eq:hpeis-cov} expresses $\hpeis$ in terms of empirical covariances, we could alternatively prove consistency by ensuring that the empirical covariances are consistent as well, for which we would need to ensure that fourth-order moments of $\log p$ and $T$ w.r.t. $\P$ exist. This strategy may be fruitful if $\peis$ does not lie in the interior of $\Psi$, although the more sophisticated treatment of \citep{Haberman1989Concavity} may also be applicable under these circumstances.

Additionally, if fourth-order moments exist, we can derive a central limit theorem, similar to \Cref{thm:cem-clt}, for \acrshort{eis}.

% region CLT proof
\begin{theorem}[\acrshort{clt} for $\hpeis$]
    \label{thm:eis-clt}
    Let $\left( \G_{\psi} \right)_{\psi \in \Psi}$ form a $k$-dimensional natural exponential family with log-densities 
    $$
    \log g_{\psi}(x) = \psi^{T} T(x) - \log Z(\psi),
    $$
    and convex parameter space $\Psi \subseteq \R^{k}$. Let $\G \gg \P$ be a proposal with weights $w = \rnd{\P}{\G}$. 

    Assume that
    \begin{enumerate}
        \item\label{it:secondISmomentsexist}$w (T_{i} - \P[T_{i}])(T_{j} - \P[T_{j}]), w (\log p)^{2} \in L^{2}(\G)$ for $i,j = 1, \dots, k$,
        \item\label{it:fourthmomentsexist} $\log p, T_i \in L^{4}(\P)$ for all $i = 1, \dots, k$
        \item $\cov_{\P}(T)$ is non-singular and $\peis \in \operatorname{int} \Psi$.
    \end{enumerate}

    Then 
    $$
        \sqrt{N} \left(\hpeis - \peis\right) \convD \Normal (0, BMB)
    $$
    where $B = \cov_{\P}(T)^{-1}$ and 
    $$
    M = \cov_{\G} \left( w \left( \log p - \psi_{\eis}^{T}(T - \P[T]) - \lambda_{\eis} \right)(T - \P[T]) \right) = \cov_{\G} \left( w (\log w_{\psi_{\eis}}) (T - \P[T]) \right).
    $$
\end{theorem}

\begin{proof}
    Similar to the proof of \Cref{thm:cem-clt}, we combine \Cref{thm:haberman-clt} and \Cref{prop:is-clt}. Let 
    \begin{align*}
        b : \mathcal X \times \R^{k + 1} \to [-\infty, \infty) && b(x, \psi') =  \begin{cases}
            -\frac{1}{2} \left( \log p(x) - \psi^{\prime T}T'(x) \right)^{2} & x \in \Psi \\
            -\infty & \text{else,}
        \end{cases}
    \end{align*}
    where $\psi' = (\psi, \lambda) \in \Psi \times\R$ and $T'(x) = \begin{pmatrix} (T(x) - \P[T]) & 1 \end{pmatrix}$. For $\psi' \in \Psi \times \R$ the map $$\psi' \mapsto \P [b(\cdot, \psi')]$$ is differentiable with gradient 
    $$
        \nabla_{\psi'} \P [b(\cdot, \psi')] = - \P \left[\left( \log p - \psi^{\prime T}T' \right) T'\right] $$
    and Hessian 
    $$
        H_{\psi'} \P[b(\cdot, \psi')] = -\P\left[T'T^{\prime T}\right] = - \begin{pmatrix}
            \P \left[ (T - \P[T])(T - \P[T])^{T} \right] & 0 \\
            0 & 1
        \end{pmatrix} = - \begin{pmatrix}
            \cov_{\P}(T) & 0 \\
            0 & 1
        \end{pmatrix}.
    $$
    The Hessian is negative definite, as $\cov_{\P}(T)$ is assumed to be positive definite. Thus condition \ref{it:C7} is fulfilled.

    For condition \ref{it:C10}, we can verify that for all $i,j = 1,\dots, k+ 1$ 
    $$
        (\nabla_{\psi'} b(\cdot, \psi'))_{i}(\nabla_{\psi'} b(\cdot, \psi'))_{j} = \left( \log p - \psi^{\prime T}T' \right)^{2} T'_{i}T'_{j}
    $$
    is in $L^{1}(\P)$ by assumption \ref{it:fourthmomentsexist} and the Hölder inequality. 

    To apply \Cref{prop:is-clt} we need to show that $w(\cdot)b'(\cdot, \psi',\xi') \in L^{2}(\G)$ for all $\xi'\in\R^{k + 1}$ and all $\psi'$ in a neighborhood of $\peis$. For this it suffices that we show 
    $$
        w^{2}(\nabla_{\psi'} b(\cdot, \psi'))_{i}(\nabla_{\psi'} b(\cdot, \psi'))_{j} = w^{2}\left( \log p - \psi^{\prime T}T' \right)^{2} T'_{i}T'_{j}
    $$
    is in $L^{1}(\G)$, which holds, again, by assumption \ref{it:secondISmomentsexist} and the Hölder inequality.

    We have thus shown a central limit theorem for $\hat\psi'_{\eis} = \left( \hpeis, \hat\lambda_{\eis} \right)$, i.e. 
    $$
        \sqrt{N} \left( \hat\psi'_{\eis} - \peis \right) \to \mathcal N(0, M'B'M')
    $$
    with $$
        B' = - \left(H_{\psi'_{\eis}} \P \left[ b(\cdot, \psi'_{\eis}) \right]\right)^{-1} = \begin{pmatrix}
            \cov_{\P}(T)^{-1} & 0 \\
            0 & 1 
        \end{pmatrix}
    $$ and $$
        M' = \cov \left( w(X) \nabla_{\psi'_{\eis}} b(X,\psi'_{\eis}) \right) 
    $$ 
    for $X \sim \G$. As $B'$ is block-diagonal, to obtain the marginal distribution of $\hpeis - \peis$, we only need the upper left $k \times k$ block of $M'$, which is given by
    $$
        M = \cov \left( w \left( \log p - \peis^{T}(T - \P[T]) - \lambda_{\eis} \right)(T - \P [T]) \right).
    $$
\end{proof}
% endregion 


A discussion of the assumptions of \Cref{thm:eis-consistent,thm:eis-clt} is in order. We start with the consistency result \Cref{thm:eis-consistent}. The integrability condition, i.e that $\log p, T \in L^{2}(\P)$ is necessary to ensure existence of $\peis$ and the existence of $\cov_{\P}(T)$ as well as $\peis \in \operatorname{int} \Psi$ ensure uniqueness, see also \Cref{prop:eis_exponential_families}. 

Regarding the central limit theorem, \Cref{thm:eis-clt}, requiring the existence of higher order moments is natural. Unfortunately, there is no direct interpretation of these requirements as generalizations of the existence of $\rho$, as was the case for the \acrshort{cem}. 

The only integrability conditions related to the proposal $\G$ are those for $w(T_{i} - \P [T_{i}]) (T_{j} - \P[T_{j}])$ and $w \log(p)^{2}$. 
These technical conditions are not easily interpreted, as assuming existence of moments of the target distribution seem more natural than those involving the extra weighting term $w = \frac{p}{g}$, which depends on the proposal $\G$ as well. 