\glsreset{eis}
\subsection{\texorpdfstring{\Acrfull{eis}}{Efficient importance sampling}}
\label{subsec:eis}
\gls{eis} \citep{Richard2007Efficient} provides an alternative to the \gls{cem}. Instead of minimizing the \gls{kld} between the target $\P$ and proposal $\G_{\psi}, \psi \in \Psi$, \gls{eis} aims at minimizing the variance of the logarithm of importance sampling weights. 
Our discussion of \citep{Chatterjee2018Sample}, \Cref{thm:chatterje2018Thm1}, especially \Cref{lem:bounded-log-variance}, suggests that this is worthwhile. 
Thus, \acrshort{eis} finds $\peis$ which is a feasible solution to the following optimization problem
\begin{align}
    \label{eq:eis_optim}
\min_{\psi \in \Psi} \text{Var}_{\P} \left[ \log w_{\psi} \right] = \min_{\psi \in \Psi} \P \left[ \log w_{\psi} - \P \log w_{\psi} \right]^{2},
\end{align}
where, as in the last section, $\log w_{\psi} = \log p - \log g_{\psi}$.

Two problems arise: $\P [\log w_{\psi}] = \Dkl{\P}{\G_{\psi}}$ is usually intractable and we usually only have access to the unnormalized weights $\frac{\tilde w_{\psi}}{\G_{\psi} \left[ w_{\psi} \right]} = w_{\psi}$, with unknown integration constant $\G_{\psi} \left[ w_{\psi} \right]$. Both can be dealt with by introducing the nuisance parameter $\lambda = \P \left[ \log \tilde w_{\psi} \right]$, utilizing the fact that the mean is the minimizer of the squared distance functional with the minimum value equal to the variance, should it exist. Indeed 
$$
    \log w_{\psi} - \P [\log w_{\psi}] = \log \tilde w_{\psi} - \log \G_{\psi} [\tilde w_{\psi}] - \P \left[ \log \tilde w_{\psi} \right] + \log \G_{\psi}[\tilde w_{\psi}] = \log \tilde w_{\psi} - \P \left[ \log \tilde w_{\psi} \right],
$$
so
$$
    \min_{\psi \in \Psi} \P \left[ \log w_{\psi} - \P \left[ \log w_{\psi} \right] \right]^{2} = \min_{\psi \in \Psi, \lambda \in \R} \P \left[ \log \tilde w_{\psi} - \lambda \right]^{2},
$$
where $\psi\in \Psi$ is a minimizer of the left-hand side if, and only if, $(\psi, \lambda) \in \Psi \times \R$ with $\lambda = \P \left[ \log \tilde w_{\psi} \right]$ is a minimizer of the right-hand side. 

Similar to the \gls{cem} we restrict our in-depth analysis to natural exponential family proposals where $$\log g_{\psi}(x) = \psi^{T}T(x) - \log Z(\psi).$$ In this case the optimization problem is reduced to
\begin{align}
    \label{eq:eis_exponential_families}
    \min_{\psi \in \Psi, \lambda \in \R} \P \left[ \log p - \psi^{T}T - \lambda \right]^{2},
\end{align}
a weighted linear least squares problem. As we consider unnormalized weights $\tilde w$, we are additionally able to get rid of the potentially non-linear term $\log Z(\psi)$.
Noticing that this is a convex objective function in $\psi$ which, similar to the \acrshort{cem}, will be very useful to derive asymptotics later on. For now, we begin with studying the existence and uniqueness of $\peis$ similar to \Cref{prop:cem_exponential_families}.

\begin{lemma}[\acrshort{eis} for exponential families]
    \label{prop:eis_exponential_families}
    Let $\left( \G_{\psi} \right)_{\psi \in \Psi}$ form a k-dimensional natural exponential family with log-densities 
    $$
        \log g_{\psi}(x) = \psi^{T} T(x) - \log Z(\psi)
    $$
    for $\Psi \subseteq \R^{k}$. Suppose that $\log p, T \in L^{2}(\P)$. 

    If there is a $\peis \in \Psi$ with
    \begin{align}
        \label{eq:peis-analytical}
        \cov_{\P} \left( T \right)\peis = \cov_{\P} \left(T, \log p \right)
    \end{align}
    it is a global minimizer of \Cref{eq:eis_optim}. If $\cov_{\P} \left( T \right)$ is non-singular,
    $$
    \peis = \cov_{\P} (T) ^{-1} \cov _{\P} (T, \log p )
    $$
    is the unique global minimizer.
\end{lemma}

\begin{proof}
    Under the proposed conditions, we may consider \Cref{eq:eis_exponential_families} instead, where the moment conditions on $\log p $ and $T$ ensure that the problem is well-posed, i.e. the target is finite for all $\psi \in \Psi$. 
    Thus the optimal $ \left( \peis, \lambda_{\eis} \right)$ are given by the \gls{blup} of $\log p $ by the sufficient statistic $T$ under $\P$ for $\peis$ and $\P \left[ \log \tilde w_{\peis} \right]$ for $\lambda_{\eis}$. Standard results from multivariate regression theory imply that the \acrshort{blup} is given by any solution of
    $$
        \cov_{\P} (T) \peis = \cov_{\P} (T, \log p),
    $$
    i.e. $\peis$ as stated in the lemma. Furthermore, if $\cov_{\P} (T)$ is non-singular, the solution to this equation is unique.
    
\end{proof}

As the optimal $\peis$ depends on several unknown quantities, \gls{eis} proceeds like the \gls{cem} and employs importance sampling with a proposal $\G$, estimating $\peis$ by
$$
\left(\hat \lambda,\hat \psi_{\text{EIS}}\right) = \argmin_{\lambda,\psi} \hat\P_{N} \left[ \log \tilde w_{\psi} - \lambda \right]
$$
where $X^{1}, \dots, X^{N} \iid \G$. 
Again, if $\G_{\psi}, \psi \in \Psi$ form an exponential family with natural parameter $\psi$, this optimization problem turns into a weighted least squares problem, so we can estimate $\peis$ with the standard weighted least squares estimator
$$
\left( \hat\lambda', \hpeis \right) = \left(\mathbf X^{T}\mathbf W\mathbf X\right)^{-1}\mathbf X^{T}\mathbf W y%
$$
where the random design matrix $\mathbf X$\footnote{if $\mathbf X\mathbf W \mathbf X$ is not invertible, replace the inverse by the Moore-Penrose pseudoinverse} and diagonal weights matrix $\mathbf W$ are given by
\begin{align*}
\mathbf X &= \begin{pmatrix}
    1 & T(X^{1})^{T} \\
    \dots&\dots\\
    1 & T(X^{N})^{T} \\
\end{pmatrix}\\
\intertext{and}
\mathbf W &= \text{diag} \left( W^{1}, \dots, W^{N} \right),
\end{align*}
and the observations are 
\begin{align*}
y = \left( \log p(X^{1}), \dots, \log p(X^{N}) \right)^{T} \in \R^{N}.
\end{align*}

Alternatively, replacing $\P$ by $\hat\P_{N}$ in \Cref{eq:peis-analytical}, we obtain the equivalent formulation
\begin{align}
    \label{eq:hpeis-cov}
    \hpeis = \cov_{\hat\P_{N}} (T)^{-1} \cov_{\hat \P_{N}} \left( T, \log p \right),
\end{align}
as long as $\cov_{\hat \P_{N}} T$ is non-singular.

An attractive feature of \gls{eis} is that if the target $\P$ is a member of the exponential family of proposals, i.e. there is a $\psi_{\P}\in\Psi$ such that $\P = \G_{\psi_{\P}}$, then \gls{eis} finds the optimal $\peis = \psi_{\P}$ a.s. for a finite number of samples.

\begin{proposition}[Finite sample convergence of \gls{eis}]
    \label{prop:eis-finite-sample}
    Suppose $\G_{\psi}, \psi \in \Psi \subseteq \R^{k}$ for a natural exponential family w.r.t. Lebesgue measure, where the support of the sufficient statistic $\operatorname{supp} T$ is open in $\R^{k}$. 
    Furthermore let $\G$ be a probability measure on $\R^{m}$ that is equivalent to $\P$, i.e. $\G \ll \P$ and $\P \ll \G$. 

    If there is a $\psi_{\P} \in \Psi$ such that $\P = \G_{\psi_{\P}}$, then $\hpeis = \psi_{\P}$ a.s. for $N \geq k$. 
\end{proposition}

\begin{proof}
   As $\P$ stems from the same exponential family as $\G_{\psi}$, the pseudo-observations are $$\log p  = \psi_{\P}^T T - \log Z(\psi_{\P}).$$ Thus $\cov_{\hat \P_{N}} \left( T, \log p  \right) = \cov_{\hat \P_{N}} \left( T \right)\psi_{\P}$. 
   If we can show that $\cov_{\hat\P_{N}} T$ is non-singular, \Cref{eq:hpeis-cov} implies that $\hpeis = \psi_{\P}$ a.s.. 

   If $\cov_{\hat \P_{N}} T$ were singular, there would exist a $\psi \in \R^{k}$ such that
   $$
   \psi^{T} \cov_{\hat\P_{N}} (T) \psi = \cov_{\hat \P_{N}} \left( \psi^{T}T \right) = 0.
   $$
    In this case the a.s. non-zero $W^{i}(X^{i}) T(X^{i})$ would lie in the orthogonal complement $\psi^{\perp}$ for all $i = 1, \dots, N$. As the weights are a.s. positive by the assumed equivalence of $\G$ and $\P$, the same holds true for $T(X^{i}), i = 1,\dots, N$.
   If $N$ is bigger than $k$, the probability that this happens is $0$, as $\operatorname{supp} T $ is open. Thus $\cov_{\hat \P_{N}} T$ is non-singular almost surely and the result is shown.
\end{proof}

Note that if in the above proposition only $\G_{\psi} \gg \P$ holds, we obtain, by a similar argument, that $$\mathbb P \left( \hpeis = \psi_{\P} \right) \stackrel{N\to\infty}\longrightarrow 1.$$
Additionally, we then have to take care of the event $ \{ w(X) = 0 \}$, whose probability is now potentially positive.

We now turn to deriving asymptotics for $\hpeis$. As for the \acrshort{cem}, we start with proving that $\hpeis$ consistently estimates $\peis$.
For this we need to ensure that $\peis$ is the unique solution to \Cref{eq:eis_optim}, as otherwise, consistent estimators of $\peis$ cannot exist. 
As \Cref{eq:eis_exponential_families} is a linear least squares problem, the objective function is convex, and so we can apply \Cref{thm:haberman-consistent} and \Cref{prop:is-consistency}.

\begin{theorem}[consistency of $\hpeis$]
    \label{thm:eis-consistent}
    Let $\left( \G_{\psi} \right)_{\psi \in \Psi}$ form a $k$-dimensional natural exponential family with log-densities 
    $$
        \log g_{\psi}(x) = \psi^{T}T(x) - \log Z(\psi)
    $$
    for convex $\Psi \subseteq \R^{k}$. Let $\G \gg \P$ be a proposal and suppose that 
    \begin{enumerate}
        \item $\log p, T \in L^{2}(\P)$ and
        \item $\cov_\P (T)$ is non-singular,
        \item $\peis \in \operatorname{int} \Psi.$ 
    \end{enumerate}
    
    Then 
    $$
        \hpeis \stackrel{N \to \infty}\longrightarrow \peis
    $$
    almost surely.
\end{theorem}

\begin{proof}
    We follow the same strategy as in the proof of \Cref{thm:cem-consistent}. Let 
    \begin{align*}
        b: \R^{p} \times \R^{k + 1} \to [-\infty, \infty)  && b(x,\psi') = \begin{cases}
            -\frac{1}{2} \left( \log p(x) - \psi^{T}T(x) - \lambda \right)^{2} & \psi \in \Psi \\
            -\infty &  \text{ else,}
        \end{cases}
    \end{align*}
    where $\psi' = (\psi, \lambda) \in \R^{k + 1}$. For fixed $x$ this function is concave, as its Hessian is negative semi-definite:
    $$
        H_{\psi'} b(x, \psi') = -\begin{pmatrix}
            1 & T(x)^{T} \\
            T(x) & T(x)T(x)^{T}
        \end{pmatrix} = -\begin{pmatrix} 1 & T(x)^{T} \end{pmatrix} \begin{pmatrix} 1 & T(x)^{T} \end{pmatrix}^{T},
    $$
    if $\psi \in \Psi$. Let $X^{1}, \dots, X^{N} \iid \P$ and let $\tilde \P _{N}$ be their empirical distribution. For $\psi \in \Psi, \lambda \in \R$ we have 
    $$
        \P \left[ b(\cdot, \psi') \right] = - \frac{1}{2} \P \left[ \left( \log p - \psi^{T}T - \lambda\right)^{2} \right] < \infty,
    $$
    as $\log p, T\in L^{2}(\P)$. Let us now check that conditions \ref{it:C1} - \ref{it:C3} are fulfilled. 

    \ref{it:C1} is fulfilled, as we assumed $\peis \in \operatorname{int} \Psi$. \ref{it:C2} holds, as $\peis$ is the unique global maximizer by \Cref{prop:eis_exponential_families}, as $\cov (T)$ is non-singular.
    \ref{it:C3} obviously holds. 

    Thus $\hpeis$ is strongly consistent if $\G = \P$. If $\G$ is different from $\P$, we can apply \Cref{prop:is-consistency}, where the existence of M-estimators is ensured by \Cref{eq:hpeis-cov}, using the Moore-Penrose inverse if $\cov_{\hat\P_{N}}(T)$ is singular. 
\end{proof}

As \Cref{eq:hpeis-cov} expresses $\hpeis$ in terms of empirical covariances, we could alternatively prove consistency by ensuring that the empirical covariances are consistent as well, for which we would need to ensure that fourth-order moments of $\log p$ and $T$ w.r.t. $\P$ exist. This strategy may be fruitful if $\peis$ does not lie in the interior of $\Psi$, although the more sophisticated treatment of \citep{Haberman1989Concavity} may also be applicable under these circumstances.

Additionally, if fourth-order moments exist, we can derive a central limit theorem, similar to \Cref{thm:cem-clt}, for \acrshort{eis}.

% region CLT proof
\begin{theorem}[\acrshort{clt} for $\hpeis$]
    \label{thm:eis-clt}
    Let $\left( \G_{\psi} \right)_{\psi \in \Psi}$ form a $k$-dimensional natural exponential family with log-densities 
    $$
    \log g_{\psi}(x) = \psi^{T} T(x) - \log Z(\psi),
    $$
    and convex parameter space $\Psi \subseteq \R^{k}$. Let $\G \gg \P$ be a proposal with weights $w = \rnd{\P}{\G}$. 

    Assume that
    \begin{enumerate}
        \item\label{it:secondISmomentsexist}$w T_{i}T_{j}, w (\log p)^{2} \in L^{2}(\G)$ for $i,j = 1, \dots, k$,
        \item\label{it:fourthmomentsexist} $\log p, T_i \in L^{4}(\P)$ for all $i = 1, \dots, k$
        \item $\cov_{\P}(T)$ is non-singular and $\peis \in \operatorname{int} \Psi$.
    \end{enumerate}

    Then 
    $$
        \sqrt{N} (\hpeis - \peis) \convD \Normal (0, BMB)
    $$
    where $B = \cov_{\P}(T)^{-1}$ and 
    $$
    M = \cov_{\G} \left( w \left( \log p - \peis^{T}T - \lambda_{\eis} - \P[T] \right)T \right).
    $$
\end{theorem}

\begin{proof}
    Similar to the proof of \Cref{thm:cem-clt}, we combine \Cref{thm:haberman-clt} and \Cref{prop:is-clt}. Let 
    \begin{align*}
        b : \mathcal X \times \R^{k + 1} \to [-\infty, \infty) && b(x, \psi') =  \begin{cases}
            -\frac{1}{2} \left( \log p(x) - \psi^{\prime T}T'(x) \right) & x \in \Psi \\
            -\infty & \text{else,}
        \end{cases}
    \end{align*}
    where $\psi' = (\psi, \lambda) \in \Psi \times\R$ and $T'(x) = \begin{pmatrix} T(x) & 1 \end{pmatrix}$. For $\psi \in \Psi$ the map $(\psi, \lambda) \to \P [b(\cdot, (\psi, \lambda))]$ is differentiable with gradient 
    $$
        \nabla_{\psi'} \P [b(\cdot, \psi')] = - \P \left[\left( \log p - \psi^{\prime T}T' \right) T'\right] = \begin{pmatrix}
            - \P \left[ T'\log p - T'T^{\prime T}\psi' \right] \\
            -\P \left[ \log p - \psi'^{T}T' \right]
        \end{pmatrix}
    $$
    and Hessian 
    $$
        H_{\psi'} \P[b(\cdot, \psi')] = -\P\left[T'T^{\prime T}\right] = - \begin{pmatrix}
            \P \left[ TT^{T} \right] & \P [T^{T}] \\
            \P [T] & 1
        \end{pmatrix}.
    $$
    The Hessian is negative definite, as for all $\psi\in \R^{k}, \lambda \in \R$ we have
    \begin{align*}
        \begin{pmatrix} \psi^{T} & \lambda \end{pmatrix} H_{\psi'} \P \left[ b(\cdot, \psi') \right]\begin{pmatrix} \psi^{T} & \lambda \end{pmatrix}^{T} &= - \left( \psi^{T} \cov_{\P} (T) \psi + \psi^{T}\P[T]\P[T]^{T}\psi + 2 \psi^{T}\P[T] \lambda + \lambda^{2}\right) \\
                                 &= - \left( \psi^{T}\cov_{\P}(T)\psi + (\lambda + \psi^{T}\P[T])^{2} \right) \leq 0,
    \end{align*}
    with equality if, and only if, both $\lambda$ and $\psi$ are $0$, as $\cov_{\P}(T)$ is assumed to be positive definite. Thus condition \ref{it:C7} is fulfilled.

    For condition \ref{it:C10}, we can verify that for all $i,j = 1,\dots, k+ 1$ 
    $$
        (\nabla_{\psi'} b(\cdot, \psi'))_{i}(\nabla_{\psi'} b(\cdot, \psi'))_{j} = \left( \log p - \psi^{\prime T}T' \right)^{2} T'_{i}T'_{j}
    $$
    is in $L^{1}(\P)$ by assumption \ref{it:fourthmomentsexist} and the Hölder inequality. 

    To apply \Cref{prop:is-clt} we need to show that $w(\cdot)b'(\cdot, \psi',\xi') \in L^{2}(\G)$ for all $\xi'\in\R^{k + 1}$ and all $\psi'$ in a neighborhood of $\peis$, for this it suffices that we show 
    $$
        w^{2}(\nabla_{\psi'} b(\cdot, \psi'))_{i}(\nabla_{\psi'} b(\cdot, \psi'))_{j} = w^{2}\left( \log p - \psi^{\prime T}T' \right)^{2} T'_{i}T'_{j}
    $$
    is in $L^{1}(\G)$, which holds, again, by assumption \Cref{it:secondISmomentsexist} and the Hölder inequality.

    We have thus shown a central limit theorem for $\hat\psi'_{\eis} = \left( \hpeis, \hat\lambda_{\eis} \right)$, i.e. 
    $$
        \sqrt{N} \left( \hat\psi'_{\eis} - \peis \right) \to \mathcal N(0, M'B'M')
    $$
    with $B' = - \left(H_{\psi'_{\eis}} \P \left[ b(\cdot, \psi'_{\eis}) \right]\right)^{-1}$ and $M' = \cov \left( w(X) \nabla_{\psi'_{\eis}} b(X,\psi'_{\eis}) \right)$ for $X \sim \G$. By using the inversion formula for block matrices, we obtain 
    \begin{align*}
        B' &= \begin{pmatrix}
            \P \left[ TT^{T} \right] & \P [T^{T}] \\
            \P [T] & 1
        \end{pmatrix}^{-1} = \begin{pmatrix}
            \Sigma + \mu\mu^{T} & \mu^{T} \\
            \mu & 1
        \end{pmatrix}^{-1} \\
        &= \begin{pmatrix}
            (\Sigma + \mu\mu^{T} - \mu\mu^{T})^{-1} & 0 \\
            0 & 1 - \mu^{T} (\Sigma + \mu\mu^{T})^{-1}\mu
        \end{pmatrix} \begin{pmatrix}
            I_{k} & - \mu^{T} \\
            -\mu(\Sigma + \mu\mu^{T})^{-1}& 1
        \end{pmatrix} \\
        &= \begin{pmatrix}
            \Sigma^{-1} & - \mu^{T}\Sigma^{-1}\\
            -\Sigma^{-1}\mu& 1 - \mu^{T} \left( \Sigma + \mu\mu^{T} \right)^{-1}\mu
        \end{pmatrix}
    \end{align*}
    where $\Sigma = \cov_{\P} (T)$ and $\mu = \P [T]$. Similarly, 
    $$
        M' = \begin{pmatrix}
            \cov_{\G} \left( w W_{\peis}T \right) &\cov_{\G} \left( w W_{\peis}T, w W_{\peis} \right) \\
            \cov_{\G} \left( w W_{\peis}, w W_{\peis}T \right) & \cov_{\G} \left( w W_{\peis} \right) 
        \end{pmatrix},
    $$
    where $W_{\peis} = \log p - \psi^{\prime T}_{\eis}T'$.

    If $\mu \neq 0$, we may change the sufficient statistic of the exponential family such that this holds, i.e. let $\tilde T = T - \P [T]$, then 
    $$
        \log g_{\psi}(x) = \psi^{T}T(x) - \log Z(\psi) = \psi^{T}\tilde T(x) - \log \tilde Z(\psi)
    $$
    where $\tilde Z(\psi) = \log Z(\psi) + \P [T]$. As $\peis$, \Cref{eq:peis-analytical}, only depends on $T - \P[T]$ under $\P$, this does not change $\peis$. Similarly, $\hpeis$, \Cref{eq:hpeis-cov}, is unaffected by subtracting a constant from $T$. Only 
    $$
        \tilde \lambda_{\eis} = \lambda_{\eis} + \P[T]
    $$
    and similarly $\hat\lambda_{\eis}$ are changed.
    
    Thus, without loss of generality, we may assume that $\P[T] = 0$. 
    Then 
    $$
        B' = \begin{pmatrix}
            \Sigma^{-1} & 0 \\
            0 & 1
        \end{pmatrix}
    $$
    is a diagonal matrix. Taking the $\peis$ marginal of the asymptotic normal distribution, we arrive at 
    $$
        \sqrt{N} \left( \hpeis - \peis \right) \to \mathcal N \left( 0, BMB \right)
    $$
    with $B = \cov_{\P}(T)$ and $M = \cov_{G} \left( w \left( \log p - \peis^{T}T - \lambda_{\eis} - \P[T] \right)T \right)$, as promised.
    
\end{proof}
% endregion 


A discussion of the assumptions of \Cref{thm:eis-consistent,thm:eis-clt} is in order. We start with the consistency result \Cref{thm:eis-consistent}. The integrability condition, i.e that $\log p, T \in L^{2}(\P)$ is necessary to ensure existence of $\peis$ and the existence of $\cov_{\P}(T)$ as well as $\peis \in \operatorname{int} \Psi$ ensure uniqueness, see also \Cref{prop:eis_exponential_families}. 

Regarding the central limit theorem \Cref{thm:eis-clt}, requiring the existence of higher order moments is natural. Unfortunately, there is no direct interpretation of these requirements as generalizations of the existence of $\rho$, as was the case for the \acrshort{cem}. 

The only integrability conditions related to the proposal $\G$ are those for $wT_{i}T_{j}$ and $w \log(p)^{2}$. 
Choosing $\left(\G_{\psi}\right)_{\psi \in \Psi}$ to consist of Gaussian distributions, the conditions on $T$ translate to the existence of certain polynomial moments of $w^{2}$ w.r.t. the proposal $\G$ (or $w$ w.r.t. $\P$). This technical condition, is not easily interpreted, as assuming existence of moments of the target distribution seem more natural than those involving the extra weighting term $w = \frac{p}{g}$, which depends on the proposal $\G$ as well. 