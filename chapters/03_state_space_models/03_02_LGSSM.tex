\glsreset{glssm}
\section{Gaussian Linear State Space Models}
\label{sec:linear_gaussian_state_space_models}

\acrfullpl{glssm} are the working horses of most methods used in this thesis because they are analytically tractable and computationally efficient. Indeed, for fixed dimension of states $m$ and observations $p$ the runtime of algorithms that we consider in this thesis is $\mathcal O(n)$, i.e. linear in the number of time points observed. 

\glsunset{glssm}
\begin{definition}[\gls{glssm}]
    \label{def:glssm}
    A \acrfull{glssm} is a joint distribution over states and observations $(X,Y)$ where states a.s. obey the transition equation
    \begin{align}
        \label{eq:glssm_states}
        X_{t + 1} & = A_{t}X_{t} + u_{t} + \varepsilon_{t + 1} &  & t = 0, \dots, n - 1,
    \end{align}
    and observations a.s. obey the observation equation
    \begin{align}
        \label{eq:glssm_observations}
        Y_{t} & = B_{t}X_{t} + v_{t} + \eta_{t} &  & t = 0, \dots, n.
    \end{align}
    Here $A_{t} \in \mathbf{R}^{m \times m}$ and $B_{t} \in \gls{sym:Rptimesm}$ are matrices that specify the systems dynamics. The \textbf{innovations} $(\varepsilon_{t + 1})_{t = 0, \dots, n-1}$ and \textbf{measurement noise} $(\eta_{t})_{t = 0, \dots, n}$ and the  starting value $X_{0} \sim \mathcal N (\E X_{0}, \Sigma_{0})$ are jointly independent. Furthermore, $\varepsilon_{t+1} \sim \mathcal N(0, \Sigma_{t})$ and $\eta_{t}\sim \mathcal N(0, \Omega_{t})$ are centered Gaussian random variables and $u_{t} \in \R^{m}, t = 0, \dots, n - 1$, $v_{t} \in \R^{p}, t = 0, \dots, n$ are deterministic biases.
\end{definition}

\begin{remark}
    From \Cref{eq:glssm_states} it is easy to see that the states $X = (X_{0}, \dots, X_{n})$ form a Gaussian Markov process and that conditional on $X_{t}$, $t \in \{0, \dots, n\}$, $Y_{t}$ is independent of $X_{s}$ and $Y_{s}$, $s < t$. Thus A \acrshort{glssm} is indeed a \acrshort{ssm}.
\end{remark}


The defining feature of a \gls{glssm} is that the joint distribution of $(X,Y)$ is Gaussian, as $(X,Y)$ may be written as an affine combination of the jointly Gaussian $(X_{0}, \varepsilon_{1}, \dots, \varepsilon_{n}, \eta_{0}, \dots, \eta_{n})$ and it is often useful to perform inferences in terms of innovations and measurement noise instead of states, see e.g. \citep[Section 4.5]{Durbin2012Time}.

As the joint distribution of $(X, Y)$ is Gaussian, so are conditional distributions of states given any set of observations.

\begin{lemma}[Gaussian conditional distributions]
    \label{lem:gaussian_conditional}
    Let $(X,Y)$ be jointly Gaussian with distribution $\mathcal N \left( \mu, \Sigma \right)$ where 
    $$
    \mu = \left(\mu_{X}, \mu_{Y}\right)
    $$
    and 
    $$
    \Sigma = \begin{pmatrix}
        \Sigma_{XX} & \Sigma_{XY} \\
        \Sigma_{YX} & \Sigma_{YY}
    \end{pmatrix},
    $$
    where $\mu$ and $\Sigma$ are partitioned according to the dimensions of $X$ and $Y$. 
    
    Then the following holds:
    
    \begin{enumerate}
        \item \label{it:cond_gaussian} If $\Sigma_{YY}$ is non-singular, $X|Y = y$ follows a Gaussian distribution with conditional expectation
            $$
            \mu_{X|Y = y} = \E \left( X | Y = y \right) = \mu_{X} + \Sigma_{XY}\Sigma_{YY}^{-1} \left( y - \mu_{Y} \right)
            $$
            and conditional covariance matrix 
            $$
            \Sigma_{X| Y =y} = \cov \left( X | Y = y \right) = \Sigma_{XX} - \Sigma_{XY} \Sigma_{YY}^{-1} \Sigma_{YX}.
            $$

        \item In particular, let $X\sim \mathcal N(\mu, \Sigma)$ and $Y = BX + \varepsilon$ for a matrix $B \in \mathbf R^{p\times m}$ and $\R^{p} \ni \varepsilon \sim \mathcal N(0, \Omega)$ independent of $X$ where $\Omega \in \R^{p\times p}$ . 
            Then, as 
            $\E Y = B \mu$, $\cov \left( X,Y \right) = \cov \left( Y, X \right)^{T}= \Sigma B^{T}$ and $\cov \left( Y \right) = B \Sigma B^{T} + \Omega$, we have
            $$
                \E \left( X | Y = y \right) = \mu + K (y - B \mu)
            $$
            and 
            $$
            \cov \left( X | Y = y \right) = \Sigma - K \Sigma K^{T} = \left( I  -  KB \right) \Sigma,
            $$
            as long as $B \Sigma B^{T} + \Omega$ is non-singular.
            Here $K = \Sigma B^{T} \left( B \Sigma B^{T} + \Omega \right)^{-1}$.

        \item If $\Sigma_{XX}$ is non-singular, then $Y - BX$ is independent of $X$ for $B = \Sigma_{YX} \Sigma_{XX}^{-1}$ and we may write
            \begin{align*}
            Y = BX + v + \eta
            \end{align*}
            for an $\eta \sim \mathcal N(0, \Omega)$ with covariance matrix $\Omega = \Sigma_{YY} - \Sigma_{YX}\Sigma_{XX}^{-1}\Sigma_{XY}$ independent of $X$, and $v = \mu_{Y} - B \mu_{X}$.
        \item Suppose that $(X,Y,Z)$ is jointly normal mean $\mu$ and covariance matrix $\Sigma$, partitioned in the same way. 
        If the conditional distribution of $X$ given $Y = y$ and $Z = z$ is given by 
        $$
        X | Y = y, Z = z \sim \mathcal N(Ky + Gz + \delta, \Xi),
        $$
        then the conditional distribution of $X$ given only $Y = y$ is
        
        $$
        X | Y = y \sim \mathcal N \left(Ky + G \mu_{Z| Y= y} + \delta, \Xi + G\cov (Z | Y) G^{T}\right).
        $$
        
    \end{enumerate}
\end{lemma}

\begin{remark}[generalized inverse]
    If $\Sigma_{YY}$ in \Cref{lem:gaussian_conditional} \ref{it:cond_gaussian} is singular, the statement remains true if we choose as $\Sigma_{YY}^{-1}$ a generalized inverse of $\Sigma_{YY}$, see \citep[8.a Note 3]{Rao2002Linear}. A generalized inverse for a matrix $A\in\R^{m \times p}$ is any matrix $A^{-} \in \R^{m \times p}$ such that $AA^{-}A = A$. Given a singular value decomposition $A = UDV^{T}$, we may obtain the Moore-Penrose inverse $A^{\dagger} = V D^{-}U^{T}$ of $A$, which is a generalized inverse of $A$, by inverting the non-zero diagonal elements of $D$. 
\end{remark}

\begin{proof}
    For the first statement, we refer the reader to \citep[Chapter 4, Lemma 1]{Durbin2012Time}.
    
    The second statement follows from substituting the value of $K$. 
    
    The third statement follows from noting that $Y-BX = \begin{pmatrix}
        -B & I
    \end{pmatrix} \begin{pmatrix}
        X \\ Y
    \end{pmatrix}$ follows a Gaussian distribution. A quick calculation reveals that $$\cov \left( Y- BX, X \right) = \Sigma_{YX} - B \Sigma_{XX} = \Sigma_{YX} - \Sigma_{YX} = 0,$$
    showing the independence. Thus $\eta = Y - BX - \delta$ follows a centered Gaussian distribution and equating covariance matrices, we see that $\Omega$ has the desired form.

    For the final statement, notice that $\xi = X - KY - GZ - \delta$ fulfills 
    $$
    \xi | Y = y, Z = z \sim \mathcal N(0, \Xi)
    $$
    which does not depend on $y$ or $z$. Thus the unconditional distribution of $\xi$ is $\mathcal N(0, \Xi)$ as well, and $\xi$ is independent of $(Y, Z)$. Rewriting $X$ in terms of $Y,Z$ and $\xi$, we obtain 
    $$
    X = KY + GZ + \delta + \xi,
    $$
    and so
    \begin{align*}
        \E \left( X | Y = y \right) &= Ky + G \E (Z| Y = y) + \delta,\\
        \intertext{as well as}
        \cov \left( X | Y = y \right) &= \cov \left(KY + GZ+\delta +\xi|  Y = y\right) \\
            &= \cov \left( GZ + \xi | Y = y  \right) \\
            &= \cov (GZ + \xi) - \cov(GZ + \xi, Y) \Sigma_{YY}^{-1} \cov (Y, GZ + \varepsilon)\\
            &= G \Sigma_{ZZ}G^{T} + \Xi - G \Sigma_{ZY}\Sigma_{YY}^{-1}\Sigma_{YZ}G^{T}\\
            &= \Xi + G \cov(Z|Y) G^{T}.
    \end{align*}
\end{proof}

After having observed $Y = y$, our main interest lies in the conditional distribution of states $X$ given $Y= y$, which we could obtain by applying \Cref{lem:gaussian_conditional}, i.e. where $B = \bdiag (B_{0}, \dots, B_{n})$ and $\Omega = \bdiag \left( \Omega_{0}, \dots, \Omega_{n} \right)$. However, this would require inversion of the $(n+1)p\times(n+1)p$ matrix $\left( B\Sigma B + \Omega \right)$ which becomes numerical infeasible quickly. Instead, we can exploit the sequential structure of the \acrshort{glssm}, which will allow us to perform conditioning on only a single observation at a time. 

To this end, let us denote by $\hat X_{t | s}$ the conditional expectation of $X_{t}$ given a set of observations $y_{:s}$ and by $\Xi_{t | s}$ the conditional covariance matrix of $X_{t}$ given $Y_{:s} = y_{:s}$. Then $$X_{t} | Y_{:s} = y_{:s} \sim \mathcal N \left( \hat X_{t|s}, \Xi_{t|s} \right).$$ For a given $t$, three values of $s$ are of particular interest: If $s = t - 1$ determining this conditional distribution is called a \textbf{prediction problem}, if $s = t$ this is a \textbf{filtering problem} and if $s = n$ a \textbf{smoothing problem}, and we call the distributions we seek the \textbf{predictive, filtering} or \textbf{smoothing distribution} respectively. 
Similarly we define $\hat Y_{t|s} = \E \left( Y_{t} \middle| Y_{:s} = y_{:s} \right)$ to be the conditional expectation of $Y_{t}$ given $Y_{:s}=y_{:s}$, note that $\hat Y_{t|s} = Y_{t}$ if $s \geq t$. Finally, let $\Psi_{t|s} = \cov \left( Y_{t} | Y_{:s} = y_{:s} \right)$ be the conditional covariance matrix of $Y_{t}$ given $Y_{:s} = y_{:s}$. Again $\Psi_{t|s} = 0$ if $s \geq t$. 

These distributions may be obtained efficiently using the celebrated Kalman filter (\Cref{alg:kalman_filter}) and smoother (\Cref{alg:kalman_smoother}) algorithms, which we state here for completeness.

\begin{algorithm}
    \caption{Kalman filter, with runtime $\mathcal O(n(m^{2} + p^{3}))$}
    \label{alg:kalman_filter}
    \begin{algorithmic}[1]
        \Require \gls{glssm} (\Cref{def:glssm}), observations $y_{0}, \dots, y_{n}$.
        \State $A_{-1} \gets I \in \mathbf R^{m\times m}$ \Comment{Identity Matrix}
        \State $u_{-1} \gets \mathbf 0 \in \mathbf R^{m}$ 
        \State $\hat X_{-1|-1} \gets \E X_0$
        \State $\Xi_{0|-1} \gets \mathbf 0_{m\times m}$
        \State $\ell_{-1} \gets 0$
        \For{$t \gets 0, \dots, n$}
            \State\label{step:kf_loop} $\hat X_{t| t - 1} \gets A_{t-1} \hat X_{t-1|t-1} + u_{t-1}$ \Comment{prediction}
            \State $\Xi_{t | t - 1} \gets A_{t - 1} \Xi_{t - 1 | t - 1 } A_{t - 1}^{T} + \Sigma_{t}$ 
            \State $\hat Y_{t|t - 1} \gets B_{t}\hat X_{t | t - 1} + v_{t}$
            \State $\Psi_{t|t - 1} \gets B_{t}\Xi_{t | t - 1} B_{t}^T + \Omega_{t}$
            \State $K_t \gets \Xi_{t | t - 1} B_{t}^T \Psi_{t | t - 1} ^{-1}$ \Comment{filtering}
            \State $\hat X_{t | t} \gets \hat X_{t | t - 1} + K_t (y_{t} - \hat Y_{t | t - 1})$
            \State $\Xi_{t| t } \gets \Xi_{t | t - 1} - K_t \Psi_{t| t - 1} K_t^T$
            \State $\ell_{t} \gets \ell_{t - 1} + \frac{p}{2} \log (2\pi) + \frac{1}{2}\log\det \Psi_{t|t -1} + \frac{1}{2} \left( y_{t} - \hat Y_{t | t - 1} \right)^{T} \Psi_{t|t-1}^{-1} \left( y_{t} - \hat Y_{t | t - 1} \right) $ \Comment{NLL}
        \EndFor
    \end{algorithmic}
\end{algorithm}

In \Cref{alg:kalman_filter} every time point $t = 0, \dots, n$ is processed in the same way, with a two-step procedure: first we predict the new observation $Y_{t}$ based on $Y_{:t-1}$. Using the linearity of the system as well as the assumed conditional independence, this is achieved by applying the system dynamics to the current conditional expectation and covariance matrices. After $Y_{t}$ has been observed, we can update the conditional distribution of the states by appealing to \Cref{lem:gaussian_conditional}. For a rigorous derivation of the Kalman filter, we refer the reader to \citep[Chapter 4]{Durbin2012Time} or the excellent monograph of \citep{Schneider1986Kalmanfilter}. 

The Kalman filter is very efficient: each loop iteration requires inversion of the $p \times p$ matrix $\Psi_{t | t - 1}$. Assuming this operation dominates the time complexity, e.g. because $m \approx p$, the time complexity of the Kalman filter is $\mathcal O(n\,m^{3})$, a drastic improvement over the naïve $\mathcal O(n^{3}\,m^{3})$, obtained by applying \Cref{lem:gaussian_conditional} to the joint distribution of $(X,Y)$. Similarly, the space complexity of \Cref{alg:kalman_filter} is $\mathcal O \left( n \left( m^{2} + p^{2} \right) \right)$, and grows only linearly in the number of time steps $n$.

\todo{comment on Yt depending on Yt-1}
Notice that the Kalman filter iteratively calculates the negative log-likelihood $\ell_{t}$
$$\ell_{t} = - \log p(y_{:t}) = - \log \sum_{s = 0}^t \log p(y_{s} | y_{:(s - 1)})$$ 
while filtering. This is possible because of the dependency structure of the \acrshort{glssm}, which makes the increments in $\ell_{t}$ tractable, as
$$
Y_{s} | Y_{:(s -1)} \sim \mathcal N \left( \hat Y_{s|s-1}, \Psi_{s|s - 1} \right),
$$
for $s = 0, \dots, n$, which is shown in the derivation of the Kalman filter. Thus, the Kalman filter enables us to perform \acrshort{mle} by giving us access to $\ell_{n}$.

\todo{historical comment}
Depending on the situation at hand, one of the many variants of the basic algorithm presented in \Cref{alg:kalman_filter} may be used. If the inversion of $\Psi_{t|t-1}$ is numerically unstable, the filtered covariance matrices $\Xi_{t|t}$ may become numerically non-positive definite. In this case, the square root filter and smoother \citep{Morf1975Squareroot} may be used. It is based on Cholesky roots of the involved covariance matrices, ensuring them to be \acrshort{psd}.

When the dimension of observations is much larger than that of the states, $p \gg m$, the information filter \citep{Fraser1969Optimum} can be used. Instead of performing operations on the covariance matrices, i.e. $\Xi_{t|t-1}$ and $\Psi_{t|t-1}$, the information filter operates on their inverses, the precision matrices $\Xi_{t|t - 1}^{-1}$ and $\Psi_{t|t-1}^{-1}$ as well as rescaled states $\Xi_{t | t - 1}^{-1}
\hat X_{t | t - 1}$ and observation $\Psi_{t | t-1}^{-1}\hat Y_{t|t -1}$ estimates. This makes the filtering step more efficient, as the computationnaly most intensive step is the calculation of $\Psi_{t | t- 1}^{-1}$. However the price one pays is that the prediction step now requires inversion of a $m\times m$ matrix, and as such the computational gains only set in when $p$ is sufficiently large compared to $m$ \cite{Assimakis2012Information}. Note that for the models we consider in \Cref{cha:analysis_of_selected_models} this is usually not the case. \todo{check that this really holds}

If the dimensions of the model are so large that calculating the $m\times m$ and $p\times p$ covariance matrices becomes an issue, the simulation based \acrfull{enkf} \citep{Evensen1994Sequential} can be used. Instead of calculating the covariance matrices analytically, the \acrshort{enkf} stores a particle approximation to the Gaussian filtering distribution and iteratively performs a prediction and update step with a particle approximation, similar to the analytical update the Kalman filter performs. Despite being based on linear Gaussian dynamics, the \acrshort{enkf} is successfully employed in many high-dimensional non-linear non Gaussian problems \citep{Katzfuss2016Understanding}. 

For non-linear problems of moderate dimension, i.e. those where we replace the right-hand side of both state (\Cref{eq:glssm_states}) and observation (\Cref{eq:glssm_observations}) equations by non-linear functions, other variants such as the \acrfull{ekf} \citep{Jazwinski1970Stochastic} and the \acrfull{ukf} \citep{Julier1997New} may be used. The \acrshort{ekf} applies the Kalman filter to a linearization of the non-linear system around the current conditional means $\hat X_{t| t-1}$ and $\hat X_{t|t}$. If the systems dynamics are highly non-linear, this approximation can fail. Alternatively, the \acrshort{ukf}, which is based on the unscented transform, directly approximates the predicted means and covariance matrix, by constructing a set of deterministic points that are propagated through the systems dynamics. \todo{more on this}

In the context of \acrshort{c19}, variants of the Kalman filter have been employed to analyse the time-varying behavior of epidemiological parameters. Usually the models start from some theoretical, e.g. compartmental, model of how the epidemic spreads. After time-discretization and possibly linearization, one ends up with a \acrshort{glssm}, to which the Kalman filter or one of its variants may be applied. 
In \citep{Arroyo-Marioli2021Tracking} the authors construct a simple \acrshort{glssm} to reconstruct the time-varying reproduction number from observed growth factors, exploiting the linear relationship between the two quantities in the SIR compartmental model and using the Kalman filter and smoother to perform inference. 
\citep{Zhu2021Extended,Song2021Maximum} directly apply the \acrshort{ekf} to time-discretized compartmental models, fitting them either to simulated \citep{Zhu2021Extended} or real \citep{Song2021Maximum} data. Similarly, \citep{Engbert2020Sequential} use the \acrshort{enkf} to fit a stochastic compartmental model to German regional data, where the \acrshort{enkf} allows to deal with the non-linear and non-Gaussian properties on these small spatial scales.

% history: moon
% epidemiological uses 
\todo{algorithmen konsistent mit gets und =, return value}
\todo{algorithmen konsistent t, t -1}
\begin{algorithm}
    \caption{Kalman smoother. Note that the Kalman filter already outputs the smoothed last state $\hat X_{n|n}$ and covariance $\Xi_{n|n}$.}
    \label{alg:kalman_smoother}
    \begin{algorithmic}[1]
        \Require \acrshort{glssm} (\Cref{def:glssm}), outputs from Kalman filter (\Cref{alg:kalman_filter})
        \For{$t \gets n - 1, \dots, 0$}
            \State $G_{t} = \Xi_{t|t} A_{t}\Xi_{t+1|t}^{-1}$
            \State $\hat X_{t | n} = \hat X_{t|t} + G_{t} \left( \hat X_{t + 1|n} - \hat X_{t + 1|t} \right)$
            \State $\Xi_{t|n} = \Xi_{t|t} - G_{t} \left( \Xi_{t + 1|t} - \Xi_{t + 1|n} \right)G_{t}^T$
        \EndFor
    \end{algorithmic}
\end{algorithm}

The Kalman smoother (\Cref{alg:kalman_smoother}) computes the marginal distributions $X_{t} | Y$ for $t = 0, \dots, n$. Upon closer inspection, the mean and covariance updates resemble that of the Kalman filter (\Cref{alg:kalman_filter}). This is no coincidence: By the assumed dependence structure, we obtain the following lemma, which will allow us to prove the recursions.
\todo{reference Chopin here}
\begin{lemma}[conditional indepdence from future observations]
    Let $t \in \{0, \dots, n - 1\}$ and $s > t$. In a \acrshort{glssm}, conditional on $X_{t + 1}$, $X_{t}$ is independent of $Y_{s}$, $s > t$. 
\end{lemma}
\begin{proof}
    As $s > t$, we have
    $$
    p(x_{t}, y_{s} | x_{t + 1}) = p(y_{s}| x_{t + 1}, x_{t}) p(x_{t} | x_{t + 1}) = p(y_{s} | x_{t + 1}) p(x_{t} | x_{t + 1})
    $$
    where the second equality follows from the dependency structure of the model. 
\end{proof}

We can now sketch the proof for the Kalman smoother recursions, based on the arguments in \citep[Chapter 7.3]{Chopin2020Introduction}. By the preceding lemma, the conditional distribution of $X_{t}$ given $Y_{:n}$ and $X_{t + 1}$ is the same as that given $Y_{:t}$ and $X_{t + 1}$. 
We may now regard $X_{t + 1} = A_{t}X_{t} + u_{t} + \varepsilon_{t + 1}$ as an additional observation at time $t$, and use the Kalman filter update to determine this conditional distribution:
$$
X_{t} | Y_{:n} = y_{:n}, X_{t + 1} = x_{t+1}\sim \mathcal N \left(\hat X_{t|t} + G_{t}(x_{t + 1} - \hat X_{t + 1 | t}), \Xi_{t |t} - G_{t} \Xi_{t + 1 | t} G_{t}^{T} \right).
$$
As $\hat X_{t|t}$ and $\hat X_{t+1|t}$ are linear functions of $Y_{:n}$ (actually $Y_{:t}$), we may apply the last statement of \Cref{lem:gaussian_conditional}, to see that, conditional on $Y_{:n} = y_{:n}$, the distribution of $X_{t}$ is Gaussian with mean
$$
\hat X_{t|t} + G_{t} \left( \hat X_{t + 1 | n} - \hat X_{t + 1 | t} \right)
$$
and covariance matrix 
$$
\Xi_{t | t} - G_{t} \Xi_{t + 1|t} G_{t}^{T} + G_{t} \Xi_{t + 1 | n} G_{t}^{T} = \Xi_{t|t} - G_{t} \left( \Xi_{t + 1 | t} - \Xi_{t + 1 | n} \right)G_{t} ^{T}.
$$
These quantities are calculated by the Kalman smoother (\Cref{alg:kalman_smoother}).

Going back to the proof of the last statement in \Cref{lem:gaussian_conditional}, we see that we may actually write
\begin{align}
    \label{eq:kalman-smoother-backwards-recursion}
    X_{t} &= \hat X_{t|t} + G_{t}(X_{t + 1} - \hat X_{t + 1 | t}) + \xi_{t},
\end{align}
for a $\xi_{t} \sim \mathcal N \left( 0, \Xi_{t | t} - G_{t} \Xi_{t + 1|t}G_{t} \right)$ which is independent of $Y_{:n}$ and $X_{t + 1}$.
This recurrence may be used to generate samples from the joint smoothing distribution, which is useful if one is interested in non-linear functionals of the smoothing distribution that involve multiple states at once, such as a moving median or maximum. It is based on the following decomposition of the smoothing density
$$
p(x|y) = p(x_{n}|y) \prod_{t = n - 1}^0 p(x_{t}|x_{t+1}, y_{:t}).
$$
The resulting algorithm is called the \gls{ffbs} (\Cref{alg:ffbs}) and was first described in \citep{Fruhwirth-Schnatter1994Data} in the context of a data augmentation algorithm for Bayesian analysis of \acrshort{glssm}. In this paper, the hyperparameters $\theta$ follow an inverse gamma distribution, and one is interested in the posterior marginals of $p(\theta|Y)$, e.g. to determine the posterior marginals of states $p(X|Y)$ using \acrshort{mcint}.

\begin{algorithm}
    \begin{algorithmic}[1]
        \Require \acrshort{glssm} (\Cref{def:glssm}), outputs from Kalman filter (\Cref{alg:kalman_filter})
        \State Simulate $\check X_{n|n} \sim \mathcal N(\hat X_{n|n}, \Xi_{n|n})$
        \For{$t \gets n-1, \dots, 0$}
            \State $G_{t} = \Xi_{t|t}A_{t}\Xi^{-1}_{t + 1 | t}$
            \State Simulate $\xi_{t} \sim \mathcal N(0, \Xi_{t|t} - G_{t}\Xi_{t+1|t}G^{T}_t)$
            \State Set $\check X_{t|n} = \hat X_{t|t} + G_{t} \left( \check X_{t + 1} - \hat X_{t + 1| t} \right) + \xi_{t}$
        \EndFor
    \end{algorithmic}
    \caption{Forwards filter, backwards smoother \citep[Proposition 1]{Fruhwirth-Schnatter1994Data}} \label{alg:ffbs}
\end{algorithm}

% comment on regularity assumptions
\begin{remark}[regularity of $\Sigma_{t}$ and $\Omega_{t}$]
    \label{rem:regular_covs}
    Throughout this section, we have assumed, either explicitly or implicitly, that the innovation and observation covariance matrices $\Sigma_{t}$ and $\Omega_{t}$ are non-singular, i.e. \acrshort{spd}.

    % filtering
    For the Kalman filter we require that for every $t$, $\Psi_{t|t-1}$ is non-singular, i.e. that we can apply \Cref{lem:gaussian_conditional} \ref{it:cond_gaussian}. This is fulfilled as soon as $\Omega_{t}$ is non-singular, which is a reasonable assumption in most models. Following the remark after \Cref{lem:gaussian_conditional}, we could also replace $\Psi_{t|t-1}^{-1}$ in \Cref{alg:kalman_filter} by its Moore-Penrose inverse. 

    % smoothing
    A similar argument can be made for singular $\Xi_{t+1|t}$, where we replace $\Xi_{t+1|t}^{-1}$ by its Moore-Penrose inverse in the Kalman smoother (\Cref{alg:kalman_smoother}) and the \acrshort{ffbs} (\Cref{alg:ffbs}). 
\end{remark}

% comment on simulation smoother

The attractive feature of \glspl{glssm} is that a large part of inference is analytically feasible: we may calculate the likelihood, smoothing distribution and sample from it. 
However, the modeling capacity of \glspl{glssm} is limited: most interesting phenomena in the context of this thesis follow neither linear dynamics nor are well modeled by a Gaussian distribution.

Nevertheless, linearization of non-linear dynamics suggests that \gls{glssm}s may have some use as approximations to these more complicated phenomena, provided they are sufficiently close to Gaussian models, e.g. unimodal and without heavy tails.
We start to move away from linear Gaussian models by allowing observations that are non-Gaussian.