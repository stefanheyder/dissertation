\section{Gaussian importance sampling for state space models}
\label{sec:gaussian_importance_sampling_for_state_space_models}

% two types of proposals: direct, SSMs
For the types of models considered in this thesis, importance sampling is used to infer the posterior distribution. Given a state space model of the form \eqref{def:ssm} and observations $Y = Y_{:n}$, let $\P$ be the distribution of the states $X=X_{:n}$, conditional on $Y$ and $f$ be a function of interest. The task at hand is now to find a suitable proposal $\G$, using the methods presented in the last section. If $n$ is large, the posterior distribution lives in a high dimensional state of dimension $m\cdot n$, so to obtain $\G$ efficiently, we should exploit the available structure. Additionally, we want $\G$ to be tractable, so simulating from it is possible and evaluating the weights $w$ up to a constant is possible. 

The multivariate Gaussian distribution is a good candidate in this setting, as simulating from it is straightforward and its density can be evaluated analytically. However, naively performing the optimal importance sampling methods from the previous section for all multivariate Gaussians is computationally inefficient as the family of distributions has $\mathcal O((n\cdot m)^{2})$ many parameters. We can, however, exploit the available structure of the \gls{ssm} to find parameterizations with fewer parameters by either using smoothing distributions of \glspl{glssm} (\Cref{subsec:glssm-approach}) or approximating with a Gaussian discrete-time Markov process (\Cref{subsec:markov-approach}). 

% discuss if there actually is a Gaussian close to the target, see heavy tails etc.
Using Gaussian proposals, while computationally efficient, also comes with some drawbacks. The whole procedure hinges on the assumption that there is a Gaussian that is close to the target distribution. In the setting of \acrshortpl{ssm} this is not guaranteed, as the targets may contain multiple modes or heavy tails, features that may, in the worst case, lead to inconsistent importance sampling estimates. %At least for the models considered in \Cref{cha:analysis_of_selected_models}, the targets arise as posterior distributions of \gls{egssm} and as such they are unimodal and have non-heavy tails, i.e. there is a Gaussian distribution such that importance sampling is feasible \todo{rethink, is this really the case, for LCSSM it depends on log-partition function of observations}. 
Additionally, even if there is a Gaussian distribution that facilitates consistent importance sampling, finding it in practice may be complicated, as the proposals generated by the \gls{la}, \gls{cem} and \gls{eis} have deteriorating performance for fixed sample size $N$ (in terms of \acrshort{ess} and convergence) with increasing dimension, see \Cref{subsec:performance_at_optimal}.

Using a \acrshort{glssm} as an importance sampling proposal for non-Gaussian state space models was introduced by \citep{Durbin1997Monte} to facilitate maximum likelihood estimation using the \acrshort{la} as a proposal. Concurrently, \citep{Shephard1997Likelihood} established a similar result in the context of \acrshort{mcmc} analysis of \acrshortpl{ssm}.

% first: Gaussian SSMs
\subsection{\texorpdfstring{The \gls{glssm}-approach}{The GLSSM-approach}}
\label{subsec:glssm-approach}
The first approach is motivated by the fact that the target posterior is again a Markov process, as are posteriors in \glspl{glssm}. Additionally, the posterior distribution in \gls{glssm}s is again Gaussian, and straightforward to simulate from by, e.g., the FFBS algorithm (\Cref{alg:ffbs}) or the simulation smoother \citep{Durbin2002Simple}. Thus parameterizing the proposals $\G$ by the posterior of a suitably chosen \gls{glssm} may be a fruitful approach.
For the models we consider in this thesis, the distribution of states is already Gaussian and the observations are conditionally independent given the states. Thus a natural \gls{glssm} to use as a proposal consists of keeping the prior distribution of states and replacing the distribution of observations with conditionally independent Gaussian distributions and the actual observations by synthetic ones. By the assumed conditional independence, this model only needs $2 p\cdot (n + 1)$ many parameters, $p\cdot (n + 1)$ for the synthetic observations and $p\cdot (n + 1)$ for their variances. We term this approach the \textbf{\gls{glssm}-approach} to importance sampling.

In total, the \gls{glssm}-approach considers parametric proposals $\G_{\psi}$ of the form
\begin{align}
    \begin{split}
    \label{eq:glssm-proposal}
    \G_{\psi} &= \mathcal L(X | Z = z),\\
    Z_{t} &= B_{t} X_{t} + \eta_{t},\\
    \eta_{t} &\sim \mathcal N \left( 0, \Omega_{t} \right),\\
    \Omega_{t} &= \diag \left( \omega^{2}_{t} \right) = \diag \left( \omega^{2}_{t,1}, \dots, \omega^{2}_{1,p} \right).
    \end{split}
\end{align}
where the distribution of $X$ is given by \eqref{eq:glssm_states}, $\psi = \left( z, \omega^{2} \right)$ for $z = \left( z_{0}, \dots, z_{n} \right) \in \R^{(n+1) \times m}$ and $\omega^{2} = \left( \omega^{2}_{0}, \dots, \omega^{2}_{n} \right) \in \R^{(n+1) \times m}$. Alternatively the natural parametrization 
\begin{align}
    \label{eq:eis_glssm_natural_parametrization}
    \psi &= \left( z \oslash \omega^{2}, - 1 \oslash \left( 2 \omega^{2} \right) \right)
\end{align}
may also be used, where $\oslash$ is the Hadamard, i.e. entry-wise, division. Simulation from $\G_{\psi}$ may be efficiently implemented by the FFBS algorithm, as $\G_{\psi}$ is the smoothing distribution of a \gls{glssm}. 

% weights
In this setting, the importance sampling weights are given by 
$$
w(x) = \frac{p(x|y)}{g(x|z)} = \frac{p(y|x)p(x)}{g(z|x)p(x)} \frac{g(z)}{p(y)} \propto \prod_{t = 0}^n \frac{p(y_{t}|x_{t})}{g(z_{t}|x_{t})},
$$
% Signals
so they can be computed efficiently. Additionally, for a \acrshort{egssm} with linear signals, $p(y_{t}|x_{t})$ and $g(z_{t}|x_{t})$ depend on $x_{t}$ only through the signal $s_{t} = B_{t}x_{t}$, and we have 
\begin{align}
\label{eq:weights_only_on_signal}
w(x) &\propto \prod_{t = 0}^{n}\frac{p(y_{t}|s_{t})}{g(z_{t}|s_{t})},
\end{align}
which implies that auto-normalized weights may be calculated by using the signal smoother \citep[Theorem 2]{Jungbacker2007Monte}.
As \citep{Durbin2012Time} \citep[Section 4.5.3]{Durbin2012Time} argue, it is often computationally more efficient to treat only on the signals $S_{:n}$ instead of the states $X_{:n}$, the idea being that the dimension of $S_{t}$, $p$, is usually much smaller than that of $X_{t}$, $m$. 

% sample from states still possbile if doingo nly signals, weights don't change
As the joint distribution of $(X, S)$ is a Gaussian distribution, by \Cref{lem:gaussian_conditional} $X|S = s$ is again Gaussian,
%\todo{add degernate case to gaussian conditional lemma}
with known conditional mean and covariance matrix and density $p(x|s) = g(x|s)$. If $(\tilde X_{t})_{t=0,\dots,n}$ is a draw from this conditional distribution a quick calculation reveals that a.s. $B_{t} \tilde X_{t} = S_{t}$, and so, as expected, the weights $w(\tilde X_{t})$ are a.s. constant and given by (up to the integration constant) \Cref{eq:weights_only_on_signal}. Producing a draw from this conditional distribution can be achieved by the FFBS algorithm (\Cref{alg:ffbs}), as $(X, S)$ form a \gls{glssm} with degenerate observation covariance matrices $\Omega_{t} = 0$.

By the assumed conditional independence of observations given signals, we have
\begin{align}
\label{eq:two_step_sample_ssms}
p(x, s|y) \propto p(x|s) p(s|y),
\end{align}
and so if one is interested in the states, rather than the signals, importance sampling with the proposal \Cref{eq:glssm-proposal} can be achieved in a two-step procedure: first sample from $g(s|z)$, then run the FFBS algorithm to sample from $g(x|s) = p(x|s)$ using the same weights for MC-integration. 

% degenerate distribution, exponential family of proposals for signals
The \gls{glssm}-approach is the standard approach for finding the \gls{la} in \gls{egssm} \citep{Durbin1997Monte,Durbin2012Time} and may even be applied when the observation densities are not log-concave as \citep{Jungbacker2007Monte} show. The approach also leads to efficient implementation for \gls{eis} \citep{Koopman2019Modified}. However, as will become apparent in the later part of this section, it is infeasible for the \gls{cem} if $n$ is large. 

We now give a concise overview over how to perform the \gls{la} and \gls{eis} for \gls{egssm}, but refer the reader for more details to the respective literature.
\citep{Danielsson1993Accelerateda} were the first to propose minimizing the variance of log-weights, which developed into the \acrshort{eis} method \citep{Liesenfeld2003Univariate, Richard2007Efficient}. However, as formulated in these earlier works, \acrshort{eis} requires careful tracking of integration constants.  For \acrshortpl{pgssm}, the modified \acrshort{eis} of \citep{Koopman2019Modified} provides a more straightforward approach to determine the proposal distribution, by noticing that it may be written as the posterior of an appropriately chosen \acrshort{glssm}, whose distribtion of states coincides with that of the original \acrshort{pgssm}. Instead of approximating the integrals in \acrshort{eis} using \acrshort{mcint}, \citep{Koopman2015Numerically} suggest to do so using numerical integration, to reduce sampling error.

The \acrshort{la} for \acrshort{pgssm} can be obtained efficiently, by noticing that the Newton-Raphson scheme to obtain the posterior mode (\Cref{eq:LA_approximation}) can be implemented using the Kalman smoother, see \citep[Chapter 10]{Durbin2012Time}. For \acrshort{egssm} with a linear signal, the \acrshort{la} is particularly easy to implement, as the conditional independence of individual observations translates to an approximating \acrshort{glssm} with independent observations as well ---  the resulting algorithm is presented in \Cref{alg:la}.

\begin{algorithm}
    \caption{The \gls{la} for \gls{egssm}}
    \label{alg:la}
    \begin{algorithmic}[1]
        \Require \acrshort{egssm} (\Cref{def:egssm}) with linear signal and natural parameters $s_{t}$, $t = 0, \dots, n$, observations $y_{0}, \dots, y_{n}$, initial values $\psi^{0} = \left( z^{0}, (\omega^{2})^{0} \right)$
        \State Set $l = 0$.
        \Repeat
            \State Run the Kalman smoother \Cref{alg:kalman_smoother} for the model \eqref{eq:glssm-proposal} to obtain $\hat s = \E_{\G_{\psi^{l}}} \left( S | Z = z^{l} \right)$.
            \For{$t = 0, \dots, n$}
                \State Set $(\omega^{l + 1})^{2}_{t,i} = H_{s_{t}^{i}} Z_{t} (s_{t}^i)$ \Comment{Hessian evaluated at $\hat s_{t}$}
                \State Set $\Omega^{l + 1}_t = \diag \left( (\omega^{2})^{l + 1}_{t, 1}, \dots, (\omega^{2})^{l+1}_{t, p}\right)$. 
                \State Set $z_{t}^{l + 1} = z_{t}^{l} - (\Omega^{l + 1}_{t})^{-1} \left( \nabla \log Z_{t}(s_{t}^{1}), \dots, \nabla \log Z_{t}(s_{t}^p)) \right)$. \Comment{Gradient evaluated at $\hat s_{t}$} 
            \EndFor
            \State Set $\psi^{l + 1} = \left( z^{l + 1} \oslash (\omega^{2})^{l + 1}, -1 \oslash (\omega^{2})^{l + 1}\right)$.
            \State Set $l = l +1$.
        \Until{$\psi^{l}$ has converged.}
        
    \end{algorithmic}
\end{algorithm}

As we will show in \Cref{sec:simulation_studies}, the \acrshort{la} may provide poor performance as a proposal, when the dimension of the \acrshort{pgssm} grows. In this case, the \acrshort{eis} proposal may perform better. Recall from \Cref{subsec:eis}, that \acrshort{eis} aims at minimizing the mean squared error between $s \mapsto \log p(s|y)$, the target log-density and $s\mapsto\log g_{\psi}(s|z)$, the proposals log-density with respect to the target. Thus in the context of \acrshortpl{pgssm}, \acrshort{eis} aims to minimize
$$
(z, \omega^{2}) \mapsto \Pps\left[\left( \log p(\cdot|y) - \log g(\cdot|z)\right)^{2}\right].
$$
By reformulating the integrand to $\left(\log p(y|s) - \log g(z|s) -  \log p(y)  + \log g(z)\right)^{2}$ and following the discussion surrounding \Cref{eq:eis_exponential_families}, we may instead minimize
$$
    \Pps \left[\left( \log p(y|\cdot) - \log g(z|\cdot) - \lambda \right)^{2}\right] = \Pps \left[ \left(\sum_{t = 0}^n \log p(y_{t}|\cdot_{t}) - \log g(z_{t}|\cdot_{t})  - \lambda \right)^{2} \right].
$$
over the parameters of interest, $(z, \omega^{2})$, and the nuisance parameter $\lambda$.

However, the dimension of $\psi$ is quite high ($2p(n+1)$), so one resorts to solving the lower dimensional problems 
\begin{align}
    \label{eq:eis_marginals_problem}
    \Pps \left[ \left( \log p(y_{t}|\cdot_{t}) -  \log g(z_{t}|\cdot_{t}) - \lambda_{t} \right)^{2} \right] = \mathbb P^{X_{t}|Y=y} \left[ \left( \log p(y_{t}|\cdot_{t}) -  \log g(z_{t}|\cdot_{t}) - \lambda_{t} \right)^{2} \right],
\end{align}
which only depends on the marginal of $X_{t}$. While this only yields approximate solutions to the problem at hand, it turns out that the proposals produced by this procedure perform well (see \Cref{sec:simulation_studies}).

\acrshort{eis} for \acrshortpl{pgssm} then proceeds as in \Cref{subsec:eis}, using importance sampling to obtain a particle approximation $\hat \P_{N}$ to $\Pps$ and solving the resulting approximate problem. As 
$$
    \log g(z_{t}|s_{t}) = - \frac{1}{2} (z_{t} - s_{t})^{T}\Omega_{t}^{-1}(z_{t} - s_{t})- \frac{p}{2}\log (2\pi) - \frac{1}{2} \log\det \Omega_{t},
$$
is affine in the natural parameters $\left( z \oslash \omega^{2}, -1 \oslash (2\omega^{2}) \right)$, the problem reduces to weighted linear least squares problem, which can be solved analytically. If the model at hand is an \acrshort{egssm} with linear signal\footnote{Actually, we do not require the model to be an \acrshort{egssm}, but only that the univariate marginals are conditionally independent.}, we can further exploit the independence of marginals, to solve only for the univariate marginals, i.e. minimize

\begin{align}
    \label{eq:eis-marignals-target}
    (z_{t,i}, \omega^{2}_{t,i}, \lambda_{t,i}) \mapsto \Pps \left[ \left( \log p(y_{t,i}|s_{t,i}) - \log g(z_{t,i}|s_{t,i}) - \lambda_{t,i} \right)^{2} \right],
\end{align}
or an importance sampling version of it.

We present the resulting algorithm in \Cref{alg:eis}, restricted to the case where the model is an \acrshort{egssm} with linear signal. As starting values $\psi^{0}$ we may take those obtained by the \acrshort{la}. 

\begin{algorithm}
    \caption{\gls{eis} for \acrshortpl{egssm} with linear signal}
    \label{alg:eis}
    \begin{algorithmic}[1]
        \Require \acrshort{egssm} (\Cref{def:egssm}) with linear signal and natural parameters $s_{t}$, $t = 0, \dots, n$, observations $y_{0}, \dots, y_{n}$, initial values $\psi^{0} = \left( z^{0}, (\omega^{2})^{0} \right)$, number of samples $N$.
        \State Set $l = 0$.
        \Repeat
            \State Run the FFBS (\Cref{alg:ffbs}) to obtain samples of the signals $(S^{i})_{i = 1, \dots, N}$. 
            \For{$t = 0, \dots, n$}
                \For{$ i = 1, \dots, p$}
                    \State Solve \Cref{eq:eis-marignals-target} for $z^{l + 1}_{t,i}$ and $(\omega^{2})^{l + 1}_{t,i}$ \Comment{Using $\hat \P_{N}$}
                \EndFor
            \EndFor
            \State Set $\psi^{l + 1} = \left( z^{l + 1} \oslash (\omega^{2})^{l + 1}, -1 \oslash (\omega^{2})^{l + 1}\right)$.
            \State Set $l = l +1$.
        \Until{$\psi^{l}$ has converged.}
        
    \end{algorithmic}
\end{algorithm}

% CE: two variants 
%% GLSSM approach fails: 
For the \gls{cem}, using the \gls{glssm}-approach turns out to be difficult numerically. For a high-level argument of why this is true, let us ignore the Markov structure of the model for the moment. As the \gls{cem} matches moments of the target and proposal, applying it to fit model \eqref{eq:glssm-proposal} amounts to matching first and second order moments of $\G_{\psi}$ to those of the target posterior $\mathcal L (X | Y = y)$ in the \gls{ssm}. Unfortunately, the covariance of $\G_{\psi}$ is given by $ \left( \Sigma^{-1} + B^{T}\Omega^{-1} B \right)^{-1}$, where $\Sigma$ is the covariance of all states, $B = \bdiag (B_{0}, \dots, B_{n})$ and $\Omega = \bdiag \left( \Omega_{0}, \dots, \Omega_{n} \right)$ (see, e.g., \citep[Section 4.13]{Durbin2012Time}). Choosing the diagonal matrix $\Omega$ such that the covariance of $\G_{\psi}$ matches this expression is numerically expensive: we either need to invert the large (dimension $(n + 1)m \times (n + 1)m$) covariance matrix, or solve numerically for the $(n + 1)p$ parameters. The problem at hand is that we cannot decouple this into $(n + 1)$ equations of dimension $p$ (or even $(n+1)p$ equations) as we did for \gls{eis}, because all entries of $(\Sigma^{-1} + B^{T}\Omega^{-1} B)^{-1}$ depend on all entries of $\Omega$. 

To make matters more concrete, the \gls{cem} finds $\psi = (z, \omega^{2})$ such that model \eqref{eq:glssm-proposal} maximizes the cross entropy with the target $\P^{X|Y=y}$. For simplicity, let us assume that $m = p$, $B$ is the identity and we only observe a single $y$. Using \Cref{lem:gaussian_conditional}, we see that when $X\sim\mathcal N(\mu, \Sigma)$, the conditional distribution of $X$ given $Z=z$, $\G_{\psi}$, is a Gaussian distribution with mean $\tilde \mu =  \mu + \Sigma \left( \Sigma  + \Omega \right)^{-1} \left( z - \mu \right)$  and covariance matrix $\tilde\Sigma = \left( \Sigma ^{-1} + \Omega^{-1}\right)^{-1}$ for $\Omega = \diag \left( \omega^{2} \right)$, where $\omega^{2} > 0$ entry-wise. Assuming that $\Sigma$ is non-singular, we can reparameterize the objective function of the \gls{cem} by $\tilde \mu$,
\begin{align}
    \label{eq:cem_reparametrization}
    \begin{split}
    \max_{z, \omega^{2}} \int p(s|y) \log g_{\psi}(s|z) \mathrm dx &= \max_{\tilde\mu, \omega^{2}} \int p(s|y) \left( - \frac{1}{2} (s - \tilde \mu)^{T} \tilde \Sigma ^{-1} \left( s - \tilde \mu \right)  - \frac{1}{2} \log\det \tilde \Sigma \right)  \d x\\
&= \max_{\tilde \mu, \omega^{2}} - \frac{1}{2} (\gamma - \tilde \mu)^{T}\tilde \Sigma ^{-1} ( \gamma - \tilde \mu) - \frac{1}{2} \operatorname{trace} \left( \tilde \Sigma^{-1} \Gamma \right) - \frac{1}{2} \log\det\tilde\Sigma,
    \end{split}
\end{align}
where $\gamma = \E \left( X | Y = y \right)$ and $\Gamma = \cov \left( X | Y = y \right)$. 
Thus the optimal $\tilde \mu$ is $\gamma$ and to find the optimal $\omega^{2}$ we have to minimize 
$$
\operatorname{trace} \left( \left( \Sigma^{-1} + \Omega^{-1} \right) \Gamma \right) - \log\det \left( \Sigma^{-1} + \Omega^{-1} \right).
$$
Taking the derivative w.r.t. $\frac{1}{\omega^{2}}$, we see that 
\begin{align}
\label{eq:gamma_post}
\Gamma_{i,i} = \left(\left( \Sigma^{-1} + \diag \left( \frac{1}{\omega_{1}}, \dots, \frac{1}{\omega_{p}}\right) \right)^{-1}\right)_{i,i} = \left( \Sigma - \Sigma \left( \Sigma + \Omega \right)^{-1}\Sigma \right)_{i,i} %= \Sigma_{i,i} - \Sigma_{i}^T \left( \Sigma + \Omega \right)^{-1} \Sigma_{i},
\end{align}
has to hold for all $i = 1, \dots, (p \times (n + 1))$, i.e. we have to choose $\omega^{2}$ such that the posterior marginal variances $\Gamma_{i,i}$ coincide with the marginal variances of $\G_{\psi}$.

Several problems arise: First of all, \Cref{eq:gamma_post} is not guaranteed to have a solution. For the $i$-th unit-vector $e_{i}\in\R^{p}$ we can reformulate \Cref{eq:gamma_post} to 
$$
\Sigma_{i,i} - \Gamma_{i,i} = e_{i}^T\Sigma^{T} \left( \Sigma + \Omega \right)^{-1}\Sigma e_{i} > 0
$$
and so we require $\Gamma_{i,i} < \Sigma_{i,i}$. While the law of total covariance asserts that
$$
\Sigma = \E \cov \left( X | Y \right) + \cov \left( \E \left( X | Y \right) \right),
$$
it does not guarantee $\Gamma \prec \Sigma$ (which would imply $\Gamma_{i,i} < \Sigma_{i,i}$), as $\cov(X|Y)$ may depend on $y$ in the non-Gaussian case.

Second, even if there is an analytical solution $\Omega$ to \Cref{eq:gamma_post}, in the \gls{cem} we replace $\Gamma_{i,i}$ by the observed marginal variances $\hat\Gamma_{i,i}$ obtained by importance sampling. The variation introduced by simulation can then lead to situations where $\hat\Gamma_{i,i} > \Sigma_{i,i}$. As an example take $X \sim \mathcal N(0, 1)$, and $Y = X + \eta$ for $\eta \sim \mathcal N(0, \rho^{2})$. Then the conditional variance of $X$ given $Y = y$ is $\Gamma = 1 - \frac{1}{1 + \rho^{2}} < 1$. Given $N$ i.i.d. samples $X^{1}, \dots X^{N} \sim \mathcal N(0,1)$, their empirical variance $\hat \Gamma = \frac{1}{N} \sum_{i = 1}^{N} (X^{i} - \bar X)^{2} $ follows a scaled $\chi_{N - 1}^{2}$ distribution, i.e. $ \frac{N\hat\Gamma}{\Gamma} \sim \chi^{2}_{N - 1}$. Notice that we use the non-Bessel corrected version of the empirical variance here, as it is the maximum-likelihood estimate. 

Then $$\P \left( \hat \Gamma > 1 \right) = \P \left( \frac{N \hat \Gamma}{\Gamma} > \frac{N}{\Gamma} \right) = 1 - F_{\chi^{2}_{N-1}} \left( N \left( 1 + \frac{1}{\rho^{2}} \right) \right)$$ is the probability that \Cref{eq:gamma_post} has no solution $\omega^{2} \in \R_{\geq 0}$. Here $F_{\chi^{2}_{N - 1}}$ is the cumulative distribution function of the $\chi^{2}_{N - 1}$ distribution. As $\rho^{2}$ goes to $\infty$, this probability approaches $1 - F_{\chi^{2}_{N - 1}}(N)$ which, for large $N$, is approximately $1 - F_{\chi^{2}_{N - 1}} (N - 1) \approx \frac{1}{2}$, as $\chi^{2}_{N-1} \approx \mathcal N\left(N - 1, 2 (N-1)\right)$ \citep[Section 18.5]{Johnson1994Continuous}.
%We illustrate this in \Cref{fig:ce_prob_failure}, displaying the probability of failure in this setting for various combinations of $N$ and $\omega^{2}$. In this figure, we see that with growing $N$ the threshold for $\omega^{2}$ leading to non-negligible failure probability becomes larger, as expected. 
Thus, even in the basic univariate Gaussian setting, for every $N$ there is an $\rho^{2}$ such that the \gls{cem} fails for \Cref{eq:glssm-proposal} with practically relevant probability. 

%\begin{figure}
%    \resizebox{\textwidth}{!}{%
%        \input{tikz/ce_prob_failure.tex}%
%    }
%    \caption{
%        We show the approximate probability that the estimated posterior variance $\hat \Gamma$ is bigger than the prior variance $1$ when varying the noise variance $\omega^{2}$. 
%    }
%    \label{fig:ce_prob_failure}
%\end{figure}

% all of these even worse if dimension grows
In higher-dimensional settings, e.g. when applying the \gls{cem} to \glspl{ssm}, we can expect this phenomenon to occur even more often. In the extreme case of independent marginals, i.e. when $\Sigma$ is a diagonal matrix, \Cref{eq:gamma_post} reduces to $(n + 1)p$ many decoupled equations, where $\hat \Gamma_{i,i}, i =1, \dots, (n + 1)p$ are independent. If all $q_{i} = \P \left(\Gamma_{i,i} > \Sigma_{i,i}\right)$ are identical to $q \in (0, 1)$, e.g. because $\Sigma$ and $\Omega$ are multiples of the identity, the number of failures follows a $\operatorname{Binom} \left( (n + 1)p, q \right)$ distribution, so that even small $q$ may lead to a non-negligible number of failures if the number of observations is high. 

% numerical scheme to solve eq.
Finally, in the multivariate setting, the system \eqref{eq:gamma_post} has no analytical solution. Instead, we have to resort to numerical methods to find a solution $\Omega$. Unfortunately, even evaluating the right-hand side of \eqref{eq:gamma_post} requires $\mathcal O(m^3)$ operations, as we have to invert $\Sigma + \Omega$. Additionally, we cannot hope to reuse a singular-value, LR, or eigenvalue-decomposition for further evaluations, as $\Sigma$ and $\Omega$ are not guaranteed to be jointly diagonalizable.
In the \gls{ssm} context we may use the Kalman-smoother to compute the marginal variances, but have to re-run the smoother for every evaluation, which is expensive. 

If we admit noise variance $\infty$ in the univariate setting, then $\Gamma > 1$ implies that the \gls{cem} chooses this as the estimate, i.e. $\G_{\hpce}$ is $\mathcal N(0, 1)$, which is equal to the prior. We can interpret this as having a missing observation, which, going back to the \gls{ssm} context, the Kalman-filter (\Cref{alg:kalman_filter}) can handle with only simple modifications, see e.g. \citep[Section 4.10]{Durbin2012Time}. However, if there are a lot of failures, the optimally chosen $\G_{\hpce}$ will be close to the prior distribution of states $X$, and importance sampling is unlikely to be effective. Hence, we turn to another approach that allows us to apply the \gls{cem} to \glspl{ssm}.

\subsection{The Markov-approach}
\label{subsec:markov-approach}
% second: Gaussian Markov process 
% discuss more flexible vs. fewer parameters
An alternative family of Gaussian proposals is given by directly modeling a Gaussian Markov process on the states $X_{:n}$. Again, this is sensible given the Markov structure of the target. This parametrization is more flexible than using the posterior of a \gls{glssm} with fixed prior as the proposal. This flexibility, however, comes at the cost of requiring a larger number of parameters. Here we propose with $\G_{\psi}$ where
\begin{align}
    \begin{split}
    \label{eq:markov-proposal}
    \G_{\psi} &= \mathcal L (U + v), \\
    v &\in \R^{(n + 1)m}, \\
    U_{0} &\sim \mathcal N(0, R_{0}R_{0}^T),\\
    U_{t} &= C_{t}U_{t - 1} + R_{t}\nu_{t}, \\
    C_{t} &\in \R^{m\times m},\\
    \nu_{t} &\sim \mathcal N(0,I_{m}), \\
    R_{t}&\in\R^{m \times m} \text{ lower triangular with positive diagonal}
    \end{split}
\end{align}
for $t = 1, \dots, n$, with $U_{0}$ and $\nu_{1}, \dots, \nu_{n}$ independent. The number of parameters in $$\psi= \left( v, C_{1}, \dots, C_{n}, R_{0}, \dots, R_{n} \right)$$ is $(n + 1)\cdot m$ for the mean $v$, $n \cdot m^{2}$ for the transition matrices $C_{t}$ and $(n + 1) \frac{m (m - 1)}{2}$ for the Cholesky roots of innovation covariances, totaling $\mathcal O(n\cdot m^{2})$ many parameters. 
While these are considerably more parameters than for the \gls{glssm}-approach for large state dimension $m$, we will see in the later part of this section that finding the optimal parameters for the \gls{cem} can be done analytically, depending only on the posterior first and second moments. 

This approach, which we term the \textbf{Markov-approach}, was originally proposed by \citep{Richard2007Efficient} for general unnormalized transition kernels as \gls{eis} proposals. However, because of its lower number of parameters, one should favor the \gls{glssm}-approach for \gls{eis} that operates on the signals, see \citep{Koopman2019Modified}.

To perform importance sampling with $\G_{\psi}$ in model \eqref{eq:markov-proposal} we not only need to simulate from $\G_{\psi}$ but also evaluate the unnormalized importance sampling weights $w(x) = \frac{p(x|y)}{g_{\psi}(x)}$. Simulation from $\G_{\psi}$ is achieved by a simple recursion. For the weights note that 
\begin{align}
\label{eq:weights_markov}
w(x) \propto \frac{p(y|x)p(x)}{g_{\psi}(x)} = \prod_{t = 0}^n \frac{p(y_{t}|x_{t})p(x_{t}|x_{t - 1})}{g_{\psi}(x_{t}|x_{t - 1})},
\end{align}
where $p(x_{0}|x_{-1}) = p(x_{0})$ and $g_{\psi}(x_{0}|x_{-1}) = g_{\psi}(x_{0})$.

The Markov structure of model \eqref{eq:markov-proposal} implies that the precision matrix of $\G_{\psi}$ is sparse, i.e. it has a block-tridiagonal form. This is a well-known property of the precision matrix of Gaussian random vectors, as the following two classical lemmas show. We show their proofs here for completeness. For a general treatment, we refer the reader to \citep[Chapters 3 and 5]{Lauritzen1996Graphical}.

\begin{lemma}
    \label{lem:gaussian_precision}
    Let $(X,Y)$ be jointly Gaussian with distribution $\mathcal N \left( \mu, \Sigma \right)$ where 
    $$
    \mu = \left(\mu_{X}, \mu_{Y}\right)
    $$
    and 
    $$
    \Sigma = \begin{pmatrix}
        \Sigma_{XX} & \Sigma_{XY} \\
        \Sigma_{YX} & \Sigma_{YY}
    \end{pmatrix},
    $$
    are partitioned according to the dimensions of $X$ and $Y$ and $\Sigma$ is non-singular.
    If $$P = \Sigma^{-1} = \begin{pmatrix} \Sigma_{XX} &  \Sigma_{XY} \\ \Sigma_{XY} & \Sigma_{YY} \end{pmatrix}^{-1}=  \begin{pmatrix} P_{XX} & P_{XY} \\ P_{YX} & P_{YY} \end{pmatrix}$$ 
    is the precision matrix of $(X,Y)$, partitioned as is $\Sigma$, then $\cov(X|Y) = P_{XX}^{-1}$.
\end{lemma}
\begin{proof}
    Without loss of generality, assume that both $X$ and $Y$ are centered. 
    The conditional density $p(x|y)$ is proportional (in $x$) to the joint density $p(x,y)$ with 
    $$\log p(x,y) = -\frac 1 2 \begin{pmatrix} x & y\end{pmatrix}  P \begin{pmatrix} x\\y\end{pmatrix} + C = -\frac 12 \left(x^TP_{XX}x + 2x^TP_{XY}y\right) + C',$$
    for constants $C, C'$ that do not depend on $x$. 
    As the conditional distribution of $X$ given $Y=y$ is Gaussian (by \Cref{lem:gaussian_conditional}), its covariance matrix is $P_{XX}^{-1}$.
\end{proof}

\begin{lemma}
    \label{lem:gaussian_precision_zeros}
    Let $(X,Y,Z) \sim \mathcal N \left( \mu, \Sigma \right)$ be jointly Gaussian with non-singular $\Sigma$. Then $X \perp Y | Z$ if, and only if, the sub-matrix of the precision matrix $P = \Sigma^{-1}$ whose rows correspond to the entries of $X$ and columns correspond to the entries of $Y$ is the $0$ matrix.
\end{lemma}
\begin{proof}
    Partition the conditional covariance matrix into
    $$
    \cov \left((X,Y) | Z\right) = \begin{pmatrix}
        \Sigma_{XX|Z} & \Sigma_{XY|Z} \\
        \Sigma_{YX|Z} & \Sigma_{YY|Z}
    \end{pmatrix}.
    $$

    As all distributions involved are Gaussian, $X \perp Y | Z$ is equivalent to $\cov \left( (X,Y) |Z \right)$ being a block-diagonal matrix with blocks $\Sigma_{XX|Z}$ and $\Sigma_{YY|Z}$, which is equivalent to its inverse being a block-diagonal matrix with blocks $\Sigma_{XX|Z}^{-1}$ and $\Sigma_{YY|Z}^{-1}$. Its inverse is, by \Cref{lem:gaussian_precision}, the sub-matrix of $P$ whose rows and columns correspond to $X$ and $Y$. 
\end{proof}

Applying \Cref{lem:gaussian_precision_zeros} to model \eqref{eq:markov-proposal}, we see that its precision matrix $P$ is sparse, i.e. it is a block-tri-diagonal matrix, as $U_{t} \perp U_{s} | U_{-t,-s}$ for $\lvert t - s\rvert > 1$ and $U_{-t,-s}$ being the vector of all $U_{0}, \dots, U_{n}$ except for $U_{t}, U_{s}$. Thus, the only entries of $P$ that are potentially non-zero are those whose row and column correspond to $(U_{t}, U_{t})$ for $t = 0, \dots, n$, $(U_{t}, U_{t - 1})$ and $(U_{t - 1}, U_{t})$ for $t=1, \dots, n$. 
Therefore, $P$ has the following block-tridiagonal structure:
\begin{align}
    \label{eq:P_structure}
P = \begin{pmatrix}
    P_{0,0} & P_{0, 1} & 0 & \cdots & \cdots & 0 & 0 \\
    P_{1, 0} & P_{1,1} & P_{1,2} & 0 & \cdots & 0 & 0\\
    0 & P_{2,1} & P_{2,2} & P_{2,3} & \cdots & 0 & 0 \\
    \vdots & \ddots & \ddots  & \ddots & \ddots & 0 & 0 \\
    0 & 0& 0& \cdots& P_{n - 1, n - 2}& P_{n - 1, n- 1} & P_{n - 1, n} \\
    0 & 0 & 0 & \cdots & 0 & P_{n, n - 1} & P_{n,n} 
\end{pmatrix}.
\end{align}

% show Cholesky root of precision matrix is also sparse -> Katzfuss (or refs therein), Lauritzen, Bernstein?, same as Schäfer paper?
The sparsity of $P$ implies that $P = LL^{T}$ has a sparse Cholesky root $L$, which will make computations efficient. 
To see that $L$ is sparse, we apply the following Theorem, slightly adapted to our notation, from the theory of Gaussian-Markov-Random-fields (GMRF), i.e. Gaussian models whose dependency structure is given by a graph, with edges between nodes indicating non-zero entries in the precision matrix.
\begin{theorem}[{\citep[Theorem 12.14]{Gelfand2010Discrete}}] 
    \label{thm:gelfand_gmrf}
    Let $X = (X_{0}, \dots, X_{n}) \in \R^{(n + 1)m}$ be a GMRF wrt to the labeled graph $G$, with mean $\mu$ and symmetric positive-definite precision matrix $P$. Let $L$ be the Cholesky factor of $P$ and define for $0 \leq t < s \leq n$ the future of $t$ except $s$ as 
    $$
        F(t,s) = \{t + 1, \dots, s - 1, s+ 1, n\}.
    $$
    Then
    $$
        X_{t} \perp X_{s} | X_{F(t,s)} \Leftrightarrow L_{t,s} = 0.
    $$
\end{theorem}
In the preceding theorem $X_{F(t,s)}$ is the vector of all $X_{u}$ for $u\in F(t,s)$ and $L_{t,s} \in \R^{m\times m}$ is the sub-matrix of $L$  whose rows correspond to $X_{t}$ and columns to $X_{s}$. 
From \Cref{thm:gelfand_gmrf} we immediately obtain the following:

\begin{corollary}[sparsity of $L$ in model \eqref{eq:markov-proposal}]
    \label{cor:sparsity_L}
    Let $U \sim \G_{\psi}$ as in \Cref{eq:markov-proposal}, $P \succ 0$ be the precision matrix of $\overset{\leftarrow}{U} = \left( U_{n}, \dots, U_{0} \right)$ and $L$ be the Cholesky root of $P$. 
    Then $L$ is a lower-block-diagonal matrix, with at most $nm^{2} + (n + 1)m\frac{m - 1}{2}$ non-zero entries:
    
    \begin{align}
        \label{eq:L_structure}
    L = \begin{pmatrix}
        L_{n,n} & 0 & \cdots & \cdots & \cdots & 0 & 0 \\
        L_{n-1, n} & L_{n-1,n-1} & 0 & \cdots & \cdots & 0 & 0\\
        0 & L_{n-2,n-1} & L_{n-2,n-2} & 0 & \cdots & 0 & 0 \\
        \vdots & \ddots & \ddots  & \ddots & \ddots & 0 & 0 \\
        0 & 0& 0& \cdots& L_{1, 2}& L_{1, 1} & 0 \\
        0 & 0 & 0 & \cdots & 0 & L_{0, 1} & L_{0,0} 
    \end{pmatrix},
    \end{align}
    where $L_{t,t} \in \R^{m\times m}, t = 0, \dots, n$ are lower triangular matrices with positive diagonal entries and $L_{t-1,t}\in\R^{m \times m}, t = 1, \dots, n$ are square matrices.
\end{corollary}

From $L$ in \Cref{cor:sparsity_L} we obtain an iterative method of sampling from $\G_{\psi}$: If $v + U \sim \G_{\psi}$, then, as $\cov U = \left( L L^{T} \right)^{-1} = L^{-T}L^{-1}$, it holds that $L^{T}U \sim \mathcal N(0, I)$ follows a standard normal distribution. Thus to simulate from $\G_{\psi}$ we may solve
$$
L^{T}U = \overset{\leftarrow} Z
$$
where $\overset{\leftarrow}Z = \left( Z_{n}, \dots, Z_{0} \right) \sim \mathcal N(0, I)$. Using the structure available in $L$, we see that this is equivalent to first solving
$$
L_{0,0}^T U_{0} = Z_{0}
$$
and then recursively solving for $t = 1, \dots, n$
$$
L_{t,t}^T U_{t} + L_{t-1, }^{T} U_{t-1} = Z_{t - 1}.
$$
Rearranging terms, provided $L_{t,t}$ is non-singular, we end up with the Markov-process
\begin{align}
\label{eq:rev_time_u}
    U_{t} = L^{-T}_{t,t} L_{t-1, t }^T U_{t - 1} +L^{-T}_{t,t} Z_{t},
\end{align}
where $Z_{t}$ is, by construction, independent of $U_{t - 1}$. Thus for model \eqref{eq:markov-proposal}, we obtain
\begin{align}
    \label{eq:parameters_markov_from_L}
    \begin{split}
    R_{t} &= L^{-T}_{t,t} \text{ for } t = 0, \dots, n,\\
    C_{t} &= L^{-T}_{t,t} L_{t-1, t }^T \text{ for } t =1, \dots, n.
    \end{split}
\end{align}
Here we see why we chose to use $\overset{\leftarrow}U$ in \Cref{cor:sparsity_L}: had we applied \Cref{thm:gelfand_gmrf} to $U$ directly we would have ended up with a Markov process in reverse time.

% show how to estimate Cholesky root analytically -> Schäfer
We now turn our attention to applying the \gls{cem} to model \eqref{eq:markov-proposal}. Following a similar argument as in the discussion surrounding \Cref{eq:cem_reparametrization}, we see that we may match the mean $v$ to that of $\P$ and it suffices to choose $P$, the precision matrix of $U$, such that it minimizes
\begin{align}
\label{eq:markov_ce_target}
\frac{1}{2} \operatorname{trace} \left( P \hat\Gamma \right) - \frac{1}{2}\log\det P
\end{align}
where $\hat\Gamma$ is the importance sampling estimate of the joint covariance matrix of all states $X$. This is equivalent to minimizing 
$$
\Dkl{\mathcal N(0,\hat\Gamma)}{\mathcal N(0, P^{-1})}.
$$
Here $P$ is restricted to precision matrices that may arise in model \eqref{eq:markov-proposal}, i.e., by \Cref{cor:sparsity_L}, $P = LL^{T}$ where $L$ possess structure as in \eqref{eq:L_structure}. 
At first glance, this problem seems more involved than solving \Cref{eq:gamma_post}: after all, the optimal $P$ depends on the whole covariance matrix $\hat\Gamma$. 
However, it turns out that the sparsity we enforce in $L$ allows us to compute analytically the optimal $\hat L$  that minimizes 
\Cref{eq:markov_ce_target}. Additionally, due to the Markov-structure of our proposal, $\hat L$ depends only on the block-tri-diagonal component of $\hat \Gamma$, i.e. only the covariances $\cov(X_{t}, X_{t-1})$ and $\cov (X_{0})$ are required. This is sensible - all information about the Markov transitions is encoded in these covariances if we assume that $X$ is a Gaussian Markov process.

To make this argument rigorous, let us apply the following result (stated in our notation).
\begin{theorem}[{\citep[Theorem 2.1]{Schafer2021Sparse}}]
    \label{thm:schafer_cholesky_analytical}
    Let $\Gamma$ be a positive-definite matrix of size $n\times n$. Given a lower-triangular sparsity set $S \subset \{1, \dots, n\}^{2}$, i.e. $i \geq j$ for all $(i,j) \in S$, let 
    $$
    \hat L = \argmin_{L \in \mathcal S} \Dkl{\mathcal N (0, \Gamma)}{\mathcal N \left( 0, (LL^{T})^{-1} \right)}
    $$
    be the Cholesky root of the closest Gaussian (wrt. the \gls{kld}) with sparsity $\mathcal S = \{A \in \R^{n \times n}: A_{i,j} \neq 0 \Rightarrow (i,j) \in S\}$. 

    Then the following holds:
    The nonzero entries of the $i$-th column of $\hat L$ are given by 
    
    \begin{align}
    \label{eq:schafer_gamma}
        L_{s_{i}, i} &= \frac{\Gamma_{s_{i}, s_{i}}^{-1} e_{1}}{\sqrt{e_{1}^{T}\Gamma_{s_{i}, s_{i}}^{-1} e_{1}}},
    \end{align}
    where $s_{i} = \{j : (i,j) \in \mathcal S\}$, $\Gamma_{s_{i}, s_{i}}$ is the restriction of $\Gamma$ to the set of indices $s_{i}$ and $e_{1} \in\R^{\lvert s_{i}\rvert}$ is the first unit vector.
\end{theorem}
Exploiting the Markov structure of our proposals, we immediately obtain the following:
\begin{corollary}
    \label{cor:markov_sparsity}
    Let $\mathcal S$ be the sparsity set of a Gaussian Markov process of the form \Cref{eq:markov-proposal}, i.e. 
    $$
        \mathcal S = \left\{ ((t,i),(s,j)) \in \left(\{0,\dots, n\}\times \{1,\dots, m\}\right)^{2} \,|\, (t = s \text{ and } i \geq  j) \text{ or } t = s + 1 \right\},
    $$
    see also \Cref{eq:L_structure},
    and let $\Gamma$ be a positive definite matrix of size $((n+1)m)\times(n + 1)m$ with blocks
    $$
        \Gamma_{s,t} = (\Gamma_{(s,i),(t,j)})_{i,j =1,\dots,m}.
    $$
    Then $\hat L$ in \Cref{thm:schafer_cholesky_analytical} depends only on the block-diagonal entries $\Gamma_{t, t}$, $ t = 0, \dots, n$ and block off-diagonal entries $\Gamma_{t, t + 1}$, $t = 0, \dots, n$.

    If, in particular, $\Gamma$ is the covariance matrix of Gaussian Markov process, $\hat L = \operatorname{chol} (\Gamma ^{-1})$.
\end{corollary}

We have thus shown the following: The covariance matrix of the KL-optimal Gaussian Markov process for the positive definite covariance matrix $\Gamma$ with $\mathcal O(n^{2}m^{2})$ entries only depends on $\mathcal O(n m^{2})$ many entries, the marginal covariances. In particular, if we can find a centered Gaussian Markov process $(X_{t})_{t = 0, \dots, n}$ whose marginal covariances fulfill 
\begin{align*}
    \cov (X_{t}) = \Gamma_{t}  && t = 0, \dots, n\\
    \cov (X_{t}, X_{t + 1}) = \Gamma_{t, t + 1} && t = 0, \dots, n,
\end{align*}
then its law $\mathcal L (X)$ is the one we seek. The following proposition puts all the pieces together.

\begin{theorem}[the \acrshort{cem} for the Markov proposal]
    \label{prop:cem-for-markov-proposal}
    Let $\P$ be a probability measure on $\R^{(n+1)\times m}$ with mean $\mu$ and positive definite covariance matrix $\Gamma$, partitioned into blocks 
    $$
        \Gamma_{s,t} = (\Gamma_{(s,i),(t,j)})_{i,j =1,\dots,m}.
    $$
    Let
    $$
        \begin{pmatrix}
            J_{t,t} & 0 \\
            J_{t + 1, t} & Z_{t+1, t+1}
        \end{pmatrix} = \operatorname{chol} \begin{pmatrix}
            \Gamma_{t,t} & \Gamma_{t, t + 1} \\
            \Gamma_{t + 1, t} & \Gamma_{t + 1, t + 1}
        \end{pmatrix}.
    $$

    Then the optimal cross-entropy parameter
    $$
        \pce = \argmin_{\psi = (v, C_{1}, \dots, C_{n}, R_{0}, \dots, R_{n})} \Dkl{\P}{\G_{\psi}}
    $$
    for the Markov proposal $\G_{\psi}$ from model \eqref{eq:markov-proposal} exists and is unique. The components of $\pce$ are given by 
    \begin{align*}
        v &= \mu\\
        R_{0} &= \operatorname{chol} (\Gamma_{0,0}) 
    \end{align*}
    and for $t = 1, \dots, n$
    \begin{align*}
        C_{t} &= J_{t + 1, t} J_{t, t}^{-1}\\
        R_{t} &= Z_{t + 1, t + 1}
    \end{align*}

    Thus, given $\nu$ and $\Gamma$, $\pce$ can be obtained in $\mathcal O(nm^{3})$ many operations.
\end{theorem}

\begin{proof}
    It only remains to show the uniqueness and existence of $\pce$, as well as its representation.
    The discussion surrounding \Cref{eq:markov_ce_target} shows that $v = \mu$ has to hold, so we may assume that $\P$ and the proposal are both centered.
    As $\Gamma$ is positive definite, so are all of its sub-matrices, and we may apply \Cref{cor:markov_sparsity}. Therefore, if we can show that there is a unique Gaussian Markovian probability measure whose covariance matrix matches $\Gamma$ as in that corollary we are done. 

    Let $(U_{t})_{t = 0, \dots, n} \sim \G_{\pce}$. Then 
    $$
        \cov (U_{0}) = R_{0}R_{0}^T = \Gamma_{0,0},
    $$
    and from the Cholesky decomposition we obtain for $t = 0, \dots, n - 1$
    $$
        \begin{pmatrix}
            J_{t,t}J_{t,t}^T & J_{t,t}J_{t + 1, t}^T\\
            J_{t +1, t}J_{t,t}^T & J_{t + 1, t}J_{t + 1, t}^{T} + Z_{t + 1, t + 1} Z_{t + 1, t + 1}^T.
        \end{pmatrix} = \begin{pmatrix}
            \Gamma_{t,t} & \Gamma_{t, t + 1} \\
            \Gamma_{t + 1, t} & \Gamma_{t + 1, t + 1}
        \end{pmatrix}.
    $$
    As $Z_{t + 1, t + 1}$ is a lower triangular matrix with positive diagonal and
    $$
        \Gamma_{t + 1, t + 1} - \Gamma_{t+1, t}\Gamma_{t}^{-1}\Gamma_{t, t +1} = Z_{t + 1, t + 1} Z_{t + 1, t + 1}^T,
    $$
    it is the Cholesky root of the Schur complement $\Gamma_{t + 1, t + 1} - \Gamma_{t+1, t}\Gamma_{t}^{-1}\Gamma_{t, t +1}$, which, recalling \Cref{lem:gaussian_conditional}, we can think of as a conditional covariance matrix. 
    Therefore, using induction over $t = 0, \dots, n - 1$, we obtain
    \begin{align*}
        \cov \left( U_{t + 1} \right) &= C_{t + 1}\cov \left( U_{t} \right) C^T_{t + 1} + R_{t + 1}R_{t + 1}^T \\
            &= J_{t + 1, t}J_{t,t}^{-1}\Gamma_{t,t}J_{t,t}^{-T}J_{t+1,t}^T + \Gamma_{t + 1, t + 1} - \Gamma_{t+1, t}\Gamma_{t}^{-1}\Gamma_{t, t +1}\\
            &= \Gamma_{t + 1, t + 1}
    \end{align*}
    and
    \begin{align*}
        \cov \left( U_{t + 1}, U_{t} \right) &= C_{t}\cov \left( U_{t} \right) = J_{t + 1, t}J_{t,t}^{-1} J_{t,t} J_{t, t}^T = \Gamma_{t + 1, t}.
    \end{align*}
    This shows the existence. For uniqueness, note that model \eqref{eq:markov-proposal} enforces that $R_{t}$ is a lower triangular matrix with positive diagonals. As $R_{t + 1}R_{t + 1}^T$ is the conditional covariance of $U_{t + 1}$ given $U_{t}$ which is, by \Cref{lem:gaussian_conditional} given by $\Gamma_{t + 1, t + 1} - \Gamma_{t + 1, t}\Gamma_{t,t}^{-1}\Gamma_{t + 1, t}$. Thus the $R$ matrices are unique as well. As $\cov (U_{t + 1}, U_{t}) = C_{t}\cov (U_{t})$, we can show that, additionally, also $C_{t}$ is unique.
\end{proof}

When using the \acrshort{cem}, we do not have access to the mean and covariances necessary to apply this proposition. Instead, we may apply the \gls{cem} to estimate $\psi$ in model \eqref{eq:markov-proposal} by replacing these unknown moments with their importance sampling estimates. Given importance samples $U^{1}, \dots, U^{N}$ for $\mathcal L (X| Y = y)$ and associated auto-normalized weights $W^{1}, \dots, W^{N}$, we estimate $v$ by 
\begin{align}
\label{eq:hat_v}
\hat v = \sum_{i = 1}^N W^{i}X^{i}
\end{align}
and the empirical covariance matrices
\begin{align}
    \label{eq:empirical_covs}
    \begin{split}
    \widehat{\cov} \left( X_{t}, X_{t - 1} \right) &= \sum_{i = 1}^N W^{i}(X_{t:t-1}^{i} - \hat v_{t-1:t}) (X_{t:t-1}^{i} - \hat v_{t-1:t})^{T}\\
\widehat{\cov} \left( X_{0}\right) &= \sum_{i = 1}^N W^{i}(X_{0}^{i} - \hat v_{0}) (X_{0}^{i} - \hat v_{0})^{T}
    \end{split}
\end{align}
These steps are summarized in \Cref{alg:cem-markov-proposal}. 

\begin{algorithm}
    \caption{The \gls{cem} for the Markov proposal \eqref{eq:markov-proposal}}
    \label{alg:cem-markov-proposal}
    \begin{algorithmic}[1]
        \Require \gls{egssm} (\Cref{def:egssm}), observations $Y$, initial estimate $\hat\psi^0 = \left( v^{0}, C^{0}, R^{0}\right)$, sample size $N$
        \State set $l = 0$
        \Repeat 
            \State\label{step:cem-simulate} sample $U^{1} + v^{l}, \dots, U^{N} + v^{l} \iid \G_{\hat\psi^{l}}$ with fixed seed \Comment{\Cref{eq:markov-proposal}} 
            \State\label{step:cem-weights} determine auto-normalized weights $W^{1}, \dots, W^{N}$ \Comment{\Cref{eq:weights_markov}}
            \State\label{step:cem-est_v} estimate $\hat v^{l + 1}$ \Comment{\Cref{eq:hat_v}} 
            \State\label{step:cem-est_cov} estimate $\widehat{\cov} (U_{t}, U_{t-1}), t=1, \dots, n$, and $\widehat\cov (U_{0})$ \Comment{\Cref{eq:empirical_covs}}
            %\State\label{step:cem-L} determine $\hat L^{l + 1}$ \Comment{\Cref{thm:schafer_cholesky_analytical}}
            \State\label{step:cem-C_R} determine $C^{l + 1}$ and $R^{l + 1}$ \Comment{\Cref{prop:cem-for-markov-proposal}}
            \State\label{step:cem-est_phi} set $\hat\psi^{l + 1} = \left( \hat v^{l + 1}, C^{l + 1}, R^{l + 1}\right)$ 
            \State set $l = l + 1$
        \Until{$\hat\psi^{l}$ converged}
        \State \textbf{return} $\hpce = \hat \psi^{l}$
    \end{algorithmic}
\end{algorithm}

% initial value: could also use LA/EIS
To run \Cref{alg:cem-markov-proposal} we require an initial value for $\hat\psi^{0}$. If a suitable $\hat\psi^{0}$ is not available, we can obtain one from the \gls{la} by sampling $X^{1}, \dots, X^{N}$ from the \gls{la} and performing steps \ref{step:cem-est_v} to \ref{step:cem-est_phi} from the loop.
Alternatively, we could also directly base our initial value on the smoothing distribution of the \gls{glssm} that the \gls{la} is based on. The Kalman smoother (\Cref{alg:kalman_smoother}) provides us with the analytically available covariances $\cov \left( X_{t}, X_{t - 1} | Z = z \right)$ and the marginal covariance $\cov \left( X_{0} | Z = z \right)$ can be computed as well. 

% convergence: number of iterations, rel. change in psi
The convergence criteria in \Cref{alg:cem-markov-proposal} is similar to that used for \gls{eis}: we stop until the absolute or entry-wise relative difference of $\hat\psi^{l}$ and $\hat \psi^{l + 1}$ is smaller than a predetermined threshold, or a fixed number of iterations has passed. For the matrices involved, we use the Frobenius norm and the Euclidean distance for the mean $v$. 

In \Cref{step:cem-simulate} we use the standard praxis of \glspl{crn} to ensure numerical convergence. This is similar to \gls{eis} and the maximum likelihood estimates from \Cref{sec:maximum_likelihood_estimation}.

% runtime
We give an overview of the time and space complexities of each line in \Cref{alg:cem-markov-proposal} in \Cref{tab:cem-time-space-complexity}
The total time complexity of a single iteration of \Cref{alg:cem-markov-proposal} is $\mathcal O \left( Nnm^{2} + nm^{3}\right)$ and its space complexity is $\mathcal O \left( Nnm + nm^{2}\right)$. Let us elaborate on the complexities of each step:
\begin{enumerate}
    \item[Step \ref{step:cem-simulate}] Generate $N$ i.i.d. samples from model \eqref{eq:markov-proposal}, where each simulation requires $\mathcal O(n)$ matrix-vector multiplications of dimension $m$. 
    \item[Step \ref{step:cem-weights}] To evaluate the weights, \Cref{eq:weights_markov}, we have to evaluate for every sample $\mathcal O(n)$-times the density of a $m$-variate Gaussian distribution, while this usually has time-complexity $\mathcal O(m^{3})$, we have access to the Cholesky root $R_{t}$, so this step has only time-complexity $\mathcal O(m^{2})$. In \Cref{eq:weights_markov} we also need to compute $p(y_{t}|x_{t})$ and $p(x_{t}|x_{t - 1})$. Assuming conditional independence of observations, $p(y_{t}|x_{t}) = \prod_{i = 1}^{m}p(y_{t}^i|(B_{t}x_{t})^{i})$, evaluating the first term requires only $\mathcal O(m^{2})$ operations. For the second term, if we allow pre-computation of the Cholesky roots of innovations off-line (in $\mathcal O(m^{3})$ time), this step reduces to $\mathcal O(m^{2})$ as well.  
    \item[Step \ref{step:cem-est_v}] Calculating the weighted mean $\hat v \in\R^{(n+1)m}$, \Cref{eq:hat_v}, requires $\mathcal O(Nnm)$ operations.
    \item[Step \ref{step:cem-est_cov}] Calculating the weighted covariance matrices, \Cref{eq:empirical_covs}, requires $(n+1)$ times multiplying $N$ many $m\times 1$ with $1 \times m$ vectors. 
    %\item[\Cref{step:cem-L}] To determine $L_{t,t}$ and $L_{t - 1, t}$ we have to solve $m$ times the linear systems of equations given in \Cref{eq:schafer_gamma}, where the dimension of the system is $2m,\dots, m + 1$. This requires $\mathcal O(m^{4})$ many operations, and we have to perform it for every one of the $n + 1$ time points. The result is $L$ with sparsity structure given by \Cref{eq:L_structure}, which has $\mathcal O (n m^{2})$ many non-zero entries.
    \item[Step \ref{step:cem-C_R}] For each of the $\mathcal O(n)$ many $C_{t}$ and $R_{t}$ we have to calculate Cholesky decompositions and invert triangular matrices of dimension $m$. 
\end{enumerate}

\begin{table}
    \centering
    \begin{tabular}{lcc}
        \toprule
        step & time complexity & space complexity \\
        \midrule 
        simulation (Step \ref{step:cem-simulate}) & $\mathcal O \left( Nnm^{2}\right)$ & $\mathcal O \left( Nnm \right)$\\
        weights (Step \ref{step:cem-weights}) & $\mathcal O (Nnm^{2})$ & $\mathcal O \left( N \right)$\\
        estimating $v$ (Step \ref{step:cem-est_v}) & $\mathcal O (Nnm)$ & $\mathcal O \left( nm \right)$\\
        estimating covariances (Step \ref{step:cem-est_cov}) & $\mathcal O (Nnm^{2})$ & $\mathcal O \left( nm \right)$\\
        determining $C$ and $R$ (Step \ref{step:cem-C_R}) & $\mathcal O (nm^{3})$ & $\mathcal O (nm^{2})$\\
        \bottomrule
    \end{tabular}
    \caption{Time and space complexities of individual steps in \Cref{alg:cem-markov-proposal}.}
    \label{tab:cem-time-space-complexity}
\end{table}

An efficient implementation of \Cref{alg:cem-markov-proposal} can improve on the practically relevant computational time. There is no need to calculate the $C_{t}$ matrices explicitly, instead we can calculate $C_{t}U_{t - 1} = J_{t + 1, t}J_{t,t}^{-1}U_{t - 1}$ efficiently by back-substitution, as $J_{t,t}$ is a lower triangular matrix. 

% if space complexity is problematic, can compute weights first with fixed seed and then iterate forwards
The main bottleneck for space lies in the $\mathcal O(Nnm)$ simulation part, and we may reduce this by simulating twice from model \eqref{eq:markov-proposal} using \glspl{crn}, and only storing the samples for a single time step (dimension $\mathcal O (Nm)$) in each simulation. In the first pass, we only calculate the weights, and in the second pass, we calculate $\hat v$ and the required covariance matrices. For this, we only need the $2N$ samples of dimension $m$ from time $t$ and $t + 1$, i.e. $\mathcal O(Nm)$ space. This reduces the total space complexity to $\mathcal O(Nm + nm^{2})$. 

We demonstrate these improvements in \Cref{alg:cem-markov-proposal-fast}. Additionally, we calculate the weights on the log scale for numerical stability.

\begin{algorithm}
    \begin{algorithmic}[1]
        \Require \gls{egssm} (\Cref{def:egssm}), observations $Y$, initial estimate $\hat\psi^0 = \left( v^{0}, R^{0}, J^{0}\right)$, sample size $N$
        \State set $l = 0$
        \Repeat 
            \State simulate $\nu^{1}_0, \dots, \nu^{N}_0 \iid \mathcal N(0, I)$ 
            \State set $U^{i}_0 = R^{l}_{0}\nu_{0}^{i}$ 
            \State set $X_{0}^{i} = v^{l}_{0} + U^{i}_0$ 
            \State set $\log w^{i} = \log p(y_{0}|X_{0}^{i}) + \log p(X_{0}^i) + \frac{1}{2} \lVert \nu^{i}_0\rVert^{2}$ \Comment{$\log g(X_{0}^{i}) = -\frac{1}{2}\lVert \nu^{i}_{0}\rVert^{2}_2 + C$ }
            \State store current RNG state
            \For {$t \gets 1, \dots, n$}
                \State simulate $\nu^{1}_t, \dots, \nu^{N}_t \iid \mathcal N(0, I)$ 
                \State set $U^{i}_t = (J_{t + 1, t})^{T}(J_{t,t})^{-1}U^{i}_{t - 1} + R^{l}_{t}\nu_{t}^{i}$ \Comment{backsubstitution}
                \State set $X_{t}^{i} = v^{l}_{t} + U^{i}_t$ 
                \State set $\log w^{i} = \log w^{i} + \log p(y_{t}|X_{t}^{i}) + \log p(X_{t}^i|X_{t - 1}^i) + \frac{1}{2} \lVert \nu^{i}_t\rVert^{2}$ 
            \EndFor
            \State set $\log w^{i} = \log w^{i} - \max_{i = 1,\dots, N} \log w^{i}$ \Comment{ensure $\log w^{i} \leq 0$}
            \State set $w^{i} = \exp (\log w^{i})$
            \State set $W^{i} = \frac{w^{i}}{\sum_{i = 1}^N w^{i}}$ \Comment{auto-normalized weights}
            \State set $v^{l + 1}_0 = \sum_{i = 1}^{N}W^{i}X_{0}^i$
            \State restore RNG state
            \For {$t \gets 1, \dots, n$}
                \State simulate $\nu^{1}_t, \dots, \nu^{N}_t \iid \mathcal N(0, I)$ 
                \State set $U^{i}_t = (J_{t + 1, t})^{T}(J_{t,t})^{-1}U^{i}_{t - 1} + R^{l}_{t}\nu_{t}^{i}$ \Comment{backsubstitution}
                \State set $X_{t}^{i} = v^{l}_{t} + U^{i}_t$ 
                \State calculate $\hat v^{l + 1}_{t}$ \Comment{\Cref{eq:hat_v}}
                \State calculate covariances \Comment{\Cref{eq:hat_v}}
            \EndFor
            \State set $\hat\psi^{l + 1} = \left( \hat v^{l + 1}, \hat R^{l + 1}, \hat J^{l + 1}\right)$ 
            \State set $l = l + 1$
        \Until{$\hat\psi^{l}$ converged}
        \State \textbf{return} $\hpce = \hat \psi^{l}$
        
    \end{algorithmic}
    \caption{Time and space improved version of \Cref{alg:cem-markov-proposal}. Instructions involving the free index $i$ are to be performed for all $i = 1, \dots, N$ samples. For simplicity of notation we let $R^{l} = (R^{l}_0, \dots, R^{l}_n)$ and $J^l = (J^{l}_{0,0}, J^{l}_{1, 0}, \dots, J^{l}_{n - 1, n -1}, J^{l}_{n, n - 1} )$ for $l \in \N_{0}$.}
    \label{alg:cem-markov-proposal-fast}
\end{algorithm}

The advantage of \Cref{alg:cem-markov-proposal,alg:cem-markov-proposal-fast} over applying the \gls{cem} to the \gls{glssm} model \eqref{eq:glssm-proposal} are multiple: First of all, as long as the involved covariance matrices are positive definite, the two algorithms produce valid proposals, i.e. they do not have the degeneracy problem we observed in \Cref{subsec:glssm-approach}. When matrices are only positive-semi definite, replacing inverses with generalized inverses still yields a valid model.
% no numerical issues w/ determining optimal parameters
Additionally, determining the optimal parameters $(v, C,R)$ or $(v,J,R)$ is numerically stable, involving only inversion of small matrices. Compare this with solving \Cref{eq:gamma_post}, where we need to employ a numerical scheme to solve for the diagonal entries of $\Omega$.

% sampling faster 
After having determined $\hpce$ for model \eqref{eq:markov-proposal}, generating $N$ samples requires only $\mathcal O(Nnm^{2})$ operations, whereas sampling from model \eqref{eq:glssm-proposal} requires $\mathcal O(nm^{3} + Nnm^{2})$ operations, as we need an initial run of the Kalman filter. Unless $N < m$, this difference is negligible, and the case where $N < m$ is not really of interest, as we would expect importance sampling to require a much larger number of samples, i.e. $N \gg m$. 

% problems: large state dimension: both $L$ and number of parameters
However, the two algorithms presented in this section also come with some drawbacks, especially if the dimension $m$ of states is large. This affects the algorithms in multiple ways: when $m$ is large, computation of the Cholesky decomposition in \Cref{prop:cem-for-markov-proposal} becomes more time-intensive. Additionally, the dimension of the parameter $\psi$ increases quadratically in $m$, so we expect convergence to be slower, requiring a larger sample size $N$ to find the optimal $\hpce$. For an empirical study in this direction, see \Cref{sec:simulation_studies}.



%\subsection{Asymptotics of the \acrshort{cem} and \acrshort{eis} in \acrshortpl{pgssm}}
%\label{subsec:asymptotics_cem_eis}
%
%We now have two methods at hand for generating optimal importance sampling proposals in \acrshortpl{pgssm} with a linear signal \todo{check again the terminology}: we may either use the \acrshort{glssm}-approach combined with \acrshort{eis} or the Markov-approach, together with the \acrshort{cem}. Naturally, we may ask which of these two approaches is preferable over the other. In this section we focus on giving an answer in terms of the \acrshortpl{clt} derived in \Cref{sec:importance_sampling} and postpose further discussion of this multi-faceted question to \Cref{sec:simulation_studies}. 
%
%Several challenges arise: First the \acrshortpl{clt} \Cref{thm:eis-clt,thm:cem-clt} assume that the family of proposals $\G_{\psi}$ form a $k$-dimensional natural exponential family with convex parameter space $\Psi$, which is the case for the \acrshort{cem}, but not for \acrshort{eis}. Second, the parameterization of a \acrshort{glssm}-proposal is different to that of a Markov-proposal, so we have to reparameterize to make comparisons reasonable. 
%
%For the first challenge, note that for \acrshort{eis}, the natural parametrization \eqref{eq:eis_glssm_natural_parametrization} together with the reparameterization trick leads to a convex target function. As 
%\begin{align*}
%    \log g(z_{t} | w_{t}) &= -\frac{1}{2} \left( z_{t} - B_{t}x_{t} \right)^{T}\Omega^{-1}_{t} \left( z_{t} - B_{t}x_{t} \right) - \log\det \left(\Omega_{t} \right)  - \frac{p}{2} \log 2 \pi 
%\end{align*}
%is a convex function of $ \psi_{t} = \left( z_{t} \oslash \omega_{t}^{2}, - 1 \oslash \left( 2 \omega_{t}^{2} \right) \right)$, the same holds for the log-weights up to a constant
%\begin{align*}
%    \log \tilde w_{\psi} = \sum_{t = 1}^n \log p(y_{t} | x_{t}) - \log g(z_{t} | x_{t}) 
%\end{align*}
%as a function of $\psi = \left( \psi_{0}, \dots, \psi_{n} \right)$. Applying \Cref{prop:is-clt}, we obtain
%\begin{theorem}[CLT for EIS with the \acrshort{glssm}-proposal]
%    % define b(x, \psi') = -1/2 (\log p())
%\end{theorem}


\subsection{Central Limit Theorems for Importance Sampling in PGSSMs}
\label{subsec:clt_pgssms}

% yada yada
Consider a \acrshort{pgssm} with states $X = (X_{0}, \dots, X_{n})$ and observations $y = (y_{0}, \dots, y_{n})$. The target of interest is then $\P^{X} = \mathbb P^{X| Y=y}$ or $\P^{S} = \mathbb P^{S | Y=y}$, if such a signal exists. As argued in the discussion surrounding \Cref{eq:two_step_sample_ssms} the two targets are equivalent. Using a similar approach to the on presented in \Cref{sec:importance_sampling}, it seems obvious that both the \acrshort{cem} and \acrshort{eis} fulfill a central limit theorem: we are constructing Gaussian proposals which form an exponential family, and so we should be able to apply \Cref{thm:cem-clt} and \Cref{thm:eis-clt} directly to this setting. 

However, we have to be somewhat more careful: the assumptions of the central limit theorems have to be checked thoroughly. Additionally, upon closer inspection, we see that the proposal distributions created by \acrshort{eis} do not necessarily form a natural exponential family, as we will see later. Lastly, the proposal we generate are Gaussian, which allows us to simplify some of the technical assumptions made earlier.

We will formulate all results in the matrix formulation of \acrshortpl{ssm}, see \citep[Section 4.13]{Durbin2012Time}. This form considers $X\in\R^{(n+1)m}$, $Y\in\R^{(n+1)p}$ and so on, ignoring the Markov-structure introduced by the \acrshort{ssm}. However, as we have seen in \Cref{eq:P_structure}, the Markov-structure is still present, if only in the precision matrix of the states $X$. 

%% CEM
We begin with the \acrshort{cem}. As the precision matrix is the natural parameter for the multivariate Gaussian exponential family, we see that model \eqref{eq:markov-proposal}, parameterized by $(P^{-1}v, P)$, with $P$ as in \Cref{eq:P_structure}, form a natural exponential family. Thus, we can apply \Cref{thm:cem-clt} to obtain a central limit theorem when applying the \acrshort{cem} for this model. 

\begin{lemma}[Markov-proposal forms exponential family]
    \label{lem:markov_proposal_exponential_family}
    Let $\G_{\psi}$, $\psi \in \Psi$ be the family of Gaussian proposals from \Cref{eq:markov-proposal}, reparameterized by 
    $$
        \psi = \left( Pv, P \right),
    $$
    where $P$ is as in \Cref{eq:P_structure}. Then $(\G_{\psi})_{\psi \in \Psi}$ form an exponential family.
\end{lemma}

\begin{proof}
    As 
    $$\Psi \subseteq \left\{(\Sigma ^{-1} \mu, \Sigma^{-1}) | \mu \in \R^{(n+1)m}, \Sigma \in \R^{((n+1)m)\times ((n+1)m)}\text{ positive definite}\right\},$$
    it suffices to show that $\Psi$ is a subspace, i.e. closed under addition and scalar multiplication. That $\Psi$ is closed under scalar multiplication is obvious. For $(P_{1}v_{1}, P_{1}), (P_{2}v, P_{2}) \in \Psi$ we have 
    $$
        (P_1v_{1}, P_{1}) + (P_2v_{2}, P_{2}) = \left( (P_{1} + P_{2}) (P_{1} + P_{2})^{-1} (P_{1}v_{1} + P_{2}v_{2}, P_{1} + P_{2})\right).
    $$
    Furthermore, as $P_{1}$ and $P_{2}$ possess the sparsity structure given by \Cref{eq:P_structure}, so does $P_{1} + P_{2}$, so $(P_1v_{1}, P_{1}) + (P_2v_{2}, P_{2}) \in \Psi$.
\end{proof}


Thus, the proposals $(\G_{\psi})_{\psi \in \Psi}$ form a natural exponential family, a sub-family of the full Gaussian exponential family. Its sufficient statistic is $T(x) = \begin{pmatrix}
    x \\ xx^{T}
\end{pmatrix}$. The central limit theorem for the \acrshort{cem} follows:

\begin{theorem}[\acrshort{clt} for the \acrshort{cem} in \acrshortpl{pgssm}]
    \label{thm:cem_clt_ssm}
    Let $\G_{\psi}$ be as in \Cref{lem:markov_proposal_exponential_family} and let $\G \gg \P^{X}$ be a proposal distribution. Suppose the following additional assumptions hold:
    \begin{enumerate}
        \item $w, w\id, w\id^2 \in L^{2}(\G)$,
        \item $\P^{X}$ posses all fourth order moments, and
        \item the covariance matrix of $\P^{X}$ is positive-definite.
    \end{enumerate}
    
    Then $\hpce$ fulfills a \acrshort{clt} of the form 
    $$
        \sqrt{N} \left(\hpce - \pce \right) \to \mathcal N(0, BMB)
    $$
    where $B$ and $M$ are as in \Cref{thm:cem-clt}.
\end{theorem}

\begin{proof}
    We will check that all assumptions of \Cref{thm:cem-clt} are fulfilled. 

    That $\pce\in\Psi$ is the unique maximizer of \Cref{eq:ce_argmax} follows from \Cref{prop:cem_exponential_families}, as the full Gaussian exponential family has non-degenerate Fisher information. That $T \in L^{2} (\P)$ follows from the definition of $T$ and the existence of forth order moments.
\end{proof}

The case for \acrshort{eis} is more involved. Let us start with the simplest case where $n=0$, where there is only a single observation $Y = y_{0} \in \R^{m}$. Indeed, all \acrshortpl{ssm} may be viewed this way, by simply ignoring the time-dependence structure and vectorizing all involved states, signals and observations. 
In this setting, proposal densities for $\P^{X}$ are of the form 
\begin{align*}
    p_{\psi}(x|z) &\propto p_{\psi}(z|x) p(x) \\
    &\propto \det(\Omega^{-1}) \exp \left( - \frac{1}{2} (z-Bx)^{T}\Omega^{-1}(z - Bx) \right) \det (\Sigma^{-1}) \exp \left( - \frac{1}{2}(x - \mu)^{T}\Sigma^{-1} (x - \mu) \right),
\end{align*}
which suggests $\psi = \left( B\Omega ^{-1}z, B^{T}\Omega^{-1}B \right)$. However, unless $B$ is invertible, the exponential family under consideration will, in general, fail to fulfill the uniqueness properties required for consistency and the central limit theorem, e.g. \Cref{prop:eis_exponential_families} can fail to apply. Additionally, working with $\psi = \left( B\Omega ^{-1}z, B^{T}\Omega^{-1}B \right)$ is tedious, as we have to take care of $B$ potentially being rank-deficient. Instead, we will focus on the target $\P^{S}$ from which we can reconstruct $\P^{X}$, as long as $s = Bx$ has a $\P^{S}$ a.s. solution. As the support of $\P^{S} = \mathbb P^{S | Y = y}$ can only be smaller than that of $\mathbb P^{S}$, this is always the case. For simplicity, we assume that $B$ has full row-rank, i.e. $\operatorname{image}(B) = \R^{(n+1)p}$

Furthermore, the proofs of \Cref{thm:eis-consistent} and \Cref{thm:eis-clt} only involve the densities of the proposal through the log-weights, up to additive constants that do not depend on $s$. In our setting, we can write the log-weights as 
$$
    \log w_{\psi}(s) = \log p(s|y) - \log g_{\psi}(s|z) = \log p(y|s) - \log  g_{\psi}(z|s) - (\log p(y) -  \log g_\psi(z)).
$$
Now $g_{\psi}(z|s)$ is just a Gaussian natural-exponential family with parameter $\psi = (\Omega^{-1}z, \Omega^{-1})$ and sufficient statistic $T(x) = \begin{pmatrix} x\\xx^{T} \end{pmatrix}$. The parameter space is 
$$
    \Psi = \left\{ (\Omega^{-1}z, \Omega^{-1}) \middle|
        z \in \R^{(n+1)p}, \Omega \in \operatorname{BDiagSPD((n+1),p)}
    \right\}
$$
where $\operatorname{BDiagSPD((n+1),p)}$ consists of all block-diagonal matrices $\Omega$ whose $n+1$ blocks on the diagonal are symmetric, positive-definite matrices of dimension $p\times p$.

In this setup, we can prove the central limit theorem for \acrshort{eis}.

\begin{theorem}[\acrshort{clt} for \acrshort{eis} in \acrshortpl{pgssm}]
    \label{thm:eis_clt_ssm}

    Let $(\G_{\psi})_{\psi\in\Psi}$ be the family of proposals with densities $g_{\psi}(\cdot|z)$, as described above. Assume that $B$ has full row-rank and let $\G \gg \P^{S}$ be a proposal with weights $w$. Furthermore, assume that the following conditions hold for $T_{s,i} = \begin{pmatrix}
        x_{s,i} \\
        x_{s,i}x_{s,i}
    \end{pmatrix}$, $s = 0, \dots, n$, $i=1,\dots,m$.

    \begin{enumerate}
        \item $wT_{s,i}T_{s',j} \in L^{2}(\G)$ for $i,j = 1,\dots, m$ and $s,s' = 0, \dots, n$.
        \item $\log p \in L^{4}(\P^{S})$ and
        \item $\cov _{\P^{S}}(T)$ is non-singular and $\peis \in \operatorname{int} \Psi$. 
    \end{enumerate}

    Then $\hpeis$ fulfills a \acrshort{clt} of the form 
    $$
        \sqrt{N} \left( \hpce - \pce \right) \to \mathcal N (0, BMB),
    $$
    where $B$ and $M$ are as in \Cref{thm:eis-clt}.
\end{theorem}

\begin{proof}
    We adapt the proof of \Cref{thm:eis-clt} to account for the new setup. The assumptions are the same, except that we can drop the assumption that $T_{s,i} \in L^{4}(\P^{S})$, as $\P^{S}$ possess all finite moments.

    Consider the weights $\log w_{\psi} = \log p(s|y) - \log g_{\psi}(s|z)$ and recall that \acrshort{eis} solves the optimization problem 
    $$
        \min_{\psi \in \Psi} \P^{S} \left[\log w_{\psi} - \P^{S} [\log w_{\psi} ]\right].
    $$
    Now 
    \begin{align*}
        \log w_{\psi} - \P^{S} [\log w_{\psi}] &= \log p(y|s) - \log g_{\psi}(z|s) - (\log p(y) - \log g_{\psi}(z))\\
        &= \log p(y|s) - \psi^{T}T(s) + \frac{(n+1)p}{2} \log 2\pi - Z(\psi) - (\log p(y) - \log g_{\psi}(z)),
    \end{align*}
    which motivates setting $\log \tilde w_{\psi} = \log p(y|s) - \psi^{T}T(s)$ and $\lambda = \P^{S} \log \tilde w_{\psi} = \frac{(n+1)p}{2} \log 2\pi - Z(\psi) - (\log p(y) - \log g_{\psi}(z))$. 
    This is precisely the setup from \Cref{eq:eis_exponential_families} which is all that is necessary for \Cref{thm:eis-clt} to apply.
\end{proof}

Notice that we did not give the precise structure of $B$ and $M$ for both \acrshortpl{clt}. The reason for this is that although we have imposed additional structure (Gaussian proposals), the matrices involved in the asymptotic covariance matrix are not more tractable. 

Let us close this section with a short recapitulation on what we have achieved. We have seen that both the \acrshort{cem} and \acrshort{eis} are able to exploit the Markov-structure of \acrshort{pgssm}, albeit in different ways. 

The \acrshort{cem} does not apply well to the \acrshort{glssm}-approach, the reason being, that matching the covariance matrix of the target to the proposal cannot be achieved by looking at marginals only. Instead, the \acrshort{glssm}-approach, which exploits the Markov-structure in the precision matrix, is more suited for the \acrshort{cem}. As the Markov-approach contains proposals that directly form an exponential family, the associated \acrshort{clt} follows naturally.

\acrshort{eis} is able to use the \acrshort{glssm}-approach, which has two benefits. First, the number of parameters is smaller, especially when there is a linear signal --- which is often the case in applications. Second, the optimization problem solved in \acrshort{eis} can be broken down into multiple smaller problems, see \Cref{eq:eis-marignals-target}. While the proposals for \acrshort{eis} do not form an exponential family, we still obtain \Cref{thm:eis_clt_ssm} by focusing on the signals and noting that the objective function is still, after appropriate reparametrization, a linear least squares problem of the parameters of interest, which is all that is necessary for the \acrshort{clt} to hold.

Similar to the \acrshortpl{clt} in \Cref{sec:importance_sampling}, we argue that \Cref{thm:cem_clt_ssm} and \Cref{thm:eis_clt_ssm} are widely applicable. The integrability assumptions on weights and targets are natural if we want importance sampling with Gaussian proposals to work, while the remaining assumptions are required for the optimal parameters $\pce$ and $peis$ to exist and be unique.

However, while the \acrshortpl{clt} provide theoretical justifications for using the two methods, the practical applicability, especially numerical stability, especially when the number of samples $N$ is low, does not follow from them. We will come back to this point in \Cref{sec:simulation_studies}.