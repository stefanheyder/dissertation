\section{Interim discussion}
\label{sec:interim-discussion}

Before we apply \gls{eis} and the \gls{cem} in the \gls{ssm} context, let us consolidate what we have achieved by the asymptotic analysis in the preceding two subsections and reason which of the two methods should be used in which circumstances.

% region optimal values
%% CE depends only on first moment of T, EIS also on second moments, as well as log p
We start with a discussion of the optimal values $\pce$ and $\peis$.
Notice that $\peis$ depends on second-order moments of the sufficient statistic $T$, as well as the shape of $\log p$, whereas the optimal parameter for the \gls{cem} $\pce$ depends only on the first-order moments of $T$. 
%% dependence on higher moments might help w/ better fit to heavier tail distributions
This dependence on higher-order moments may be beneficial for the \gls{eis} method, for example, if the covariance of $T$ under $\P$ is very different from that under $\G_{\psi}$. 
%% reiterate Dkl vs. log var

% endregion

% region assumptions
%% EIS requires higher moments to work, so might fail if target does not have these moments
The two methods differ concerning the assumptions that are required for uniqueness, consistency and the central limit theorem to hold if the proposals come from an exponential family. 
%% uniqueness
For uniqueness, \Cref{prop:cem_exponential_families} and \Cref{prop:eis_exponential_families}, both methods require that the covariance of $T$ is non-singular, however, the measures under which the covariance is considered differ: for the \gls{cem} we need $\cov_{\G_{\pce}} (T)$ to be non-singular, while for \gls{eis} the same has to hold for $\cov_{\P} (T)$. While the former is easy to ensure, the latter depends on the intractable target $\P$ and may be more difficult to verify in practice, depending on $T$. 

%% consistency & CLT
Regarding the consistency results, \Cref{thm:cem-consistent,thm:eis-consistent} as well as the central limit theorems, \Cref{thm:cem-clt,thm:eis-clt}, \gls{eis} requires that the sufficient statistic be twice as often $\P$-integrable as the \gls{cem}. Additionally, the \gls{eis} results assume that $\log p$ is sufficiently often $\P$-integrable. Therefore, \gls{eis} is, at first glance, more restrictive than the \gls{cem}. However, our application will perform importance sampling with Gaussian proposals where $T(x) = \begin{pmatrix} x \\ xx^{T}\end{pmatrix}$. For importance sampling to be consistent in this setting, we have to assume that the target has lighter tails than the Gaussian proposal, which implies that all polynomial moments of the target, and thus of $T$ exist. A similar argument can be made for $\log p$, and so the assumptions are likely to be fulfilled when Gaussian importance sampling is consistent.

% region asymptotic covariances
%% assuming Cov_P T approx Cov_psi T, main differences lies in Ms
To compare the asymptotic covariance matrices of both methods, note that both covariance matrices have the same ``{}bread-meat-bread''{} factorization, as they are asymptotic covariance matrices of M-estimators. We see that both $$B_{\ce} = I(\psi) = \cov_{\G_{\pce}}(T)^{-1}$$ and $$B_{\eis} = \cov_{\P}(T)^{-1}$$ are precision matrices of the sufficient statistic $T$, one with respect to the optimal \gls{cem} proposal and one with respect to the target. Thus, if $\P$ is well approximated by $\G_{\pce}$, we would expect these two components to be close to one another. 
For $$M_{\ce} = \cov_{\G} \left( w (T - \P[T]) \right)$$ and $$M_{\eis} = \cov_{\G}\left( w \log (w_{\peis})(T-\P[T]) \right),$$ there is a more notable difference, i.e. the presence of the $\log w_{\peis}$ term. If the \gls{eis} approximation performs well, we can expect this term to be small, as it is the prediction error of the least squares approximation of $\log g_{\psi}$ to $\log p$. Therefore, we expect that \gls{eis} outperforms the \gls{cem} in terms of asymptotic variance in these settings. In agreement with \Cref{prop:eis-finite-sample}, $M_{\eis} = 0$ if $\log p = \log g_{\psi_{\P}}$ so that $\peis = \psi_{\P}$.
%% however Mce might allow for optimization, by choosing G s.t. Cov wT is small, ie VM?
% endregion

% region choice of \G
Additionally, both $M_{\ce}$ and $M_{\eis}$ depend on the proposal $\G$, and indicate how one might tailor the initial proposal $\G$ to produce low-variance estimates. For the \gls{cem} we might choose $\G$ such that the trace or determinant of $\G [w^{2} (T - \P[T]) (T - \P [T])^{T}]$ becomes small. This is not necessarily achieved by the \gls{cem} proposal $\G_{\hpce}$, and so it may be worthwhile to investigate using two types of proposals in the \gls{cem}, one that makes $M_{\ce}$ small and $\G_{\pce}$. This is especially relevant as our simulation studies, \Cref{sec:simulation_studies}, suggest that the asymptotic covariance of the \gls{cem} is usually larger than that of \gls{eis}. For \gls{eis}, a similar approach might be fruitful, but is not as urgent as that for the \gls{cem}, as the asymptotic covariance of \gls{eis} is usually small enough to be feasible in practice.
% endregion

% region novelty / extension / outlook
%% asymptotic properties of both methods have not been studied to authors knowledge
Finally, let us stress that these asymptotic considerations are, to the author's knowledge, novel results and should be straightforward to extend if the proposals $\left(\G_{\psi}\right)_{\psi \in \Psi}$ do not form a natural exponential family. As any minimal exponential family may be reduced to a natural exponential family by reparametrization, see \citep[Theorem 1.9]{Brown1986Fundamentals}, the delta method can be used to derive \glspl{clt} in this case as well, as \Cref{prop:cem_exponential_families} and \Cref{prop:eis_exponential_families} still apply. If the family is not minimal the optimal values $\peis$ and $\pce$ may be non-unique, so we cannot hope to estimate them consistently. In this case the user should choose a minimal parametrization, see again \citep[Theorem 1.9]{Brown1986Fundamentals}. 
For non-exponential family proposals our results should also carry over, provided the usual regularity conditions ensuring uniqueness, consistency and asymptotic normality for M-estimators hold. If the objective functions are not concave as they are in our setting, one usually requires uniformly bounded third-order derivatives of the objective function to exist. 

Furthermore, our results can also be extended to the so-called \gls{vmm} which determines an optimal proposal by solving the following optimization problem:
$$
\psi_{\text{VM}} = \argmin_{\psi\in\Psi} \var_{\G_{\psi}} \left( w_{\psi}  \right) = \argmin_{\psi \in \Psi} \G_{\psi} \left[w_{\psi}^2\right] = \argmin_{\psi\in\Psi} \P \left[w_{\psi} \right],
$$
where the first equality holds as $\G_{\psi}[w_{\psi}] = 1$ for all $\psi$. Thus the \gls{vmm} chooses $\psi$ such that the second moment of importance sampling weights, $\rho$, becomes small. Again, this is sensible by the discussion surrounding $\rho$ and the \gls{ess}. Similar to the \gls{cem} and \gls{eis}, one uses importance sampling with a proposal $\G$ to approximate $\P [w_{\psi}]$ by $\hat\P_{N} [w_{\psi}]$, and solves this noisy version of the problem.
Unfortunately, there is no closed form for the optimal $\psi_{\text{VM}}$ or $\hat\psi_{\text{VM}}$, even if the proposals form a natural exponential family. Still, as $\psi \mapsto w_{\psi}(x)$ is convex, so is $\psi \mapsto \P [w_{\psi}]$, and we can apply \Cref{thm:haberman-consistent,thm:haberman-clt} in combination with \Cref{prop:is-consistency,prop:is-clt} to show, under suitable regularity conditions, the consistency and asymptotic normality of the method. 
% endregion

% region transition
Now that we have gained theoretical insight into optimal importance sampling, let us apply these insights to the \glspl{ssm} that we are interested in.
% endregion