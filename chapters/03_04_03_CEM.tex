\glsreset{cem}
\subsection{The \texorpdfstring{\Acrfull{cem}}{Cross-Entropy method}}
\label{subsec:cem}
Recall from our discussion surrounding \Cref{thm:chatterje2018Thm1} that for importance sampling to be effective, we should have a small \acrshort{kld} between the target $\P$ and the proposal $\G$. As the \acrshort{kld} depends on global properties, i.e. the Radon-Nikodym derivative $\rnd{\P}{\G}$, minimizing it should lead to a global approximation of $\P$, improving on the local-approximation provided by the \acrshort{la}.

The \gls{cem}\citep{Rubinstein1999CrossEntropy,Rubinstein2004CrossEntropy} implements this idea and selects from a parametric family $ \left( \G_{\psi} \right)_{\psi \in \Psi}$ of proposals the one that minimizes the \gls{kld} to the target. Here $\Psi$ is usually a subset of $\R^{k}$. We will usually assume the existence of a common dominating measure $\mu$ for both $\P$ and all $\G_{\psi}$, $\psi \in \Psi$ with corresponding densities $p$ and $g_{\psi}$, $\psi \in \Psi$. The importance sampling weights are then given by 
$$
w_{\psi}(x) = \frac{p(x)}{g_{\psi}(x)},
$$
$x \in \mathcal X$, or, if either on of $p$ and $g_\psi$ is only available up to a constant, by 
$$
\tilde w_{\psi} (x) \propto \frac{p(x)}{g_{\psi}(x)}.
$$
If the dependence on $\psi$ is not of interest or the particular $\psi$ is obvious from the context, we may drop the subscript. 

The \acrshort{cem} finds $\psi_{\text{CE}}$ which solves the following optimization problem
\begin{align*}
    \psi_{\text{CE}} &= \argmin_{\psi \in \Psi} \Dkl{\P}{\G_{\psi}} \\
    &= \argmin_{\psi \in \Psi} \P \left[ \log w_{\psi}\right]
\end{align*}
The existence and uniqueness of $\psi_{\text{CE}}$ will depend heavily on the choice of parametric family $(\G_{\psi})_{\psi \in \Psi}$ and $\P$. 

If $\P$ and $\G_{\psi}$ possess densities $p$ and $g_{\psi}$ w.r.t. some common measure $\mu$, the same for all $\psi$, we may reformulate the optimization problem to maximize the cross-entropy between $p$ and $g_{\psi}$ instead:
\begin{align}
    \begin{split}
    \psi_{\text{CE}} &= \argmin_{\psi \in \Psi} \P \left[\log p \right] - \P \left[\log g_{\psi}\right] \\
    &= \argmax_{\psi \in \Psi} \P \left[\log g_{\psi}\right]
    \end{split} \label{eq:ce_argmax}.
\end{align}
Suppose now that $\psi \mapsto \log g_{\psi}(x)$ is (strictly) concave for $\P$-almost every $x \in \mathcal X$ and $\Psi$ is a convex subset of $\R^{k}$. Then $\psi \mapsto \P \left[\log g_{\psi}\right]$ is (strictly) concave as well. As a consequence, we may apply the usual results from convex optimization, i.e. every local maximum is a global one and if $\psi \mapsto \log g_{\psi}(x)$ is strictly convex for $\P$-almost every $x$, there is at most one maximizer \citep[Theorem 3.4.2]{Bazaraa2006Nonlinear}.

As we have seen in \Cref{lem:log-concavity}, the densities of exponential families are log-concave in the natural parameter, and as such they will be the ideal candidates for our investigations. 

%\begin{theorem}[consistencty of $\hat\psi_{\text{CE}}$]
%    \label{thm:ce-m-estimator}
%    Assume the following technical conditions apply:
%    \begin{itemize}
%        \item[A1] Uniform consistency of importance sampling
%            $$\sup_{\psi \in \Psi}\lVert \hat \G_N (w\log g_\psi) - \G(w \log g_\psi) \rVert \stackrel{P}{\to} 0$$ 
%        \item[A2] Regularity condition
%            $$ \partial_\psi \P \left(\log g_\theta\right) = \P \left(\partial_\theta \log g_\theta\right) $$
%            for all $\psi \in \Psi$
%        \item[A3] positive definite misspecified Fisher information
%            $$ \P \left(\left(\partial_\psi \log g_\theta\right)\left(\partial_\theta \log g_\theta\right)^T\right) > 0$$
%    \end{itemize}
%
%    Then $\hat\psi_{\text{CE}}$ is a consistent estimator of $\psi_{\text{CE}}$.
%\end{theorem}
%
%\begin{theorem}[asymptotic normality of $\hat\psi_{\text{CE}}$]
%    
%\end{theorem}

% analytical solution, MLE
An additional attractive property of the \gls{cem} for exponential families with natural parameter $\psi \in \R^{k}$, the optimal $\psi_{\text{CE}}$ only depends on the expected value $\P [T]$. 

\begin{proposition}[The \acrshort{cem} for exponential families]
    \label{prop:cem_exponential_families}
    Let $\left( \G_{\psi} \right)_{\psi \in \Psi}$ form a $k$-dimensional natural exponential family with log-densities 
    $$
    \log g_{\psi}(x) = \log h(x) + \psi^{T} T(x) - \log Z(\psi),
    $$
    and convex parameter space $\Psi \subseteq \R^{k}$. 
    Suppose $T, \log h \in L^{1}(\P)$.

    If there is a $\psi$ such that 
    $$
    \P[T] = \G_{\psi} [T],
    $$
    then $\psi = \pce$ is a maximizer of \Cref{eq:ce_argmax}. If $\cov_{\G_{\psi}} T$ is positive definite for all $\psi\in\Psi$, then the maximizer is unique.
\end{proposition}

\begin{proof}
    The target may be rewritten as
    $$
    \psi \mapsto f(\psi) = \P \left[\log g_{\psi}(x)\right]= \P [\log h] - \log Z(\psi) + \psi^{T} \P [T].
    $$
    As $\log Z(\psi)$ is the cumulant-generating function of $\G_{\psi}$ it is twice differentiable, and so is $f$. The gradient of $\log Z(\psi)$ is 
    $$
    \nabla_{\psi} \log Z(\psi) = \G_{\psi} T
    $$
    and its Hessian is 
    $$
    H_{\psi} \log Z(\psi) = \cov_{\G_{\psi}} T
    $$
    the covariance of $T$ under $\G_{\psi}$. Thus the Hessian of $f$ is 
    $$
    H_{\psi} f = - \cov_{\G_{\psi}} T,
    $$
    which is negative-semi-definite. Therefore $f$ is concave, and any local maximizer $\psi$ is a global maximizer. The gradient of $f$ is 
    $$
        \nabla_{\psi} f(\psi) = \P[T] - \G_{\psi} [T],
    $$
    so the optimal $\psi_{\text{CE}}$ solves
    $$
    \P[T] = \G_{\pce}[T].
    $$
    If $\cov_{\G_{\psi}} T$ is positive definite for all $\psi \in \Psi$, $f$ is strictly concave, and the maximizer is unique.
\end{proof}
As a consequence, the \acrshort{cem} for natural exponential families reduces to matching the moments of the sufficient statistic of the target and proposal.
In many cases, this system of equations can be solved analytically or by gradient descent algorithms.
Let us discuss the assumptions and applicability of this proposition. Assuming that $T, \log h \in L^{1}(\P)$ is necessary for the target to be finite, so it cannot be dropped. As $T$ and $\log h$ typically consist of polynomial, rational or exponential functions, this is not too restrictive. The proof of uniqueness relies on $\cov_{\G_{\psi}} T$ being positive definite, to ensure that $\psi \mapsto \log Z(\psi)$ is strictly convex. This could also be achieved by requiring the exponential family to be minimal, see \citep[Theorem 1.13 (iv)]{Brown1986Fundamentals}. The existence of a $\psi$ such that $\P T = \G_{\psi} T$ is not restrictive for most commonly used distributions: for the (multivariate) normal, Poisson, negative binomial and binomial distribution there is always a unique solution, as the sufficient statistics consist of means and covariances. 

While $\P T$ is usually not available, it is itself amenable to importance sampling. Given a proposal $\G$ we may estimate $\P T$ by $\hat\P_N T = \sum_{i = 1}^{N} W^{i} T(X^{i})$ for $X^{1}, \dots, X^{N} \iid \G$ and auto-normalized importance sampling weights $W^{i}$ and in turn, applying \Cref{prop:cem_exponential_families}, estimate $\psi_{\text{CE}}$ by $\hat \psi_{\text{CE}}$ solving
$$
\hat \P_N T = \G_{\hpce} T.
$$
As $T, \log h \in L^{1}(\hat \P_{N})$, the only conditions we have to check to apply the above proposition is that this equation has a unique solution and that $\Psi$ is convex. 

In what follows, we will derive novel results on the performance of this estimator. In particular, we will investigate under which conditions $\hpce$ is consistent and asymptotically normal. While we restrict ourselves here to the setting of $k$-dimensional natural exponential families, these results should generalize to other classes of distributions as well. The advantage that this class of families has is that due to the structure of the densities, they provide straightforward (regularity) conditions for the asymptotic results to hold. As the target functions are concave, these conditions are rather liberal. We start with proving the consistency of $\hpce$. 

\begin{proposition}[consistency of $\hpce$]
    \label{prop:cem-consistent}
    Adopt the same assumptions as in \Cref{prop:cem_exponential_families}. Furthermore, let $\G$ be a proposal distribution, assume that $\pce$ is the unique maximizer of \Cref{eq:ce_argmax} and that $\pce$ is in the interior of the convex parameter space $\Psi$ and there exists a neighborhood of $\pce$ that is contained in $\Psi$. Then $\hpce$ is a strongly consistent estimator of $\pce$.
\end{proposition}

The proof is based on the following theorem of \citeauthor{Haberman1989Concavity}.

\begin{theorem}[{\citep[Theorem 5.1]{Haberman1989Concavity}}\footnote{Note that while the actual theorem assumes conditions 1,2,5 and 6 in the paper, C3 as stated here implies conditions 5 and 6, see also the discussion in Sections 2.3 and 2.4 in \citep{Haberman1989Concavity}.}]
    \label{thm:haberman-consistent}
    Let $\Psi \subseteq \R^{k}$, $\mathcal X$ a separable, complete metric space and $b: \mathcal X \times \R^{k} \to [- \infty, \infty )$ such that for every $x \in \mathcal X$ the function $$b(x, \cdot): \R^{k} \to [-\infty, \infty), \psi \mapsto b(x, \psi)$$ is concave. Let $\P$ be a probability measure on $\mathcal X$ such that $\P [b(\cdot, \psi)] < \infty$  for all $\psi \in \R^{k}$. Assume that $\psi^{\ast} \in \Psi$ is the unique maximizer of $$b_{\Psi}: \Psi \to [-\infty, \infty), \psi \mapsto \P [b(\cdot,\psi)].$$ For $N\in\N$, let $X^{1}, \dots, X^{N}  \iid \P$ and $\hat \P_{N} = \frac{1}{N} \sum_{i = 1}^N \delta_{X^{i}}$ their empirical distribution. Let $\left(\hat\psi_{N}\right)_{N \in \N}$ be a sequence M-estimators, i.e. a sequence of maximizers of
    $$
    \hat b_{\Psi}: \Psi \to [-\infty, \infty), \psi \mapsto \hat \P_{N} [b(\cdot, \psi)].
    $$
    
    Assume that the following conditions hold:
    \begin{enumerate}[label=(C\arabic*),ref=(C\arabic*)]
        \item\label{it:C1} For some closed set $N$, $\psi^{\ast}$ is in the interior of $N$ and $\Psi \cap N$ is closed. 
        \item\label{it:C2} $\psi^{\ast}$ is the unique maximizer of $$b_{\operatorname{cl} (\Psi)}: \operatorname{cl} (\Psi) \to [-\infty, \infty), \psi \mapsto \P [b(\cdot, \psi)],$$ where $ \operatorname{cl}$ denotes the closure of $\Psi$ in $\R^{k}$.
        \item\label{it:C3} $\Psi$ is convex and $b_{\Psi}$ is finite on a nonempty open set.
    \end{enumerate}

    Then $$\hat \psi_{N} \stackrel{N \to \infty}\to \psi^{\ast}$$ $\P$-almost surely, so $\hat\psi_{N}$ is strongly consistent.
\end{theorem}

The assumptions of this theorem ensure that the unique optimum is in the interior of $\Psi$ and \glqq{}well-separated\grqq{} from its boundary, so concavity of $b(x, \psi)$ together with the law of large numbers yield uniform convergence of $\hat P_{N} [b(\cdot, \psi)] \to \P [b(\cdot, \psi)]$ on compacta and thus also for $\hat\psi_{N}$.

To apply this theorem to our setting, let us begin by incorporating importance sampling. 

\begin{lemma}
    \label{lem:is-consistency}
    Assume that the conditions of \Cref{thm:haberman-consistent} are fulfilled and let $\G \gg \P$ be another probability measure with Radon-Nikdoym derivative $w(x) = \rnd{\P}{\G}(x)$. Let $X^{1}, \dots, X^{N} \iid \G$ be samples and consider the particle approximations 
    \begin{align*}
        \tilde \P_{N} &= \frac{1}{N}\sum_{i = 1}^N w(X^{i})\delta_{X^{i}},  \\ 
        \hat \P_{N} &= \sum_{i = 1}^{N} W^{i} \delta_{X^{i}},
    \end{align*}
    and suppose there exist sequences of M-estimators 
    \begin{align*}
    \tilde \psi_{N} &\in \argmax_{\psi \in \Psi} \tilde \P _{N} \left[ b(\cdot, \psi) \right],\\
    \hat \psi_{N} &\in \argmax_{\psi \in \Psi} \hat \P _{N} \left[ b(\cdot, \psi) \right].
    \end{align*}

    Then both $\tilde\psi_{N}$ and $\hat\psi_{N}$ are strongly consistent estimators of $\psi^{\ast}$.
\end{lemma}
\begin{proof}
    Define a new objective function $\tilde b : \mathcal X \times \R^{k} \to [-\infty, \infty)$ by 
    $$
    \tilde b(x, \psi) = w(x) b(x, \psi).
    $$
    Then $\G \left[ \tilde b(\cdot, \psi) \right] = \P \left[ b(\cdot, \psi) \right]$ for all $\psi\in\Psi$, and so $\psi^{\ast}$ is the unique global maximum of $\psi \mapsto \G \left[ \tilde b(\cdot, \psi) \right]$. 
    As $\G [\tilde b(\cdot, \psi)] = \P [b(\cdot, \psi)] < \infty$ and for fixed $x$ $\tilde b(x, \cdot) = w(x)b(x,\cdot)$ is concave, we may directly apply \Cref{thm:haberman-consistent} to $\tilde\psi_{N}$, showing its strong consistency.

    For $\hat\psi_{N}$, notice that for a fixed sample $X^{1}, \dots, X^{N} \iid \G$ and any function $f: \mathcal X \to [-\infty, \infty)$ we have, a.s., 
    $$
        \hat \P _{N} [f] = \sum_{i = 1}^{N} W^{i} f(X^{i}) = \frac{\G [\tilde w]}{\sum_{i = 1}^N \tilde w(X^{i})} \sum_{i = 1}^{N} \frac{\tilde w_{i}}{\G[\tilde w]} f(X^{i}) = \frac{\G [\tilde w]}{\sum_{i = 1}^{N} \tilde w(X^{i})} \tilde \P _{N} [f] \propto \tilde \P_{N} [f],
    $$
    where $\tilde w$ are the unnormalized weights, i.e. $ \frac{\tilde w(x)}{\G [\tilde w]} = w(x), x \in\mathcal X$.
    Thus $\hat \psi_{N}$ maximizes $\tilde \P_{N} [b(\cdot, \psi)]$ as well, and the result follows from the consistency of $\tilde \psi_{N}$.
\end{proof}

\begin{proof}[Proof (\Cref{prop:cem-consistent})]
    We show that the assumptions of \Cref{thm:haberman-consistent} are fulfilled. Let
    \begin{align*}
        b: \R^{p}\times \R^{k} \to [-\infty, \infty) && b(x, \psi) = \begin{cases}
            \log g_{\psi}(x) & \psi \in \Psi, \\
            - \infty & \text{else.}
        \end{cases} 
    \end{align*}
    As $\Psi$ is convex and $g_{\psi}(x)$ is log-concave (see \Cref{lem:log-concavity}), $b(x, \cdot)$ is concave. Let $X^{1}, \dots, X^{N} \iid \P$ and let $\tilde{\P}_{N} = \frac{1}{N} \sum_{i = 1}^N \delta_{X^{i}}$. For $\psi \in \Psi$ we have
    $$
    \P [b(\cdot, \psi)] = \P [\log h] + \psi^{T}\P[T] - \log Z(\psi) < \infty,
    $$
    as $\log h, T \in L^{1}(\P)$, while for $\psi \notin \Psi$ this integral is $-\infty$. Thus we only have to check that \ref{it:C1}-\ref{it:C3} are fulfilled. 
    
    For condition \ref{it:C1} note that, as $\pce$ is in the interior of $\Psi$, we may choose $\varepsilon > 0$ such that the closed $\varepsilon$ ball around $\pce$, $\bar B_{\varepsilon} (\pce)$ is completely contained in $\Psi$, so letting $N = \bar B_{\varepsilon} (\pce)$ implies the condition. condition \ref{it:C2} is fulfilled by the definition of $b$ and condition \ref{it:C3} is fulfilled by considering the neighborhood of $\pce$ that is assumed to be contained in $\Psi$. Finally, by \Cref{lem:is-consistency}, $\hpce$ is strongly consistent.
\end{proof}

The assumptions on $\pce$ and $\Psi$ in \Cref{prop:cem-consistent} could be somewhat looser, as the concavity of the target function is a rather strong property. In natural exponential families, 
$$
\Psi = \left\{ \psi \in \R^{k} \,:\, Z(\psi) < \infty \right\}
$$
is always convex so this is not a strong restriction. In regular exponential families, $\Psi$ is open and so only uniqueness of $\pce$ is required. \todo{pce is moment based, does that not make it unique?}. 

\todo{check conditions under which cumulant generating function is diff. + equal to covariance}
Not only is $\log g_{\psi}$ concave, but it also has second-order derivatives. Indeed, its Hessian is given by the inverse of the Fisher-information matrix $I(\psi)^{-1}$:
$$
H_{\psi} \log g_{\psi} = - H_{\psi} \log Z (\psi) = - \cov_{\G_{\psi}} \left(T\right) = -I(\psi)^{-1}.
$$
These rather strong properties enable us to derive a central limit theorem for the \acrshort{cem} with natural exponential family proposals under quite liberal conditions.
Let us start again with a general version of a central limit theorem for M-estimators based on concave objective functions. 
\begin{theorem}[{\citep[Theorem 6.1]{Haberman1989Concavity}}\footnote{Note, again, that the original theorem is based on conditions 7,8,9 in the paper. However, under \ref{it:C7}, condition \ref{it:C10} implies conditions 8 and 9 in the paper. See the discussion in Section 3.1 in \citep{Haberman1989Concavity}.}]
    Consider the same setting as in \Cref{thm:haberman-consistent}. 

    Assume further that $\psi^{\ast}$ lies in the interior of $\Psi$ and that the following conditions hold: 
    \begin{enumerate}[label=(C\arabic*),ref=(C\arabic*)]
        \setcounter{enumi}{6}
        \item\label{it:C7} The Hessian $H_{\psi} \P \left[ b(\cdot, \psi) \right]$ exists and is non-singular.
        \setcounter{enumi}{9}
        \item\label{it:C10} For $X\sim \P$ and some neighborhood $N$ of $\psi^{\ast}$ 
        \begin{align*}
            \sigma^{2}(\psi, \xi) = \mathbb E \left( b'(X, \psi, \xi)  \right) < \infty && \psi \in N, \xi \in \R^{k},
        \end{align*}
        where $b'(x, \psi, \xi) = \lim_{a \downarrow} a^{-1} \left( b(x, \psi + a\xi) - b(x, \psi)\right)$ is the directional derivative. Note that if $b$ is differentiable at $x$, $b'(x, \psi, \xi) = \xi^{T}\nabla_{\psi} b(x, \psi)$.
    \end{enumerate}
    
    Let $M = \cov \left( \nabla_{\psi} b(X, m) \right)$ and let $B = - \left(H_{\psi} \P \left[ b(\cdot, \psi) \right]\right)^{-1}$. Then 
    \begin{equation}
        \sqrt{n} \left( \hat\psi_{N} - \psi \right) \convD \Normal (0, BMB).
    \end{equation}
    
\end{theorem}



% proof: appeal to vdV, pd ensures that M(\theta) is convex, so global maximum unique and well separated
$\hat\psi_{\text{CE}}$ is a Z-estimator, i.e. an estimator that arises from solving a random system of equations, and we can analyze its asymptotic behavior using standard results from the theory of Z-estimators. 
The following theorem of \citep{VanderVaart2000Asymptotic} will be useful in analyzing the asymptotic behavior of the estimators we consider in this thesis. We state it here, using our notation, for completeness.

\begin{theorem}[asymptotic normality of Z-estimators, {\citep[Theorem 5.21]{VanderVaart2000Asymptotic}}]
    \label{thm:clt_z_est_vdv}
    For every $\psi$ in an open subset of $\R^{k}$, let $x \mapsto f_{\psi}(x)$ be a measurable vector-valued function such that, for every $\psi_{1}$ and $\psi_{2}$ in a neighborhood of $\psi_{0}$ and a measurable function $\dot f$ with $\G \left[\dot f\right] < \infty$,
    \begin{align}
    \label{eq:clt-vdv-local-lipschitz}
    \lVert f_{\psi_{1}}(x) - f_{\psi_{2}}(x)\rVert \leq \dot f(x) \lVert \psi_{1} - \psi_{2}\rVert \tag{LL}.
    \end{align}

    Assume that $\G \left[\lVert f_{\psi_{0}}\rVert \right] < \infty$ and that the map $\psi \mapsto \G \left[f_{\psi}\right]$ is differentiable at $\psi_{0}$, with nonsingular derivative matrix $B^{-1}$. Let $X_{1}, \dots, X_{N} \iid \G$ and $\hat \G_{N} = \sum_{i = 1}^{N} \delta_{X^{i}}$. If $\hat\psi_{N}$ fulfills $$\hat\G_{N} \left[f_{\hat\psi_{N}}\right] = o_{P} \left( N^{-\frac{1}{2}} \right),$$ and $\hat\psi_{N} \to \psi_{0}$ in probability, then
    \begin{align}
        \label{eq:clt-vdv}
        \sqrt{N} \left( \hat\psi_{N} - \psi_{0} \right) \Dto \mathcal N(0, BMB^{T}),
    \end{align}
    where $M = \G f_{\psi_{0}} f_{\psi_{0}}^{T}$.
\end{theorem}

\begin{notation}[central limit theorem for Z-estimators]
    \label{not:notation-clt}
    The central limit theorems derived in this and the next section will make frequent use of \Cref{thm:clt_z_est_vdv}. We will use the following consistent notation in the statement of theorems and their proofs:
    \begin{itemize}
        \item $f_\psi(x): \R^{k} \to \R^{k}$ the estimating equation              
        \item $B = \left(\G\partial_{\psi} f_{\psi}\right)^{-1}$ the bread matrix
        \item $M = \G f_{\psi}f_{\psi}^{T}$ the meat matrix
        \item $V = BMB$ the asymptotic covariance matrix
        \item $\log g_{\psi}(x) = \psi^{T}T(x) + \log h(x) - \log Z(\psi)$ the density of the natural exponential family considered
        \item $\dot z (\psi) = \nabla_{\psi}\log Z(\psi) = \G_{\psi} T$ the derivative of the log-normalizing constant $\psi \mapsto \log Z(\psi)$
        \item $\ddot z(\psi) = \partial_{\psi} \dot z(\psi) =\cov_{\G_{\psi}}T$ the Hessian of the log-normalizing constant $\psi \mapsto \log Z(\psi)$
    \end{itemize}
    The naming of $B$ and $M$ stems from the sandwich estimator \citep{White1982Maximum}, where the bread $B$ is the Jacobian of the estimating equations $\P f_{\psi} = 0$ and the meat $M$ is the covariance matrix of $f_{\psi}$ under $\P$, thus making a \glqq{}meat sandwich\grqq{}.
\end{notation}

\todo{discussion on relevance and novelty of these results}
\begin{theorem}[consistency of $\hat \psi_{\text{CE}}$]
    \label{thm:ce-consistent}
    \todo{from van der Vaart/Casella Berger}
\end{theorem}


\begin{theorem}[asymptotic normality of $\hat \psi_{\text{CE}}$]
    \label{thm:ce-clt}
    %\todo{require unique solution?}
    Let $\G_{\psi}$ form a natural exponential family with densities $g_{\psi}(x) = \frac{h(x)}{Z(\psi)} \exp \left( \psi^{T}T(x)\right) $ w.r.t. $\mu$. Let $\G, \P$ be two other probability measures such that $\G \ll \P$ and let $w = \frac{\d \P}{\d \G}$ be the normalized importance sampling weights. 
    Suppose further that 
    \begin{enumerate}[label={\bfseries(A{\arabic*})}]
        \item\label{it:exist-unique-psice} $\G_{\hpce} T = \hat\P_{N} T$ $\mu$-a.s. has a unique solution $\hpce$,
        \item\label{it:ce-regularity} $\hpce$ consistently estimates $\pce$,
        \item\label{it:zdot-ll} $\psi \mapsto \dot z(\psi) = \nabla_\psi \log Z(\psi)$ is locally Lipschitz around $\psi_{\text{CE}}$,
        \item\label{it:w-t-wt-L2} $w,T,wT \in L^{2}(\G)$, and
        \item\label{it:FI-psd} the Fisher information $I(\psi_{\text{CE}})$ is positive definite and equal to $-\ddot z(\pce)$, additionally $\psi \mapsto I(\psi)$ is continuous in a neighborhood of $\pce$.
    \end{enumerate}

    Then, as $N$ goes to $\infty$,
    $$
        \sqrt{N} \left(\hat\psi_\text{CE} - \psi_{\text{CE}}\right) \Dto \mathcal N \left(0, V_{\text{CE}}\right)
    $$
    where 
    $$
    V_{\text{CE}} = B_\ce M_\ce B_\ce,%I(\psi_{\text{CE}})^{-1}  \text{Cov}_{\G} \left( w (T - \G_{\pce}T) \right) I(\psi_{\text{CE}})^{-1},
    $$
    with 
    \begin{align*}
        B_{\ce} &= I(\pce)^{-1}, \\
        M_{\ce} &= \cov_{\G} \left( w (T - \G_{\pce}T) \right).
    \end{align*}
    Moreover $\G (w(T - \G_{\pce}T)) = 0$, so we may estimate $V_{\text{CE}}$ consistently by plug-in:
    $$
    \hat V_{\text{CE}} = I(\hat\psi_{\text{CE}})^{-1}  \left(\sum_{i = 1}^N W^{2}_{i} \left(T(X^{i}) - \G_{\hpce}T \right)\left(T(X^{i}) - \G_{\hpce} T\right)^{T} \right)I(\hat\psi_{\text{CE}})^{-1}.
    $$
\end{theorem}

\begin{proof} We check that the conditions of the central limit theorem for Z-estimators (\Cref{thm:clt_z_est_vdv}) are fulfilled. This proof uses the notation established in \Cref{not:notation-clt}. Consider the estimating equations for $\pce$ 
    $$x\mapsto f_\psi(x) = \nabla_{\psi} \left(\tilde w(x)\log g_{\psi}(x)\right) = \tilde w(x) T(x) - \tilde w(x) \dot z (\psi),$$ where $\tilde w(x)$ are the unnormalized importance sampling weights. 
    Let $\hat\G_{N} = \frac{1}{N} \sum_{i = 1}^N \delta_{X^{i}}$ be the empirical measure. Then 
    \begin{align*}
    \hat \G_{N} f_{\hpce} = \frac{1}{N} \sum_{i = 1}^N \tilde w(X^{i}) \left( T(X^{i}) - \dot z (\hpce)\right) = \left(\frac{1}{N}\sum_{i=1}^N \tilde w(X^{i})\right) \hat \P_{N} \left[T - \dot z(\hpce)\right] = 0 && \text{a.s.},
    \end{align*}
    by \ref{it:exist-unique-psice}, as $W^{i} = \frac{\tilde w(X^{i})}{\sum_{i=1}^N \tilde w(X^{i})}$. 
    
    As $$\left\lVert f_{\psi_1}(x) - f_{\psi_2}(x)\right\rVert = \tilde w(x) \left\lVert \dot z (\psi_1) - \dot z(\psi_2)\right\rVert$$ for all $\psi_{1}, \psi_{2}\in \Psi$,  $\G \tilde w < \infty $ and \ref{it:zdot-ll} imply the local Lipschitz condition \Cref{eq:clt-vdv-local-lipschitz} in \Cref{thm:clt_z_est_vdv}.
    Furthermore, by \ref{it:w-t-wt-L2} it holds
    $$
    \G \left\lVert f_\psi \right\rVert^2 \leq \G \tilde w^2 \left\lVert \dot z(\psi) \right\rVert ^2  + 2 \lVert \dot z(\psi) \rVert \G \lVert \tilde wT\rVert + \G \left\lVert \tilde wT\right\rVert^2 < \infty.
    $$

    Additionally $\psi \mapsto\G f_\psi = (\G \tilde w) \dot z (\psi) + \G \tilde wT$ is differentiable everywhere, as $\psi \mapsto \log Z(\psi)$ is the cumulant generating function of the base measure. Its Jacobian is $(\G \tilde w) \ddot z(\psi)$, where  $\ddot z(\psi) = \partial_\psi \dot z(\psi)$ is the Hessian of the cumulant generating function, which equals the negative Fisher information $-I(\pce)$ as $\G_{\psi}, \psi \in \Psi$ form a natural exponential family and the regularity conditions \ref{it:FI-psd} allow differentiation under the integral. Thus 
    $$
        B_{\ce} = I(\pce)^{-1} = -\ddot z(\pce) = - \frac{1}{\G[\tilde w]} \partial_{\psi} \G [f_{\psi}]_{\vert \psi = \pce}.
    $$
    
    By definition of $\pce$,
    $$
    \G \left[ f_{\pce} \right] = \G[\tilde w] \left(\P \left[ T + \dot z(\pce) \right]\right) = \G[\tilde w](\P [T] + \dot z (\pce)) = 0,
    $$
    so 
    \begin{align*}
        M_{\ce} &= \text{Cov}_{\G} \left( w (T - \dot z (\pce)) \right) \\
        &= (\G \tilde w)^{2}\text{Cov}_{\G} \left( \tilde w(T - \dot z (\pce)) \right) \\
        &= (\G \tilde w)^{2}\text{Cov}_{\G}(f_{\pce}) = (\G \tilde w)^{2}\G [f_{\pce}f_{\pce}^{T}].
    \end{align*}

    By \Cref{eq:clt-vdv} the asymptotic covariance matrix is 
    $$
    V_{\ce} = \partial_{\psi}\G [f_{\psi}]_{\vert \psi = \pce} \G \left[f_{\pce}f_{\pce}^{T}\right] \partial_{\psi}\G [f_{\psi}]_{\vert \psi = \pce}= B_{\ce}M_{\ce}B_{\ce}
    $$
    which shows the asymptotic normality. 

    Estimating $B_{\ce}$ by $\hat B_{\ce}= I(\hpce)^{-1}$ and $$M_{\ce} = \G W^{2} (T - \G_{\pce}T)(T - \G_{\pce}T)^{T} = \P W (T - \G_{\pce})(T - \G_{\pce})^T$$ by $$\hat M_{\ce} = \hat\P_{N} W \left( T - \G_{\hpce} T \right)\left( T - \G_{\hpce} T \right)^{T}$$
    yields the stated plug-in estimator. \todo{something is fishy here}
    The promised consistency follows from \ref{it:w-t-wt-L2} and \ref{it:FI-psd}.
\end{proof}

The form of the asymptotic covariance matrix is that of the sandwich estimator \citep{White1982Maximum}, corrected for the importance sampling with $\G$. This is not surprising: the \acrshort{cem} essentially performs maximum likelihood estimation of $\psi$ where the data come from the misspecified $\P$. Additionally, we have to correct the variance for performing importance sampling with $\G$, instead of sampling directly from $\P$.

As mentioned above, if $(\G_\psi)_{\psi \in \Psi}$ do not form an exponential family, $\hat\psi_{\text{CE}}$ will still be consistent and asymptotically normal, provided the usual regularity conditions for M-estimators, or Z-estimators if $\log g_{\psi}$ is sufficiently smooth, apply. 

% break-down in higher dimensions?

% iterative procedure, CRNs

% applications of CEM
The \gls{cem} is routinely used for estimating failure probabilities for rare events \citep{Homem-de-Mello2007Study} and has been applied to Bayesian inference \citep{Engel2023Bayesian,Ehre2023Certified} and optimal control problems \citep{Kappen2016Adaptive,Zhang2014Applications}.
\todo{more lit. review CEM}
