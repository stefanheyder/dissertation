\glsreset{glssm}
\section{Gaussian Linear State Space Models}
\label{sec:linear_gaussian_state_space_models}

\acrfullpl{glssm} are the working horses of most methods used in this thesis because they are analytically tractable and computationally efficient. Indeed for fixed dimension of states $m$ and observations $p$ the runtime of algorithms that we consider in this thesis is $\mathcal O(n)$.

\glsunset{glssm}
\begin{definition}[\gls{glssm}]
    \label{def:glssm}
    A \acrfull{glssm} is a joint distribution over states and observations $(X,Y)$ where states a.s. obey the transition equation
    \begin{align}
        \label{eq:glssm_states}
        X_{t + 1} & = A_{t}X_{t} + u_{t} + \varepsilon_{t + 1} &  & t = 0, \dots, n - 1,
    \end{align}
    and observations a.s. obey the observation equation
    \begin{align}
        \label{eq:glssm_observations}
        Y_{t} & = B_{t}X_{t} + v_{t} + \eta_{t} &  & t = 0, \dots, n.
    \end{align}
    Here $A_{t} \in \mathbf{R}^{m \times m}$ and $B_{t} \in \mathbf{R}^{p \times m}$ are matrices that specify the systems dynamics. The \textbf{innovations} $(\varepsilon_{t + 1})_{t = 0, \dots, n-1}$ and \textbf{measurement noise} $(\eta_{t})_{t = 0, \dots, n}$ and the  starting value $X_{0} \sim \mathcal N (\E X_{0}, \Sigma_{0})$ are jointly independent. Furthermore, $\varepsilon_{t+1} \sim \mathcal N(0, \Sigma_{t})$ and $\eta_{t}\sim \mathcal N(0, \Omega_{t})$ are centered Gaussian random variables and $u_{t} \in \R^{m}, t = 0, \dots, n - 1$, $v_{t} \in \R^{p}, t = 0, \dots, n$ are deterministic biases.
\end{definition}

\begin{remark}
    From \Cref{eq:glssm_states} it is easy to see that the states $X = (X_{0}, \dots, X_{n})$ form a Gaussian Markov process and that conditional on $X_{t}$, $t \in \{0, \dots, n\}$, $Y_{t}$ is independent of $X_{s}$ and $Y_{s}$, $s < t$. Thus A \acrshort{glssm} is indeed a \acrshort{ssm}.
\end{remark}

The defining feature of a \gls{glssm} is that the joint distribution of $(X,Y)$ is Gaussian, as $(X,Y)$ may be written as an affine combination of the jointly Gaussian $(X_{0}, \varepsilon_{1}, \dots, \varepsilon_{n}, \eta_{0}, \dots, \eta_{n})$ and it is often useful to perform inferences in terms of innovations and measurement noise instead of states, see e.g. \citep[Section 4.5]{Durbin2012Time}.

As the joint distribution of $(X, Y)$ is Gaussian, so are conditional distributions of states given any set of observations.

\begin{lemma}[Gaussian conditional distributions]
    \label{lem:gaussian_conditional}
    Let $(X,Y)$ be jointly Gaussian with distribution $\mathcal N \left( \mu, \Sigma \right)$ where 
    $$
    \mu = \left(\mu_{X}, \mu_{Y}\right)
    $$
    and 
    $$
    \Sigma = \begin{pmatrix}
        \Sigma_{XX} & \Sigma_{XY} \\
        \Sigma_{YX} & \Sigma_{YY}
    \end{pmatrix},
    $$
    for non-singular $\Sigma_{YY}$, where $\mu$ and $\Sigma$ are partitioned according to the dimensions of $X$ and $Y$. 
    
    Then $X|Y = y$ is also a Gaussian distribution with conditional expectation
    $$
    \mu_{X|Y = y} = \E \left( X | Y = y \right) = \mu_{X} + \Sigma_{XY}\Sigma_{YY}^{-1} \left( y - \mu_{Y} \right)
    $$
    and conditional covariance matrix 
    $$
    \Sigma_{X| Y =y} = \cov \left( X | Y = y \right) = \Sigma_{XX} - \Sigma_{XY} \Sigma_{YY}^{-1} \Sigma_{YX}.
    $$
    
    In particular, let $X\sim \mathcal N(\mu, \Sigma)$ and $Y = BX + \varepsilon$ for a matrix $B \in \mathbf R^{p\times m}$ and $\R^{p} \ni \varepsilon \sim \mathcal N(0, \Omega)$ independent of $X$ where $\Omega \in \R^{p\times p}$ . 
    Then, as 
    $\E Y = B \mu$, $\cov \left( X,Y \right) = \cov \left( Y, X \right)^{T}= \Sigma B^{T}$ and $\cov \left( Y \right) = B \Sigma B^{T} + \Omega$, we have
    $$
        \E \left( X | Y = y \right) = \mu + K (y - B \mu)
    $$
    and 
    $$
    \cov \left( X | Y = y \right) = \Sigma - K \Sigma K^{T} = \left( I  -  KB \right) \Sigma,
    $$
    as long as $B \Sigma B^{T} + \Omega$ is non-singular.
    Here $K = \Sigma B^{T} \left( B \Sigma B^{T} + \Omega \right)^{-1}$.
\end{lemma}
\begin{proof}
    For the first statement, we refer the reader to \citep[Chapter 4, Lemma 1]{Durbin2012Time}. The second statement follows from substituting the value of $K$.
\end{proof}

After having observed $Y = y$, our main interest lies in the conditional distribution of states $X$ given $Y= y$, which we could obtain by applying \Cref{lem:gaussian_conditional}, i.e. where $B = \bdiag (B_{0}, \dots, B_{n})$ and $\Omega = \bdiag \left( \Omega_{0}, \dots, \Omega_{n} \right)$. However, this would require inversion of the $(n+1)p\times(n+1)p$ matrix $\left( B\Sigma B + \Omega \right)$ which becomes numerical infeasible quickly. Instead, we can exploit the sequential structure of the \acrshort{glssm}, which will allow us to perform conditioning on only a single observation at a time. 

To this end, let us denote by $\hat X_{t | s}$ the conditional expectation of $X_{t}$ given a set of observations $y_{:s}$ and by $\Xi_{t | s}$ the conditional covariance matrix of $X_{t}$ given $Y_{:s} = y_{:s}$. Then $$X_{t} | Y_{:s} = y_{:s} \sim \mathcal N \left( \hat X_{t|s}, \Xi_{t|s} \right).$$ For a given $t$, three values of $s$ are of particular interest: If $s = t - 1$ determining this conditional distribution is called a \textbf{prediction problem}, if $s = t$ this is a \textbf{filtering problem} and if $s = n$ a \textbf{smoothing problem}, and we call the distributions we seek the \textbf{predictive, filtering} or \textbf{smoothing distribution} respectively. 
Similarly we define $\hat Y_{t|s} = \E \left( Y_{t} \middle| Y_{:s} = y_{:s} \right)$ to be the conditional expectation of $Y_{t}$ given $Y_{:s}=y_{:s}$, note that $\hat Y_{t|s} = Y_{t}$ if $s \geq t$. Finally, let $\Psi_{t|s} = \cov \left( Y_{t} | Y_{:s} = y_{:s} \right)$ be the conditional covariance matrix of $Y_{t}$ given $Y_{:s} = y_{:s}$. Again $\Psi_{t|s} = 0$ if $s \geq t$. 

These distributions may be obtained efficiently using the celebrated Kalman filter (\Cref{alg:kalman_filter}) and smoother (\Cref{alg:kalman_smoother}) algorithms, which we state here for completeness.

\begin{algorithm}
    \caption{Kalman filter, with runtime $\mathcal O(n(m^{2} + p^{3}))$}
    \label{alg:kalman_filter}
    \begin{algorithmic}
        \Require \gls{glssm} (\Cref{def:glssm}), observations $y_{0}, \dots, y_{n}$.
        \State $A_{-1} \gets I \in \mathbf R^{m\times m}$ \Comment{Identity Matrix}
        \State $u_{-1} \gets \mathbf 0 \in \mathbf R^{m}$ 
        \State $\hat X_{-1|-1} \gets \E X_0$
        \State $\Xi_{0|-1} \gets \Sigma_{0}$
        \State $\ell_{-1} \gets 0$
        \For{$t \gets 0, \dots, n$}
            \State $\hat X_{t| t - 1} \gets A_{t-1} \hat X_{t-1|t-1} + u_{t-1}$ \Comment{prediction}
            \State $\Xi_{t | t - 1} \gets A_{t - 1} \Xi_{t - 1 | t - 1 } A_{t - 1}^{T} + \Sigma_{t}$ 
            \State $\hat Y_{t|t - 1} \gets B_{t}\hat X_{t | t - 1} + v_{t}$
            \State $\Psi_{t|t - 1} \gets B_{t}\Xi_{t | t - 1} B_{t}^T + \Omega_{t}$
            \State $K_t \gets \Xi_{t | t - 1} B_{t}^T \Psi_{t | t - 1} ^{-1}$ \Comment{filtering}
            \State $\hat X_{t | t} \gets \hat X_{t | t - 1} + K_t (y_{t} - \hat Y_{t | t - 1})$
            \State $\Xi_{t| t } \gets \Xi_{t | t - 1} - K_t \Psi_{t| t - 1} K_t^T$
            \State $\ell_{t} \gets \ell_{t - 1} + \frac{p}{2} \log (2\pi) + \frac{1}{2}\log\det \Psi_{t|t -1} + \frac{1}{2} \left( y_{t} - \hat Y_{t | t - 1} \right)^{T} \Psi_{t|t-1}^{-1} \left( y_{t} - \hat Y_{t | t - 1} \right) $ \Comment{NLL}
        \EndFor
    \end{algorithmic}
\end{algorithm}

In \Cref{alg:kalman_filter} every time point $t = 0, \dots, n$ is processed in the same way, with a two-step procedure: first we predict the new observation $Y_{t}$ based on $Y_{:t-1}$. Using the linearity of the system as well as the assumed conditional independence, this is achieved by applying the system dynamics to the current conditional expectation and covariance matrices. After $Y_{t}$ has been observed, we can update the conditional distribution of the states by appealing to \Cref{lem:gaussian_conditional}. Indeed
\todo{fix this eq. in same style as below}
\begin{align*}
p(x_{t}|y_{:t}) &= \int p(x_{t}, x_{t - 1}|y_{:t}) \mathrm d x_{t-1} \\
    &= \int p(x_{t}|x_{t - 1}, y_{t}) p(x_{t - 1} | y_{:(t-1)})
\end{align*}
For a rigorous derivation of the Kalman filter, we refer the reader to \citep[Chapter 4]{Durbin2012Time} or the excellent monograph of \citep{Schneider1986Kalmanfilter}. 

The Kalman filter is very efficient: each loop iteration requires inversion of the $p \times p$ matrix $\Psi_{t | t - 1}$. Assuming this operation dominates the time complexity, e.g. because $m \approx p$, the time complexity of the Kalman filter is $\mathcal O(n\,m^{3})$, a drastic improvement over the naïve $\mathcal O(n^{3}\,m^{3})$, obtained by applying \Cref{lem:gaussian_conditional} to the joint distribution of $(X,Y)$. Similarly, the space complexity of \Cref{alg:kalman_filter} is $\mathcal O \left( n \left( m^{2} + p^{2} \right) \right)$, and grows only linearly in the number of time steps $n$.

Notice that the Kalman filter iteratively calculates the negative log-likelihood $\ell_{t}$
$$\ell_{t} = - \log p(y_{:t}) = - \log \sum_{s = 0}^t \log p(y_{s} | y_{:(s - 1)})$$ 
while filtering --- this is possible because of the dependency structure of the state space model, which makes the increments in $\ell_{t}$ tractable:
$$
Y_{s} | Y_{:(s -1)} \sim \mathcal N \left( \hat Y_{s|s-1}, \Psi_{s|s - 1} \right),
$$
as 
\begin{align*}
    p(y_{s}|y_{:(s - 1)}) &= \int p(y_{s}| x_{s-1}) p(x_{s -1}|y_{:(s-1)})\d x_{s-1}
\end{align*}
for $s = 0, \dots, n$ by the assumed conditional independence. Thus, the Kalman filter enables us to perform \acrshort{mle}, giving us access to $\ell_{n}$.

\todo{historical comment}
Depending on the situation at hand, one of the many variants of the basic algorithm presented in \Cref{alg:kalman_filter} may be used. If the inversion of $\Psi_{t|t-1}$ is numerically unstable, the filtered covariance matrices $\Xi_{t|t}$ may become numerically non-positive definite. In this case, the square root filter and smoother \citep{Morf1975Squareroot} may be used. It is based on Cholesky roots of the involved covariance matrices, ensuring them to be \acrshort{psd}.
\todo{comment on other variants}

When the dimension of observations is much larger than that of the states, $p \gg m$, the information filter \citep{Fraser1969Optimum} can be used. Instead of performing operations on the covariance matrices, i.e. $\Xi_{t|t-1}$ and $\Psi_{t|t-1}$, the information filter operates on their inverses, the precision matrices $\Xi_{t|t - 1}^{-1}$ and $\Psi_{t|t-1}^{-1}$. This makes the filtering step more efficient, as the computationnaly most intensive step is the calculation of $\Psi_{t | t- 1}^{-1}$. However the price one pays is that the prediction step now requires inversion of a $m\times m$ matrix, and as such the computational gains only set in when $p$ is sufficiently large compared to $m$ \todo{cite Assimakis information filter vs. kalman filter paper}. Note that for the models we consider in \Cref{cha:analysis_of_selected_models} this is usually not the case. \todo{check that this really holds}

If the dimensions of the model are so large that calculating the $m\times m$ and $p\times p$ covariance matrices becomes an issue, the simulation based \acrfull{enkf} \cite{Evensen1994Sequential} can be used. Instead of calculating the covariance matrices analytically, the \acrshort{enkf} stores a particle approximation to the Gaussian filtering distribution and iteratively performs a prediction and update step with a particle approximation, similar to the analytical update the Kalman filter performs. Despite being based on linear Gaussian dynamics, the \acrshort{enkf} is successfully employed in many high-dimensional non-linear non Gaussian problems \citep{Katzfuss2016Understanding}. 

For non-linear problems of moderate dimension, i.e. those where we replace the right-hand side of both state (\Cref{eq:glssm_states}) and observation (\Cref{eq:glssm_observations}) equations by non-linear functions, other variants such as the \acrfull{ekf} \cite{Jazwinski1970Stochastic} and the \acrfull{ukf} \cite{Julier1997New} may be used. The \acrshort{ekf} applies the Kalman filter to a linearization of the non-linear system around the current conditional means $\hat X_{t| t-1}$ and $\hat X_{t|t}$. If the systems dynamics are highly non-linear, this approximation can fail. Alternatively, the \acrshort{ukf}, which is based on the unscented transform, directly approximates the predicted means and covariance matrix, by constructing a set of deterministic points that are propagated through the systems dynamics. \todo{more on this}

As the applied context of this thesis is in mathematical epidemiology, we also give a short overview of the use \acrshortpl{glssm} for monitoring of \acrshort{c19}. \todo{more on this}

% history: moon
% epidemiological uses 
\todo{algorithmen konsistent mit gets und =}
\begin{algorithm}
    \caption{Kalman smoother. Note that the Kalman filter already outputs the smoothed last state $\hat X_{n|n}$ and covariance $\Xi_{n|n}$.}
    \label{alg:kalman_smoother}
    \begin{algorithmic}
        \Require \acrshort{glssm} (\Cref{def:glssm}), outputs from Kalman filter (\Cref{alg:kalman_filter})
        \For{$t \gets n - 1, \dots, 0$}
            \State $G_{t} = \Xi_{t|t} A_{t}\Xi_{t+1|t}^{-1}$
            \State $\hat X_{t | n} = \hat X_{t|t} + G_{t} \left( \hat X_{t + 1|n} - \hat X_{t + 1|t} \right)$
            \State $\Xi_{t|n} = \Xi_{t|t} - G_{t} \left( \Xi_{t + 1|t} - \Xi_{t + 1|n} \right)G_{t}^T$
        \EndFor
    \end{algorithmic}
\end{algorithm}

The Kalman smoother (\Cref{alg:kalman_smoother}) computes the marginal distributions $X_{t} | Y$ for $t = 0, \dots, n$. Upon closer inspection, the mean and covariance updates resemble that of the Kalman filter (\Cref{alg:kalman_filter}). This is no coincidence: By the assumed dependence structure, conditional on $X_{t + 1}$, $X_{t}$ is independent of $Y_{s}$, $s > t$. Thus
$$
p(x_{t}|y_{:n}) = \int p(x_{t}, x_{t+1}|y_{:n}) \d x_{t + 1} = \int p(x_{t}|x_{t+ 1}, y_{:t})p(x_{t+1}| y_{:n}) \d x,
$$
which allows to view the update equations of the Kalman smoother as adding \glqq{}observations\grqq{} $\hat X_{t+1 | n}$ to our model. \todo{more clarity in this}

\todo{ffbs erklären}
\begin{algorithm}
    \begin{algorithmic}
        \Require TODO
    \end{algorithmic}
    \caption{Forwards filter, backwards smoother \citep[Proposition 1]{Fruhwirth-Schnatter1994Data}} \label{alg:ffbs}
\end{algorithm}

The modeling capacity of \glspl{glssm} is, however, limited: most interesting phenomena follow neither linear dynamics nor are well modeled by a Gaussian distribution.
Nevertheless, linearization of non-linear dynamics suggests that  \gls{glssm}s may have some use as approximations to these more complicated phenomena, provided they are sufficiently close to Gaussian models, e.g. unimodal and without heavy tails.
We start to move away from linear Gaussian models by allowing observations that are non-Gaussian.
