
\section{Gaussian Linear State Space Models}
\label{sec:linear_gaussian_state_space_models}

\acrfullpl{glssm} are the working horses of most methods used in this thesis because they are analytically tractable and computationally efficient. Indeed for fixed dimension of states $m$ and observations $p$ the runtime of algorithms that we consider in this thesis is $\mathcal O(n)$.

\begin{definition}[\gls{glssm}]
    \label{def:glssm}
    A \gls{glssm} is a state space model where states obey the transition equation
    \begin{align}
        \label{eq:glssm_states}
        X_{t + 1} & = A_{t}X_{t} + u_{t} + \varepsilon_{t + 1} &  & t = 0, \dots, n - 1,
    \end{align}
    and observations obey the observation equation
    \begin{align}
        \label{eq:glssm_observations}
        Y_{t} & = B_{t}X_{t} + v_{t} + \eta_{t} &  & t = 0, \dots, n.
    \end{align}
    Here $A_{t} \in \mathbf{R}^{m \times m}$ and $B_{t} \in \mathbf{R}^{p \times m}$ are matrices that specify the systems dynamics. The \textbf{innovations} $\varepsilon_{t + 1}$ and \textbf{measurement noise} $\eta_{t}$ are independent from one another and from the starting value $X_{0} \sim \mathcal N (\E X_{0}, \Sigma_{0})$. Furthermore, $\varepsilon_{t+1} \sim \mathcal N(0, \Sigma_{t})$ and $\eta_{t}\sim \mathcal N(0, \Omega_{t})$ are centered Gaussian random variables and $u_{t} \in \R^{m}, t = 0, \dots, n - 1$, $v_{t} \in \R^{p}, t = 0, \dots, n$ are deterministic biases.
\end{definition}

The defining feature of a \gls{glssm} is that the joint distribution of $(X,Y)$ is Gaussian, as $(X,Y)$ may be written as an affine combination of the jointly Gaussian $(X_{0}, \varepsilon_{1}, \dots, \varepsilon_{n}, \eta_{0}, \dots, \eta_{n})$ and it is often useful to perform inferences in terms of innovations and measurement noise instead of states, see e.g. \cite[Section 4.5]{Durbin2012Time}.

As the joint distribution of $(X, Y)$ is Gaussian, so are conditional distributions of states given any set of observations.

\begin{lemma}[Gaussian conditional distributions]
    \label{lem:gaussian_conditional}
    Let $(X,Y)$ be jointly Gaussian with distribution $\mathcal N \left( \mu, \Sigma \right)$ where 
    $$
    \mu = \left(\mu_{X}, \mu_{Y}\right)
    $$
    and 
    $$
    \Sigma = \begin{pmatrix}
        \Sigma_{XX} & \Sigma_{XY} \\
        \Sigma_{YX} & \Sigma_{YY}
    \end{pmatrix},
    $$
    for non-singular $\Sigma_{YY}$. 
    
    Then $X|Y = y$ is also a Gaussian distribution with conditional expectation
    $$
    \mu_{X|Y = y} = \E \left( X | Y = y \right) = \mu_{X} + \Sigma_{XY}\Sigma_{YY}^{-1} \left( y - \mu_{Y} \right)
    $$
    and conditional covariance matrix 
    $$
    \Sigma_{X| Y =y} = \cov \left( X | Y = y \right) = \Sigma_{XX} - \Sigma_{XY} \Sigma_{YY}^{-1} \Sigma_{YX}.
    $$
    
    In particular, if $Y = BX + \varepsilon$ for a matrix $B \in \mathbf R^{p\times m}$ and $\ni \varepsilon \sim \mathcal N(0, \Omega)$ for $\Omega \in \R^{p\times p}$ independent of $X$, then, as 
    $\mu_Y = B \mu_{X}$, $\Sigma_{XY} = \Sigma_{YX}^T = \Sigma_{XX}B^{T}$ and $\Sigma_{YY} = B \Sigma_{XX} B^{T} + \Omega$, we have
    $$
        \mu_{X|Y = y} = \mu_{X} + K (y - B \mu_{X})
    $$
    and 
    $$
    \Sigma_{X|Y = y} = \Sigma_{XX} - K \Sigma_{YY} K^{T} = \left( I  -  KB \right) \Sigma_{XX}
    $$
    with $K = \Sigma_{XX}B^{T} \left( B \Sigma_{XX}B^{T} + \Omega \right)^{-1}$.
\end{lemma}
\begin{proof}
    For the first statement, we refer the reader to \cite[Chapter 4, Lemma 1]{Durbin2012Time}. The second statement follows from substituting the value of $K$.

\end{proof}

Let us denote by $\hat X_{t | s}$ the conditional expectation of $X_{t}$ given a set of observations $Y_{:s}$ and by $\Xi_{t | s}$ the conditional covariance matrix of $X_{t}$ given $Y_{:s}$. Then $X_{t} | Y_{:s} \sim \mathcal N \left( \hat X_{t|s}, \Xi_{t|s} \right)$. For a given $t$, three values of $s$ are of particular interest: If $s = t - 1$ determining this conditional distribution is called a \textbf{prediction problem}, if $s = t$ this is a \textbf{filtering problem} and if $s = n$ a \textbf{smoothing problem}, and we call the distributions we seek the \textbf{predictive, filtering} or \textbf{smoothing distribution} respectively. 
Similarly we define $\hat Y_{t|s} = \E \left( Y_{t} \middle| Y_{:s} \right)$ to be the conditional expectation of $Y_{t}$ given $Y_{:s}$, note that $\hat Y_{t|s} = Y_{t}$ if $s \geq t$. Finally, let $\Psi_{t|s} = \cov \left( Y_{t} | Y_{:s} \right)$ be the conditional covariance matrix of $Y_{t}$ given $Y_{:s}$. Again $\Psi_{t|s} = 0$ if $s \geq t$. 

These distributions may be obtained efficiently using the celebrated Kalman filter (\Cref{alg:kalman_filter}) and smoother (\Cref{alg:kalman_smoother}) algorithms, which we state here for completeness.
\todo{KF + KS literature review, historical comment}

\begin{algorithm}
    \caption{Kalman filter, with runtime $\mathcal O(n(m^{2} + p^{3}))$}
    \label{alg:kalman_filter}
    \begin{algorithmic}
        \Require \gls{glssm} (\Cref{def:glssm}), observations $Y_{0}, \dots, Y_{n}$.
        \State $A_{-1} \gets I \in \mathbf R^{m\times m}$ \Comment{Identity Matrix}
        \State $u_{-1} \gets \mathbf 0 \in \mathbf R^{m}$ 
        \State $\hat X_{-1|-1} \gets \E X_0$
        \State $\Xi_{0|-1} \gets \Sigma_{0}$
        \For{$t \gets 0, \dots, n$}
            \State $\hat X_{t| t - 1} \gets A_{t-1} \hat X_{t-1|t-1} + u_{t-1}$ \Comment{prediction}
            \State $\Xi_{t | t - 1} \gets A_{t - 1} \Xi_{t - 1 | t - 1 } A_{t - 1}^{T} + \Sigma_{t}$ 
            \State $\hat Y_{t|t - 1} \gets B_{t}\hat X_{t | t - 1} + v_{t}$
            \State $\Psi_{t|t - 1} \gets B_{t}\Xi_{t | t - 1} B_{t}^T + \Omega_{t}$
            \State $K_t \gets \Xi_{t | t - 1} B_{t}^T \Psi_{t | t - 1} ^{-1}$ \Comment{filtering}
            \State $\hat X_{t | t} \gets \hat X_{t | t - 1} + K_t (Y_{t} - \hat Y_{t | t - 1})$
            \State $\Xi_{t| t } \gets \Xi_{t | t - 1} - K_t \Psi_{t| t - 1} K_t^T$
        \EndFor
    \end{algorithmic}
\end{algorithm}
In \Cref{alg:kalman_filter} every time point $t = 0, \dots, n$ is processed in the same way, with a two-step procedure: first we predict the new observation $Y_{t}$ based on $Y_{:t-1}$. Using the linearity of the system as well as the assumed conditional independence, this is achieved by applying the system dynamics to the current conditional expectation and covariance matrices. After $Y_{t}$ has been observed, we can update the conditional distribution of the states by appealing to \Cref{lem:gaussian_conditional}.
For a rigorous derivation of the Kalman filter, we refer the reader to \cite[Chapter 4]{Durbin2012Time} or the excellent monograph of \citeauthor{Schneider1986Kalmanfilter}, \cite{Schneider1986Kalmanfilter}. 

The Kalman filter is very efficient: each loop iteration requires inversion of the $p \times p$ matrix $\Psi_{t | t - 1}$. Assuming this operation dominates the time complexity, e.g. because $m \approx p$, the time complexity of the Kalman filter is $\mathcal O(n\,m^{3})$, a drastic improvement over the na√Øve $\mathcal O(n^{3}\,m^{3})$, obtained by applying \Cref{lem:gaussian_conditional} to the joint distribution of $Y$. Similarly, the space complexity of \Cref{alg:kalman_filter} is $\mathcal O \left( n \left( m^{2} + p^{2} \right) \right)$, and grows only linearly in the number of time steps $n$.

Depending on the situation at hand, one of the many variants of the basic algorithm presented in \Cref{alg:kalman_filter} may be used. If the inversion of $\Psi_{t|t-1}$ is numerically unstable, the filtered covariance matrices $\Xi_{t|t}$ may become numerically non-positive definite. In this case, the square root filter and smoother \cite{Morf1975Squareroot} may be used. It is based on Cholesky roots of the involved covariance matrices, ensuring them to be positive-semi definite. 
\todo{comment on information filter?}
%If ... one may use the information filter, that operates on the precision matrices instead of covariance matrices \todo{write some more about them here}.
\begin{algorithm}
    \caption{Kalman smoother}
    \label{alg:kalman_smoother}
    \begin{algorithmic}
        \Require todo
    \end{algorithmic}
\end{algorithm}

Notice that the Kalman filter calculates the likelihood $p(y)$ while filtering --- this is possible because of the dependency structure of the state space model --- this makes inference via maximum likelihood possible in \gls{glssm}s.

To ensure numerical stability in these algorithms, the square root filter and smoother \cite{Morf1975Squareroot} may be used, see also \cite{Schneider1986Kalmanfilter} for an accessible introduction to it and other variants.

The Kalman smoother computes the marginal distributions $X_{t} | Y$ for $t = 0, \dots, n-1$ and, owing to the Markov structure of the states, these are enough to specify the joint distribution $X|Y$, allowing to simulate from it.

\begin{algorithm}
    \begin{algorithmic}
        \Require TODO
    \end{algorithmic}
    \caption{Forwards filter, backwards smoother \cite[Proposition 1]{Fruhwirth-Schnatter1994Data}}
    \label{alg:ffbs}
\end{algorithm}

The modeling capacity of \glspl{glssm} is, however, limited: most interesting phenomena follow neither linear dynamics nor are well modeled by a Gaussian distribution.
Nevertheless, linearization of non-linear dynamics suggests that  \gls{glssm}s may have some use as approximations to these more complicated phenomena, provided they are sufficiently close to Gaussian models, e.g. unimodal and without heavy tails.
We start to move away from linear Gaussian models by allowing observations that are non-Gaussian.
