\chapter{Introduction}

% COVID-19 need for insight into spread, morbidity and mortality
The \gls{c19} pandemic put the scientific community to the test: how infectious, morbid and mortal was the dieseas? when and for how long did infected people become infectious? how effective are the countermeasures taken? how safe and effective are the vaccines that were developed at an unprecedented speed? Some of these questions, e.g. those about the epidemiology of \acrshort{c19}, are confined to well-established areas of research, while others, e.g. those about the efficacy of countermeasures, required collaboration across a wide range of disciplines: from infectious disease epidemiology, mathematical and statistical modeling, social and communication science to non-scientific actors such as legislators, journalists and politicians.

% much data, need for knowledge 
Although there is still a lot of scientific and societal follow-up work to be done, given the magnitude of this challenge, it is astonishing how well science and society as a whole have handled the pandemic. A key factor in this accomplishment is the large-scale availability of data surrounding the pandemic. In many countries, including Germany, data on reported cases, deaths, vaccinations and deaths were published daily by the respective national health authorities, i.e. the \gls{rki} \citep{RobertKoch-Institut2022SARSCoV2,RobertKoch-Institut2021COVID19Hospitalisierungen} in Germany. Additionally, mobility data from mobile communications providers allowed researchers to relate human movement to the spread of \acrshort{c19} \citep{Kraemer2020Effect,Schlosser2020COVID19}. 
As each day the news reported on the number of newly reported cases and deaths, numerous dashboards with analyses of \acrshort{c19} data were made available and an abundant number of scientific works was created, effectively communicating with the public, whose cooperation with countermeasures was critical, became more and more important. To disseminate insights to the public, we need to understand and communicate to them the underlying dynamics of an epidemic.

% pandemic is inherently a random phenomenon
% while large scale dynamics may be well approx. by det. systems,
% small scale require precise modelling
An epidemic outbreak is inherently a random phenomenon \citep{Diekmann2013Mathematical}. Who becomes infected, for how long they stay infectious, whom they meet while they are infectious and whom they finally infect are all aspects that depend to a certain degree on chance. If one is interested in large-scale phenomena, e.g. effects of immunization in a large population, one may get away with a deterministic model \citep{Britton2019Stochastic}, such as the classical S(E)IR model \citep{Kermack1927Contribution} or variants of it. However, as soon as one is interested in more detailed phenomena, as we are in this thesis, stochastic and statistical modeling becomes essential. 

An epidemic outbreak is also inherently a local phenomenon, especially in the early phase of the epidemic. In the extreme case, there is only a single infectious person and, for the most part, their potential infectees will belong to the same spatial region as the infector. Therefore, we should incorporate this locality into our models. To fit such models to data, the data has to include spatial information. Luckily, the case and death data are available at the subnational level in most countries. In Germany, it is even available at the county (Landkreis, NUTS3) level \citep{RobertKoch-Institut2022SARSCoV2}. 

% statistics provide insight
As statisticians, having access to such a large amount of data is both a blessing and a curse. While more, and ideally better, data allows us to formulate and answer more relevant questions, the models we create to accommodate these data become more and more intricate. Intricate models require more care in modeling, fitting and interpretation, as more things can go wrong along the way. Thus we will tread carefully. As we incorporate more detailed effects into our models, fitting the models to data becomes difficult to practically impossible using established techniques. While there are some remedies for this curse of dimensionality, e.g. exploiting as much available structure as possible, there is an ongoing need for new procedures enabling inference in these settings. Additionally, we need mathematical as well as practical insight into the performance of these procedures to make informed decisions in applied settings: which methods should we prefer under which circumstances?

% need for
%% good models
%% fitting techniques
%% theoretical insight into these techniques
%% analysis of selected models
These considerations set the stage for this thesis. Driven by the need for good statistical models that allow us to answer urgent questions in infectious disease epidemiology, with \acrshort{c19} as a driving example, we will start with an analysis of what is required of these sought-after models. We will define and discuss the role of several epidemiological indicators, i.e. quantities that have an interpretation related to the epidemic. It turns out that we will usually be interested in quantifying the speed at which the epidemic proliferates, and we discuss several popular indicators that measure this speed. A useful statistical analysis should provide interpretable insight into the problem at hand, so we focus on how straightforward this interpretation is, giving recommendations on when to use which indicator. To estimate these indicators from data, we have to create statistical models that include them. Before we do so, we will create a list of desiderata from the context of \acrshort{c19}. 

Once we have a clear view of the epidemiological problems at hand, we show that many of the desiderata can be covered by using \acrshortpl{ssm}, a flexible framework for modeling non-stationary time series. Unfortunately, we will require that these \acrshortpl{ssm} include integer-valued, non-Gaussian, observations, which makes fitting the models to data analytically impossible and numerically difficult, as one is essentially faced with a high-dimensional non-Gaussian Bayesian inference problem. Instead, inference will be based on simulation methods, most notably importance sampling. To apply these methods, the practitioner has some flexibility in the so-called proposal distribution, a tractable approximation to the Bayesian posterior. Different disciplines have developed simulation-based techniques that allow the user to choose optimal proposals, where optimality is based on different performance criteria for different methods. In this thesis, we focus on two methods: the \gls{cem} and \gls{eis}. In the literature, a comparison between these two methods is missing: there are neither mathematical nor empirical results comparing the two. We fill this gap by first proving central limit theorems for both methods, allowing for a theoretical comparison. Additionally, we also provide simulation studies comparing the methods on instructive univariate and \acrshortpl{ssm} examples. To this end, we also develop a new algorithm that allows the \acrshort{cem} to be applied to \glspl{ssm}. 

Finally, we demonstrate how to solve a selection of infectious disease epidemiology problems using the mathematical insights we gained. These examples focus on the \acrshort{c19} epidemic in Germany and illustrate the modeling, computational and applied aspects of this thesis. 

\todo{add some more refs?}