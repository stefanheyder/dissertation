[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Thesis",
    "section": "",
    "text": "This repository contains the code and latex files to build my PhD thesis. The code heavily relies on my package isssm, extending it to non-standard use-cases.\nYou can find the most recent version of my thesis under the following link: https://stefanheyder.github.io/dissertation/thesis.pdf.\nThe code is developed in Jupyter notebooks and organized by the chapters and sections of the thesis. Simulations and analyses are run with Python, figures are created with R, using ggplot and tikzDevice to create tikz graphics.",
    "crumbs": [
      "Thesis"
    ]
  },
  {
    "objectID": "index.html#readme",
    "href": "index.html#readme",
    "title": "Thesis",
    "section": "",
    "text": "This repository contains the code and latex files to build my PhD thesis. The code heavily relies on my package isssm, extending it to non-standard use-cases.\nYou can find the most recent version of my thesis under the following link: https://stefanheyder.github.io/dissertation/thesis.pdf.\nThe code is developed in Jupyter notebooks and organized by the chapters and sections of the thesis. Simulations and analyses are run with Python, figures are created with R, using ggplot and tikzDevice to create tikz graphics.",
    "crumbs": [
      "Thesis"
    ]
  },
  {
    "objectID": "4 Models/4.1 Removing reporting delays and weekday effects/data.html",
    "href": "4 Models/4.1 Removing reporting delays and weekday effects/data.html",
    "title": "Data",
    "section": "",
    "text": "library(here)\nsource(here(\"code/setup.R\"))\nlibrary(latex2exp)\n\nhere() starts at /Users/stefan/workspace/work/phd/thesis\n\n\nAttache Paket: ‘zoo’\n\n\nDie folgenden Objekte sind maskiert von ‘package:base’:\n\n    as.Date, as.Date.numeric\n\n\n\nAttache Paket: ‘reshape2’\n\n\nDas folgende Objekt ist maskiert ‘package:tidyr’:\n\n    smiths\n\n\n\n\n\nRKI data - reporting triangle\nWe use the reporting triangle for the number of cases, i.e. on any day \\(t\\) the number of cases \\[I_{s,t}\\] that are reported associated with date \\(s &lt; t\\).\nWe begin our analysis on April 1st 2020, when data have become stable enough to warrant an analysis.\nMost delays are less than 4 days, so we consider only those delays, grouping all later delays into a single fifth or larger day.\nWe perform the same pre-processing as described in chapter 2.3.\n\nrep_tri &lt;- read_csv(here(\"data/processed/RKI_4day_rt.csv\"))\n\n\nrep_tri %&gt;%\n    pivot_longer(2:5, names_to = \"tau\", values_to = \"i\") %&gt;%\n    group_by(county_date) %&gt;%\n    mutate(p_hat = i / sum(i)) %&gt;%\n    ungroup() %&gt;%\n    mutate(weekday = wday(county_date, label = TRUE, week_start = 1)) %&gt;%\n    filter(year(county_date) == 2020) %&gt;%\n    ggplot(aes(x = weekday, y = p_hat * 100, fill = tau)) +\n    geom_boxplot() +\n    labs(x = \"\", fill = \"$\\\\tau$\", y = \"$\\\\hat p_{t, \\\\tau}$ [\\\\%]\")\n\nggsave_tikz(here(\"tikz/weekday_effect_delays.tex\"), height = 3)\n\npdf: 2",
    "crumbs": [
      "4 Models",
      "4.1 Removing reporting delays and weekday effects",
      "Data"
    ]
  },
  {
    "objectID": "4 Models/4.1 Removing reporting delays and weekday effects/prediction_model.html",
    "href": "4 Models/4.1 Removing reporting delays and weekday effects/prediction_model.html",
    "title": "Application 3: Predictions",
    "section": "",
    "text": "from ssm4epi.models.reporting_delays import (\n    _model,\n    to_log_probs,\n    n_iterations,\n    N_meis,\n    N_mle,\n    N_posterior,\n    key,\n    percentiles_of_interest,\n    account_for_nans,\n)\n\nimport jax\nfrom pyprojroot.here import here\n\njax.config.update(\"jax_enable_x64\", True)\n\nimport pandas as pd\nimport jax.random as jrn\nfrom jax import numpy as jnp, vmap\nfrom isssm.laplace_approximation import laplace_approximation as LA\nfrom isssm.modified_efficient_importance_sampling import (\n    modified_efficient_importance_sampling as MEIS,\n)\nfrom isssm.estimation import mle_pgssm, initial_theta\nfrom isssm.importance_sampling import (\n    pgssm_importance_sampling,\n    ess_pct,\n    mc_integration,\n    prediction_percentiles,\n    normalize_weights,\n    prediction,\n)\nfrom isssm.kalman import state_mode\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nmpl.rcParams[\"figure.figsize\"] = (20, 6)\n\n\ndf = pd.read_csv(here() / \"data/processed/RKI_4day_rt.csv\")\ndf[\"date\"] = pd.to_datetime(df[\"county_date\"])\nN_prediction = int(1e5)\n\n\nfrom isssm.typing import PGSSM\n\n\ndef f_pred(x, s, y):\n    return y[-7:].sum()\n\n\ndef predict_next_week(date_start, date_end, theta, key):\n    # indices of df that correspond to the dates\n    idx_start = df[df[\"date\"] == date_start].index[0]\n    idx_end = df[df[\"date\"] == date_end].index[0]\n\n    aux = (idx_end - idx_start + 1 + 7, 4)\n\n    dates = pd.to_datetime(df.iloc[idx_start : idx_end + 1][\"date\"])\n\n    y = jnp.asarray(df.iloc[idx_start : idx_end + 1, 1:5], dtype=jnp.float64)\n    y_nan = jnp.concatenate((y, jnp.full((7, 4), jnp.nan)), axis=0)\n    missing_inds = jnp.isnan(y_nan)\n    _, y_miss = account_for_nans(_model(theta, aux), y_nan, missing_inds)\n    _model_miss = lambda theta, aux: account_for_nans(\n        _model(theta, aux), y_nan, missing_inds\n    )[0]\n\n    theta0 = initial_theta(y_miss, _model_miss, theta, aux, n_iterations).x\n    key, subkey = jrn.split(key)\n    theta_hat = mle_pgssm(\n        y_miss, _model_miss, theta0, aux, n_iterations, N_mle, subkey\n    ).x\n\n    fitted_model = _model_miss(theta_hat, aux)\n\n    proposal_la, _ = LA(y_miss, fitted_model, n_iterations)\n\n    key, subkey = jrn.split(key)\n    proposal_meis, _ = MEIS(\n        y_miss,\n        fitted_model,\n        proposal_la.z,\n        proposal_la.Omega,\n        n_iterations,\n        N_meis,\n        subkey,\n    )\n\n    prediction_model = _model(theta_hat, aux)\n    old_dist = prediction_model.dist\n    prediction_model = PGSSM(\n        *prediction_model[:-2],\n        lambda s, xi: old_dist(s, xi[..., 1:]),\n        prediction_model.xi\n    )\n\n    key, subkey = jrn.split(key)\n    mean, sd, quantiles = prediction(\n        f_pred,\n        y_miss,\n        proposal_meis,\n        fitted_model,\n        N_prediction,\n        subkey,\n        percentiles_of_interest,\n        prediction_model=prediction_model,\n    )\n\n    # only return last week\n    return (dates, y, theta_hat), jnp.hstack([mean[None], sd[None], quantiles[:]])\n\n\n# string to datetime\nfirst_date_end = pd.to_datetime(\"2021-03-08\")\nfirst_date_start = first_date_end - pd.DateOffset(days=5 * 7 - 1)\n\nn_weeks = 10\n\nkey = jrn.PRNGKey(5234524234)\n\n# from showcase model\ntheta_manual = jnp.array(\n    [\n        -8.422721513295569,\n        -7.476399652760843,\n        -4.225735089851275,\n        -3.988944142986405,\n        -0.41375665515244603,\n    ]\n)\n\nkey, sk_predict = jrn.split(key)\n\ndate_end = first_date_end\ndate_start = first_date_start\ntheta = theta_manual\n\nresults = []\nfor i in range(n_weeks):\n    result = predict_next_week(date_start, date_end, theta, sk_predict)\n\n    date_start = date_start + pd.DateOffset(days=7)\n    date_end = date_end + pd.DateOffset(days=7)\n    (_, _, theta), _ = result\n\n    results.append(result)\n\n\ndef result_to_pi(result):\n    (dates, y, theta_hat), (mean, sd, *quantiles) = result\n\n    return jnp.array([quantiles[1], mean, quantiles[-2]])\n\n\npis = jnp.array([result_to_pi(result) for result in results])\n\n\nidx_start = df[df[\"date\"] == first_date_start].index[0]\nidx_end = df[df[\"date\"] == date_end].index[0]\n\ndates = pd.to_datetime(df.iloc[idx_start : idx_end + 1][\"date\"])\n\ny = jnp.asarray(df.iloc[idx_start : idx_end + 1, 1:5], dtype=jnp.float64)\n\nplt.plot(y.sum(axis=-1).reshape((-1, 7)).sum(axis=1))\nplt.plot(jnp.arange(5, 15), pis[:, 1])\nplt.plot(jnp.arange(5, 15), pis[:, 0], color=\"grey\", linestyle=\"--\")\nplt.plot(jnp.arange(5, 15), pis[:, 2], color=\"grey\", linestyle=\"--\")",
    "crumbs": [
      "4 Models",
      "4.1 Removing reporting delays and weekday effects",
      "Application 3: Predictions"
    ]
  },
  {
    "objectID": "4 Models/4.1 Removing reporting delays and weekday effects/model.html",
    "href": "4 Models/4.1 Removing reporting delays and weekday effects/model.html",
    "title": "Model for removing weekday effects and reporting delays",
    "section": "",
    "text": "mpl.rcParams[“figure.figsize”] = (20, 6)",
    "crumbs": [
      "4 Models",
      "4.1 Removing reporting delays and weekday effects",
      "Model for removing weekday effects and reporting delays"
    ]
  },
  {
    "objectID": "4 Models/4.1 Removing reporting delays and weekday effects/model.html#tuning-parameter-for-methods",
    "href": "4 Models/4.1 Removing reporting delays and weekday effects/model.html#tuning-parameter-for-methods",
    "title": "Model for removing weekday effects and reporting delays",
    "section": "Tuning parameter for methods",
    "text": "Tuning parameter for methods",
    "crumbs": [
      "4 Models",
      "4.1 Removing reporting delays and weekday effects",
      "Model for removing weekday effects and reporting delays"
    ]
  },
  {
    "objectID": "4 Models/4.1 Removing reporting delays and weekday effects/model.html#states",
    "href": "4 Models/4.1 Removing reporting delays and weekday effects/model.html#states",
    "title": "Model for removing weekday effects and reporting delays",
    "section": "States",
    "text": "States\nWe consider states \\(X_t = \\left(\\log I_{t}, W_t, \\dots, W_{t - 5}, q_{1,t}, q_{2,t}, q_{3,t}\\right)\\) with\n\n\\(\\log I_{t + 1} = \\log I_{t} + \\log \\rho_{t}\\)\n\\(\\log \\rho_{t + 1} = \\log \\rho_{t} + \\varepsilon^{\\rho}_{t + 1}\\)\n\\(M_{t + 1} = -\\frac{1}{2}\\sigma^{2}_{2} + \\varepsilon^M_{t +1 } \\sim \\mathcal N (-\\frac{1}{2} \\sigma^{2}_M, \\sigma^{2}_M)\\), “muck” term (s.t. \\(\\mathbf E M_{t + 1} = 1\\))\n\\(W_{t + 1} = - \\sum_{s = 0}^5 W_{t - s} + \\varepsilon^W_{t + 1}\\) , \\(\\varepsilon^W_{t + 1} \\sim \\mathcal N(0, \\sigma^2_W)\\)\n\\(q_{t,\\tau} = q_{t,\\tau} + \\varepsilon_{t + 1}^{q,\\tau}\\); \\(\\tau = 1,2,3\\)\n\nObservations are the breakdown of incidences with Meldedatum \\(t\\) into the delays \\(\\tau = 1, \\dots\\). Note that on date \\(t\\), \\(Y_t\\) is only partially observed: \\[\nY^i_{t} \\sim \\operatorname{Pois} \\left( p_{t,\\tau}\\exp \\left( W_{t} + \\log I_{t} + M_{t}\\right)\\right) = \\operatorname{Pois} \\left( p_{t,\\tau} \\exp(W_{t} M_{t}) I_{t}\\right),\n\\] for \\(\\tau = 1,\\dots, 4\\), where the parametrization is such that the first parameter is the mean, the second the overdispersion parameter.\nHere \\[\n    %p_{t,\\tau} = \\frac{\\exp \\left( q_{t,\\tau} \\right)}{\\sum_{j = 1}^{4}\\exp \\left( q_{j,t} \\right)}.\n    p_{t,\\tau} = \\frac{\\exp \\left( q_{t,\\tau} \\right)}{1 + \\sum_{j = 1}^{3}\\exp \\left( q_{j,t} \\right)},\n\\] for \\(\\tau = 1, 2, 3\\) and \\[\n    p_{4, t} = \\frac{1}{1 + \\sum_{j = 1}^{3}\\exp \\left( q_{j,t} \\right)},\n\\] similar as in Multinomial logistic regression.\nWe let \\[\n    \\begin{align*}\n    S_{t} &= B_{t}X_{t} \\\\\n    &= \\left( \\log I_{t} + \\log W_{t} + M_{t}, q_{1,t}, q_{2,t}, q_{3, t}\\right)\n    \\end{align*}\n\\]\nNow \\[\n    Y_{t}^\\tau | S_{t} \\sim \\operatorname{Pois} \\left( p_{t,\\tau}\\exp \\left( \\log I_{t} + \\log W_{t} + M_{t} \\right)\\right).\n\\] Note the following:\n\n\\(Y_t^\\tau\\) depends on all signals, not just on one, however the observations are still conditionally independent\n\nThe parameter \\(\\theta\\) of this model is \\[\n    \\theta = \\left( \\log \\sigma^{2}_{\\log \\rho}, \\log \\sigma^{2}_{W}, \\log \\sigma^{2}_q , \\log \\sigma^{2}_{M}\\right).\n\\]\nThe isssm.laplace_approximation module assumes that \\(y_{t,i}\\) only depends on \\(\\theta_{t,i}\\) which is not the case here. To fix this, we monkey-patch both the LA and MEIS.\nAdditionaly, we have to account for missing values in both methods. If \\(y_t\\) is missing, we transform the model to be \\[\n    y_{t} | s_{t} ~ \\delta_{s_{t}}\n\\] and \\(B_t x_t = s_t = 0\\). Then \\(\\log p(y_{t} | s_{t}) = 0\\) for all \\(s_t\\), with gradient \\(0_{p}\\) and Hessian \\(0_{p \\times p}\\). For any initial value of \\(s_t\\) the LA observation is \\[\n    z_{t} =  s_{t} + \\underbrace{\\ddot p(y_{t}|s_{t})}_{= 0_{p \\times p}}~ ^{\\dagger} \\underbrace{\\dot p(y_{t}| s_{t})}_{= 0_{p}} = s_{t}\n\\] with covariance matrix \\[\n    \\Omega_{t} = \\ddot p(y_{t} | s_{t})^{\\dagger} = 0_{p\\times p}.\n\\] which keeps \\(s_t\\) constant. Here \\(\\dagger\\) indicates the Moore-Penrose generalized inverse. Thus setting \\(s_{t} = 0\\), this results in a GLSSM where \\(y_t\\) is as if it is missing.\nFor EIS, the pseudo-observations in the WLS are \\(\\log p(y_{t} | s_{t}^i) = 0\\), by a similar argument. Thus the estimates are \\(\\hat\\beta_{t} = 0_{2p + 1}\\), and, taking pseudo-inverses again, we arrive at the same conclusion.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndf = pd.read_csv(here() / \"data/processed/RKI_4day_rt.csv\")\ni_start = 0\nnp1 = 150\ndata_selected = df.iloc[i_start : i_start + np1, 1:]\ndates = pd.to_datetime(df.iloc[i_start : i_start + np1, 0])\ny = jnp.asarray(data_selected.to_numpy())\n\nplt.plot(dates, y)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom isssm.importance_sampling import pgssm_importance_sampling, ess_pct\nfrom jax import random as jrn\nfrom ssm4epi.models.util import visualize_pgssm\n\n\ntheta_manual = jnp.log(\n    # s2_log_rho, s2_W, s2_q, s2_D\n    jnp.array([0.001**2, 0.1**2, 0.5**2, 0.01**2, 0.1**2])\n)\n\n# allow variance to be larger as mean\naux = (np1, 4)\npgssm = _model(theta_manual, aux)\n\nvisualize_pgssm(pgssm)",
    "crumbs": [
      "4 Models",
      "4.1 Removing reporting delays and weekday effects",
      "Model for removing weekday effects and reporting delays"
    ]
  },
  {
    "objectID": "4 Models/4.1 Removing reporting delays and weekday effects/model.html#monkey-patching",
    "href": "4 Models/4.1 Removing reporting delays and weekday effects/model.html#monkey-patching",
    "title": "Model for removing weekday effects and reporting delays",
    "section": "Monkey patching",
    "text": "Monkey patching\n\nfrom isssm.laplace_approximation import laplace_approximation\nfrom isssm.modified_efficient_importance_sampling import (\n    modified_efficient_importance_sampling as MEIS,\n)\n\nkey = jrn.PRNGKey(123423423)\n\n\nfrom isssm.estimation import mle_pgssm, initial_theta\n\n\ninitial_result = initial_theta(y, _model, theta_manual, aux, 20)\ntheta0 = initial_result.x\ninitial_result\n\n  message: Optimization terminated successfully.\n  success: True\n   status: 0\n      fun: 5.692800563209951\n        x: [-8.423e+00 -7.476e+00 -4.235e+00 -3.989e+00 -4.155e-01]\n      nit: 54\n      jac: [ 2.859e-08  9.477e-08 -5.204e-07 -2.981e-07  3.293e-07]\n hess_inv: [[ 2.131e+02  9.608e+00 ... -2.118e+01  1.941e-01]\n            [ 9.608e+00  1.806e+02 ... -1.266e+01  4.999e-02]\n            ...\n            [-2.118e+01 -1.266e+01 ...  1.930e+01  1.884e-01]\n            [ 1.941e-01  4.999e-02 ...  1.884e-01  8.456e+00]]\n     nfev: 660\n     njev: 60\n\n\n\n# key, subkey = jrn.split(key)\n# mle_result = mle_pgssm(y, _model, theta0, aux, 20, 1000, subkey)\n# theta_hat = mle_result.x\n# mle_result\n\ntheta_hat = theta0\n\n\ns_manual = jnp.exp(theta_manual / 2)\ns_0 = jnp.exp(theta0 / 2)\n\nk = theta_manual.size\nplt.scatter(jnp.arange(k) - 0.2, s_manual, label=\"Manual\")\nplt.scatter(jnp.arange(k), s_0, label=\"Initial\")\ns_mle = jnp.exp(theta_hat / 2)\nplt.scatter(jnp.arange(k) + 0.2, s_mle, label=\"MLE\")\nplt.xticks(jnp.arange(k), [\"$\\\\log \\\\rho$\", \"$W$\", \"$q$\", \"D\", \"Wq\"])\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nfitted_model = _model(theta0, aux)\n\nproposal_la, _ = laplace_approximation(y, fitted_model, 100)\nkey, subkey = jrn.split(key)\nproposal_meis, _ = MEIS(\n    y, fitted_model, proposal_la.z, proposal_la.Omega, 10, int(1e4), subkey\n)\n\nkey, subkey = jrn.split(key)\nsamples_meis, log_weights_meis = pgssm_importance_sampling(\n    y, fitted_model, proposal_meis.z, proposal_meis.Omega, 10000, subkey\n)\nkey, subkey = jrn.split(key)\nsamples_la, log_weights_la = pgssm_importance_sampling(\n    y, fitted_model, proposal_la.z, proposal_la.Omega, 10000, subkey\n)\ness_pct(log_weights_la), ess_pct(log_weights_meis)\n\n\nfrom isssm.importance_sampling import normalize_weights, mc_integration\nfrom isssm.typing import GLSSM\nfrom isssm.kalman import kalman, smoother, state_mode\nfrom isssm.laplace_approximation import posterior_mode\nfrom isssm.util import mm_time\n\n\n\n\n\n\n\n\n\nsignal_la = posterior_mode(proposal_la)\n\nstate_modes_meis = vmap(state_mode, (None, 0))(fitted_model, samples_meis)\nx_smooth = mc_integration(state_modes_meis, log_weights_meis)\n\nx_smooth_la = state_mode(fitted_model, signal_la)\n\n# plt.plot(jnp.exp(x_smooth[:, 12]), label=\"smoothed\")\n# plt.plot(jnp.exp(x_smooth[:, 2:8].sum(axis=1)), label=\"smoothed\")\n# plt.plot(jnp.exp(x_smooth[:, 0]), label=\"smoothed\")\n# plt.plot(y.sum(axis=1), label=\"Y\")\n# plt.plot(jnp.exp(x_smooth[:, 1]), label=\"smoothed\")\n\nI_smooth = jnp.exp(x_smooth[:, 0])\nI_smooth_LA = jnp.exp(x_smooth_la[:, 0])\n\nrho_smooth = jnp.exp(x_smooth[:, 1])\nrho_smooth_LA = jnp.exp(x_smooth_la[:, 1])\n\nD_smooth = jnp.exp(x_smooth[:, 2])\nD_smooth_LA = jnp.exp(x_smooth_la[:, 2])\n\nW_smooth = jnp.exp(x_smooth[:, 3])\nW_smooth_LA = jnp.exp(x_smooth_la[:, 3])\n\nlog_ratios = x_smooth[:, 9:12]\nlog_probs = to_log_probs(log_ratios)\n\nfig, axs = plt.subplots(4, 2, figsize=(10, 10))\n\naxs = axs.flatten()\nfig.tight_layout()\n\naxs[0].set_title(\"incidences\")\naxs[0].plot(dates, I_smooth, label=\"$I_t$\")\naxs[0].plot(dates, I_smooth_LA, label=\"$I_t$ LA\")\naxs[0].plot(dates, y.sum(axis=1), label=\"$Y_t$\")\naxs[0].legend()\n\naxs[1].set_title(\"growth factor\")\naxs[1].plot(dates, rho_smooth, label=\"$\\\\log \\\\rho_t$\")\naxs[1].plot(dates, rho_smooth_LA, label=\"$\\\\log \\\\rho_t$ LA\")\n\naxs[2].set_title(\"weekday effect\")\naxs[2].plot(dates, W_smooth, label=\"$W_t$\")\naxs[2].plot(dates, W_smooth_LA, label=\"$W_t$ LA\")\n\naxs[3].set_title(\"delay probabilities\")\n\n\naxs[3].plot(dates, jnp.exp(log_probs[:, 0]), label=\"$p_{t, 1}$\")\naxs[3].plot(dates, jnp.exp(log_probs[:, 1]), label=\"$p_{t, 2}$\")\naxs[3].plot(dates, jnp.exp(log_probs[:, 2]), label=\"$p_{t, 3}$\")\naxs[3].plot(dates, jnp.exp(log_probs[:, 3]), label=\"$p_{t, 4}$\")\naxs[3].plot(dates, jnp.exp(log_probs).sum(axis=1), label=\"total p\")\naxs[3].legend()\n\naxs[4].set_title(\"Log ratios\")\naxs[4].plot(dates, log_ratios[:, 0], label=\"$q_{t, 1}$\")\naxs[4].plot(dates, log_ratios[:, 1], label=\"$q_{t, 2}$\")\naxs[4].plot(dates, log_ratios[:, 2], label=\"$q_{t, 3}$\")\nfor d in dates[::7]:\n    axs[4].axvline(d, color=\"black\", alpha=0.2)\n\naxs[5].set_title(\"Dirt\")\naxs[5].plot(dates, D_smooth)\n\naxs[6].set_title(\"Weekday log_ratios\")\naxs[6].plot(dates, x_smooth[:, jnp.array([13, 19, 25])])\nplt.show()",
    "crumbs": [
      "4 Models",
      "4.1 Removing reporting delays and weekday effects",
      "Model for removing weekday effects and reporting delays"
    ]
  },
  {
    "objectID": "4 Models/4.1 Removing reporting delays and weekday effects/model.html#christmas-period-model",
    "href": "4 Models/4.1 Removing reporting delays and weekday effects/model.html#christmas-period-model",
    "title": "Model for removing weekday effects and reporting delays",
    "section": "Christmas period model",
    "text": "Christmas period model\n\ndates.iloc[110]\n\nTimestamp('2021-01-20 00:00:00')\n\n\n\nchristmas_inds = jnp.arange(75, 110)\ny_nans = y.astype(jnp.float64).at[christmas_inds].set(jnp.nan)\nchristmas_inds = jnp.isnan(y_nans)\n_model_miss = lambda theta, aux: account_for_nans(\n    _model(theta, aux), y_nans, christmas_inds\n)[0]\ntheta0_miss = initial_theta(y_miss, _model_miss, theta_manual, aux, 100)\nmodel_miss, y_miss = account_for_nans(\n    _model(theta0_miss.x, aux), y_nans, christmas_inds\n)\nproposal_la_miss, _ = laplace_approximation(y_miss, model_miss, 100)\nkey, subkey = jrn.split(key)\nproposal_meis_miss, _ = MEIS(\n    y_miss, model_miss, proposal_la_miss.z, proposal_la_miss.Omega, 10, 4000, subkey\n)\n\nkey, subkey = jrn.split(key)\nmissing_samples, missing_log_weights = pgssm_importance_sampling(\n    y_miss,\n    model_miss,\n    proposal_meis_miss.z,\n    proposal_meis.Omega,\n    1000,\n    subkey,\n)\ness_pct(missing_log_weights)\n\nArray(0.0496248, dtype=float64)\n\n\n\ness_pct(missing_log_weights)\n\nArray(0.0496248, dtype=float64)\n\n\n\nx_smooth = (\n    # vmap(smooth_x)(samples_la) * normalize_weights(log_weights_la)[:, None, None]\n    vmap(state_mode, (None, 0))(model_miss, missing_samples)\n    * normalize_weights(missing_log_weights)[:, None, None]\n).sum(axis=0)\n\n\n# plt.plot(jnp.exp(x_smooth[:, 12]), label=\"smoothed\")\n# plt.plot(jnp.exp(x_smooth[:, 2:8].sum(axis=1)), label=\"smoothed\")\n# plt.plot(jnp.exp(x_smooth[:, 0]), label=\"smoothed\")\n# plt.plot(y.sum(axis=1), label=\"Y\")\n# plt.plot(jnp.exp(x_smooth[:, 1]), label=\"smoothed\")\n\nI_smooth = jnp.exp(x_smooth[:, 0])\nI_smooth_LA = jnp.exp(x_smooth_la[:, 0])\n\nrho_smooth = jnp.exp(x_smooth[:, 1])\nrho_smooth_LA = jnp.exp(x_smooth_la[:, 1])\n\nD_smooth = jnp.exp(x_smooth[:, 2])\nD_smooth_LA = jnp.exp(x_smooth_la[:, 2])\n\nW_smooth = jnp.exp(x_smooth[:, 3])\nW_smooth_LA = jnp.exp(x_smooth_la[:, 3])\n\nlog_ratios = x_smooth[:, 9:12]\nlog_probs = to_log_probs(log_ratios)\n\nfig, axs = plt.subplots(3, 2, figsize=(10, 10))\n\naxs = axs.flatten()\nfig.tight_layout()\n\naxs[0].set_title(\"incidences\")\naxs[0].plot(dates, I_smooth, label=\"$I_t$\")\naxs[0].plot(dates, I_smooth_LA, label=\"$I_t$ LA\")\naxs[0].plot(dates, y.sum(axis=1), label=\"$Y_t$\")\naxs[0].legend()\n\naxs[1].set_title(\"growth factor\")\naxs[1].plot(dates, rho_smooth, label=\"$\\\\log \\\\rho_t$\")\naxs[1].plot(dates, rho_smooth_LA, label=\"$\\\\log \\\\rho_t$ LA\")\n\naxs[2].set_title(\"weekday effect\")\naxs[2].plot(dates, W_smooth, label=\"$W_t$\")\naxs[2].plot(dates, W_smooth_LA, label=\"$W_t$ LA\")\n\naxs[3].set_title(\"delay probabilities\")\n\n\naxs[3].plot(dates, jnp.exp(log_probs[:, 0]), label=\"$p_{t, 1}$\")\naxs[3].plot(dates, jnp.exp(log_probs[:, 1]), label=\"$p_{t, 2}$\")\naxs[3].plot(dates, jnp.exp(log_probs[:, 2]), label=\"$p_{t, 3}$\")\naxs[3].plot(dates, jnp.exp(log_probs[:, 3]), label=\"$p_{t, 4}$\")\naxs[3].plot(dates, jnp.exp(log_probs).sum(axis=1), label=\"total p\")\naxs[3].legend()\n\naxs[4].set_title(\"Log ratios\")\naxs[4].plot(dates, log_ratios[:, 0], label=\"$q_{t, 1}$\")\naxs[4].plot(dates, log_ratios[:, 1], label=\"$q_{t, 2}$\")\naxs[4].plot(dates, log_ratios[:, 2], label=\"$q_{t, 3}$\")\nfor d in dates[::7]:\n    axs[4].axvline(d, color=\"black\", alpha=0.2)\n\naxs[5].set_title(\"Dirt\")\naxs[5].plot(dates, D_smooth)\nplt.show()",
    "crumbs": [
      "4 Models",
      "4.1 Removing reporting delays and weekday effects",
      "Model for removing weekday effects and reporting delays"
    ]
  },
  {
    "objectID": "4 Models/4.2 Regional growth factor model/showcase.html",
    "href": "4 Models/4.2 Regional growth factor model/showcase.html",
    "title": "Application 1: Showcase",
    "section": "",
    "text": "from ssm4epi.models.regional_growth_factor import (\n    key,\n    n_iterations,\n    N_mle,\n    N_meis,\n    N_posterior,\n    percentiles_of_interest,\n    make_aux,\n    dates_full,\n    cases_full,\n    n_ij,\n    n_tot,\n    account_for_nans,\n    growth_factor_model,\n)\n\nimport jax.numpy as jnp\nimport jax\nimport jax.random as jrn\n\nfrom isssm.importance_sampling import prediction\nfrom isssm.laplace_approximation import laplace_approximation as LA\nfrom isssm.modified_efficient_importance_sampling import (\n    modified_efficient_importance_sampling as MEIS,\n)\n\nfrom pyprojroot.here import here\n\njax.config.update(\"jax_enable_x64\", True)\nfrom isssm.estimation import initial_theta\nimport pickle\n\n\ninitial_date = dates_full[15]\nnp1 = 10\ndates = dates_full[15 : 15 + np1]\naux = make_aux(initial_date, cases_full, n_ij, n_tot, np1)\n\ny = aux[0][1:]\ny_nan = y.at[-1].set(jnp.nan)\nmissing_inds = jnp.isnan(y_nan)\ntheta_manual = jnp.array(\n    [5.950e00, -2.063e00, -5.355e00, -4.511e-01, -5.711e-01, 7.932e-01]\n)\n_, y_miss = account_for_nans(\n    growth_factor_model(theta_manual, aux), y_nan, missing_inds\n)\n_model_miss = lambda theta, aux: account_for_nans(\n    growth_factor_model(theta, aux), y_nan, missing_inds\n)[0]\n\ntheta0_result = initial_theta(y_miss, _model_miss, theta_manual, aux, n_iterations)\ntheta0 = theta0_result.x\nfitted_model = _model_miss(theta0, aux)\n\nproposal_la, info_la = LA(y_miss, fitted_model, n_iterations)\nkey, subkey = jrn.split(key)\nproposal_meis, info_meis = MEIS(\n    y_miss, fitted_model, proposal_la.z, proposal_la.Omega, n_iterations, N_meis, subkey\n)\nkey, subkey = jrn.split(key)\n\n\ndef f_pred(x, s, y):\n    y_total = y[-1].sum()[None]\n    y_counties = y[-1]\n    growth_factors = s.reshape(-1)\n    return jnp.concatenate([y_total, y_counties, growth_factors])\n\n\npreds = prediction(\n    f_pred,\n    y_miss,\n    proposal_la,\n    fitted_model,\n    N_posterior,\n    subkey,\n    percentiles_of_interest,\n    growth_factor_model(theta0, aux),\n)\n\nresult = (theta0, proposal_meis, preds, dates, y)\n\nwith open(here() / \"data/results/4_local_outbreak_model/results.pickle\", \"wb\") as f:\n    pickle.dump(result, f)\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[2], line 51\n     38 preds = prediction(\n     39     f_pred,\n     40     y_miss,\n   (...)\n     46     growth_factor_model(theta0, aux),\n     47 )\n     49 result = (theta0, proposal_meis, preds, dates, y)\n---&gt; 51 with open(here() / \"data/results/4_local_outbreak_model/results.pickle\", \"wb\") as f:\n     52     pickle.dump(result, f)\n\nFile /opt/homebrew/Caskroom/miniconda/base/envs/research/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324, in _modified_open(file, *args, **kwargs)\n    317 if file in {0, 1, 2}:\n    318     raise ValueError(\n    319         f\"IPython won't let you open fd={file} by default \"\n    320         \"as it is likely to crash IPython. If you know what you are doing, \"\n    321         \"you can use builtins' open.\"\n    322     )\n--&gt; 324 return io_open(file, *args, **kwargs)\n\nFileNotFoundError: [Errno 2] No such file or directory: '/Users/stefan/workspace/work/phd/thesis/data/results/4_local_outbreak_model/results.pickle'\n\n\n\n\npreds\n\n(Array([1.66255525e+04, 2.12210181e+00, 1.03437335e+00, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00], dtype=float64),\n Array([7.66581469e+03, 2.91949927e+00, 1.49179858e+00, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00], dtype=float64),\n Array([[6.15800000e+03, 0.00000000e+00, 0.00000000e+00, ...,\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n        [6.56250733e+03, 0.00000000e+00, 0.00000000e+00, ...,\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n        [7.59300000e+03, 0.00000000e+00, 0.00000000e+00, ...,\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n        ...,\n        [2.92606867e+04, 8.00000000e+00, 3.00000000e+00, ...,\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n        [3.63747669e+04, 1.00000000e+01, 4.00000000e+00, ...,\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n        [4.12856030e+04, 1.30000000e+01, 7.00000000e+00, ...,\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]], dtype=float64))",
    "crumbs": [
      "4 Models",
      "4.2 Regional growth factor model",
      "Application 1: Showcase"
    ]
  },
  {
    "objectID": "4 Models/4.2 Regional growth factor model/figures.html",
    "href": "4 Models/4.2 Regional growth factor model/figures.html",
    "title": "Figures",
    "section": "",
    "text": "library(here)\nsource(here(\"setup.R\"))\nlibrary(reticulate)\n\nexpit &lt;- function(x) {\n    1 / (1 + exp(-x))\n}\nlogit &lt;- function(x) {\n    log(x / (1 - x))\n}\ntheta_to_params &lt;- function(theta) {\n    c(\n        \"alpha\" = expit(theta[1]),\n        \"s2_r\" = exp(theta[2]),\n        \"s2_spatial\" = exp(theta[3]),\n        \"q\" = expit(theta[4]),\n        \"C\" = exp(theta[5]) + 1,\n        \"r\" = exp(theta[6])\n    )\n}\n\n\ntoennis_results &lt;- py_load_object(here(\"data/results/4_local_outbreak_model/results.pickle\"))\n\ntheta &lt;- toennis_results[[1]]\npredictions &lt;- toennis_results[[3]]\ndates &lt;- toennis_results[[4]]\ny &lt;- toennis_results[[5]]\n\n\ntheta_to_params(theta) %&gt;%\n    enframe() %&gt;%\n    ggplot(aes(name, value)) +\n    geom_point()\n\n\n\n\n\n\n\n\n\nnp &lt;- import(\"numpy\")\nmeans &lt;- np$array(predictions[[1]])\nsds &lt;- np$array(predictions[[2]])\nquantiles &lt;- np$array(predictions[[3]])\n\ndim(quantiles)\n\n\n234401\n\n\n\n# plot(exp(means[402:412]))\nplot(exp(means[seq(407, by = 400, length.out = 10)]))",
    "crumbs": [
      "4 Models",
      "4.2 Regional growth factor model",
      "Figures"
    ]
  },
  {
    "objectID": "4 Models/4.3 Nowcasting hospitalizations/figures.html",
    "href": "4 Models/4.3 Nowcasting hospitalizations/figures.html",
    "title": "Figures for the hospitalization model",
    "section": "",
    "text": "library(here)\nsource(here(\"setup.R\"))",
    "crumbs": [
      "4 Models",
      "4.3 Nowcasting hospitalizations",
      "Figures for the hospitalization model"
    ]
  },
  {
    "objectID": "4 Models/4.3 Nowcasting hospitalizations/figures.html#application-1-showcase",
    "href": "4 Models/4.3 Nowcasting hospitalizations/figures.html#application-1-showcase",
    "title": "Figures for the hospitalization model",
    "section": "Application 1: Showcase",
    "text": "Application 1: Showcase\n\ndf_files &lt;- tibble(fname = list.files(here(\"data/results/4_hospitalizations/showcase\"), full.names = T)) %&gt;%\n    filter(str_detect(fname, \".npy$\")) %&gt;%\n    # mutate(age_group = paste0(\"A\", str_extract(fname, \"\\\\d\\\\d-\\\\d\\\\d\"))) %&gt;%\n    mutate(age_group = c(\"A00-04\", \"A00+\", \"A05-14\", \"A15-34\", \"A35-59\", \"A60-79\", \"A80+\")) %&gt;%\n    mutate(n_delay = c(5, 8, 5, 7, 7, 8, 8))\n\nread_age_predictions &lt;- function(fname, n_delay) {\n    dates &lt;- seq(from = ymd(\"2021-11-22\"), to = ymd(\"2022-04-29\"), by = \"1 day\")\n    variables &lt;- c(\n        \"prob_hosp\",\n        paste0(\"p_delay_smooth\", seq(n_delay)),\n        paste0(\"p_delay\", seq(n_delay)),\n        \"weekday_0\",\n        \"weekday_1\"\n    )\n\n    read_predictions(fname, dates, variables)\n}\n\ndf_predictions &lt;- df_files %&gt;%\n    mutate(data = map2(fname, n_delay, read_age_predictions)) %&gt;%\n    select(-fname, -n_delay) %&gt;%\n    unnest() %&gt;%\n    mutate(age_group = ordered_age_group(age_group))\n\n\nError in `mutate()`:\ni In argument: `data = map2(fname, n_delay, read_age_predictions)`.\nCaused by error in `map2()`:\ni In index: 1.\nCaused by error in `dimnames(predictions) &lt;- list(type = c(\"mean\", \"sd\", probs), date = as.character(\n    dates), variable = variables)`:\n! L\"ange von 'dimnames' [3] ungleich der Arrayausdehnung\nTraceback:\n\n1. df_files %&gt;% mutate(data = map2(fname, n_delay, read_age_predictions)) %&gt;% \n .     select(-fname, -n_delay) %&gt;% unnest() %&gt;% mutate(age_group = ordered_age_group(age_group))\n2. mutate(., age_group = ordered_age_group(age_group))\n3. unnest(.)\n4. select(., -fname, -n_delay)\n5. mutate(., data = map2(fname, n_delay, read_age_predictions))\n6. mutate.data.frame(., data = map2(fname, n_delay, read_age_predictions))\n7. mutate_cols(.data, dplyr_quosures(...), by)\n8. withCallingHandlers(for (i in seq_along(dots)) {\n .     poke_error_context(dots, i, mask = mask)\n .     context_poke(\"column\", old_current_column)\n .     new_columns &lt;- mutate_col(dots[[i]], data, mask, new_columns)\n . }, error = dplyr_error_handler(dots = dots, mask = mask, bullets = mutate_bullets, \n .     error_call = error_call, error_class = \"dplyr:::mutate_error\"), \n .     warning = dplyr_warning_handler(state = warnings_state, mask = mask, \n .         error_call = error_call))\n9. mutate_col(dots[[i]], data, mask, new_columns)\n10. mask$eval_all_mutate(quo)\n11. eval()\n12. map2(fname, n_delay, read_age_predictions)\n13. map2_(\"list\", .x, .y, .f, ..., .progress = .progress)\n14. with_indexed_errors(i = i, names = names, error_call = .purrr_error_call, \n  .     call_with_cleanup(map2_impl, environment(), .type, .progress, \n  .         n, names, i))\n15. withCallingHandlers(expr, error = function(cnd) {\n  .     if (i == 0L) {\n  .     }\n  .     else {\n  .         message &lt;- c(i = \"In index: {i}.\")\n  .         if (!is.null(names) && !is.na(names[[i]]) && names[[i]] != \n  .             \"\") {\n  .             name &lt;- names[[i]]\n  .             message &lt;- c(message, i = \"With name: {name}.\")\n  .         }\n  .         else {\n  .             name &lt;- NULL\n  .         }\n  .         cli::cli_abort(message, location = i, name = name, parent = cnd, \n  .             call = error_call, class = \"purrr_error_indexed\")\n  .     }\n  . })\n16. call_with_cleanup(map2_impl, environment(), .type, .progress, \n  .     n, names, i)\n17. .f(.x[[i]], .y[[i]], ...)\n18. read_predictions(fname, dates, variables)   # at line 17 of file &lt;text&gt;\n19. .handleSimpleError(function (cnd) \n  . {\n  .     if (i == 0L) {\n  .     }\n  .     else {\n  .         message &lt;- c(i = \"In index: {i}.\")\n  .         if (!is.null(names) && !is.na(names[[i]]) && names[[i]] != \n  .             \"\") {\n  .             name &lt;- names[[i]]\n  .             message &lt;- c(message, i = \"With name: {name}.\")\n  .         }\n  .         else {\n  .             name &lt;- NULL\n  .         }\n  .         cli::cli_abort(message, location = i, name = name, parent = cnd, \n  .             call = error_call, class = \"purrr_error_indexed\")\n  .     }\n  . }, \"L\\\"ange von 'dimnames' [3] ungleich der Arrayausdehnung\", \n  .     base::quote(dimnames(predictions) &lt;- list(type = c(\"mean\", \n  .         \"sd\", probs), date = as.character(dates), variable = variables)))\n20. h(simpleError(msg, call))\n21. cli::cli_abort(message, location = i, name = name, parent = cnd, \n  .     call = error_call, class = \"purrr_error_indexed\")\n22. rlang::abort(message, ..., call = call, use_cli_format = TRUE, \n  .     .frame = .frame)\n23. signal_abort(cnd, .file)\n24. signalCondition(cnd)\n25. (function (cnd) \n  . {\n  .     local_error_context(dots, i = frame[[i_sym]], mask = mask)\n  .     if (inherits(cnd, \"dplyr:::internal_error\")) {\n  .         parent &lt;- error_cnd(message = bullets(cnd))\n  .     }\n  .     else {\n  .         parent &lt;- cnd\n  .     }\n  .     message &lt;- c(cnd_bullet_header(action), i = if (has_active_group_context(mask)) cnd_bullet_cur_group_label())\n  .     abort(message, class = error_class, parent = parent, call = error_call)\n  . })(structure(list(message = c(i = \"In index: 1.\"), trace = structure(list(\n  .     call = list(IRkernel::main(), kernel$run(), handle_shell(), \n  .         executor$execute(msg), tryCatch(evaluate(request$content$code, \n  .             envir = .GlobalEnv, output_handler = oh, stop_on_error = 1L), \n  .             interrupt = function(cond) {\n  .                 log_debug(\"Interrupt during execution\")\n  .                 interrupted &lt;&lt;- TRUE\n  .             }, error = .self$handle_error), tryCatchList(expr, \n  .             classes, parentenv, handlers), tryCatchOne(tryCatchList(expr, \n  .             names[-nh], parentenv, handlers[-nh]), names[nh], \n  .             parentenv, handlers[[nh]]), doTryCatch(return(expr), \n  .             name, parentenv, handler), tryCatchList(expr, names[-nh], \n  .             parentenv, handlers[-nh]), tryCatchOne(expr, names, \n  .             parentenv, handlers[[1L]]), doTryCatch(return(expr), \n  .             name, parentenv, handler), evaluate(request$content$code, \n  .             envir = .GlobalEnv, output_handler = oh, stop_on_error = 1L), \n  .         evaluate_call(expr, parsed$src[[i]], envir = envir, enclos = enclos, \n  .             debug = debug, last = i == length(out), use_try = stop_on_error != \n  .                 2L, keep_warning = keep_warning, keep_message = keep_message, \n  .             log_echo = log_echo, log_warning = log_warning, output_handler = output_handler, \n  .             include_timing = include_timing), timing_fn(handle(ev &lt;- withCallingHandlers(withVisible(eval_with_user_handlers(expr, \n  .             envir, enclos, user_handlers)), warning = wHandler, \n  .             error = eHandler, message = mHandler))), handle(ev &lt;- withCallingHandlers(withVisible(eval_with_user_handlers(expr, \n  .             envir, enclos, user_handlers)), warning = wHandler, \n  .             error = eHandler, message = mHandler)), try(f, silent = TRUE), \n  .         tryCatch(expr, error = function(e) {\n  .             call &lt;- conditionCall(e)\n  .             if (!is.null(call)) {\n  .                 if (identical(call[[1L]], quote(doTryCatch))) \n  .                   call &lt;- sys.call(-4L)\n  .                 dcall &lt;- deparse(call, nlines = 1L)\n  .                 prefix &lt;- paste(\"Error in\", dcall, \": \")\n  .                 LONG &lt;- 75L\n  .                 sm &lt;- strsplit(conditionMessage(e), \"\\n\")[[1L]]\n  .                 w &lt;- 14L + nchar(dcall, type = \"w\") + nchar(sm[1L], \n  .                   type = \"w\")\n  .                 if (is.na(w)) \n  .                   w &lt;- 14L + nchar(dcall, type = \"b\") + nchar(sm[1L], \n  .                     type = \"b\")\n  .                 if (w &gt; LONG) \n  .                   prefix &lt;- paste0(prefix, \"\\n  \")\n  .             }\n  .             else prefix &lt;- \"Error : \"\n  .             msg &lt;- paste0(prefix, conditionMessage(e), \"\\n\")\n  .             .Internal(seterrmessage(msg[1L]))\n  .             if (!silent && isTRUE(getOption(\"show.error.messages\"))) {\n  .                 cat(msg, file = outFile)\n  .                 .Internal(printDeferredWarnings())\n  .             }\n  .             invisible(structure(msg, class = \"try-error\", condition = e))\n  .         }), tryCatchList(expr, classes, parentenv, handlers), \n  .         tryCatchOne(expr, names, parentenv, handlers[[1L]]), \n  .         doTryCatch(return(expr), name, parentenv, handler), withCallingHandlers(withVisible(eval_with_user_handlers(expr, \n  .             envir, enclos, user_handlers)), warning = wHandler, \n  .             error = eHandler, message = mHandler), withVisible(eval_with_user_handlers(expr, \n  .             envir, enclos, user_handlers)), eval_with_user_handlers(expr, \n  .             envir, enclos, user_handlers), eval(expr, envir, \n  .             enclos), eval(expr, envir, enclos), df_files %&gt;% \n  .             mutate(data = map2(fname, n_delay, read_age_predictions)) %&gt;% \n  .             select(-fname, -n_delay) %&gt;% unnest() %&gt;% mutate(age_group = ordered_age_group(age_group)), \n  .         mutate(., age_group = ordered_age_group(age_group)), \n  .         unnest(.), select(., -fname, -n_delay), mutate(., data = map2(fname, \n  .             n_delay, read_age_predictions)), mutate.data.frame(., \n  .             data = map2(fname, n_delay, read_age_predictions)), \n  .         mutate_cols(.data, dplyr_quosures(...), by), withCallingHandlers(for (i in seq_along(dots)) {\n  .             poke_error_context(dots, i, mask = mask)\n  .             context_poke(\"column\", old_current_column)\n  .             new_columns &lt;- mutate_col(dots[[i]], data, mask, \n  .                 new_columns)\n  .         }, error = dplyr_error_handler(dots = dots, mask = mask, \n  .             bullets = mutate_bullets, error_call = error_call, \n  .             error_class = \"dplyr:::mutate_error\"), warning = dplyr_warning_handler(state = warnings_state, \n  .             mask = mask, error_call = error_call)), mutate_col(dots[[i]], \n  .             data, mask, new_columns), mask$eval_all_mutate(quo), \n  .         eval(), map2(fname, n_delay, read_age_predictions), map2_(\"list\", \n  .             .x, .y, .f, ..., .progress = .progress), with_indexed_errors(i = i, \n  .             names = names, error_call = .purrr_error_call, call_with_cleanup(map2_impl, \n  .                 environment(), .type, .progress, n, names, i)), \n  .         withCallingHandlers(expr, error = function(cnd) {\n  .             if (i == 0L) {\n  .             }\n  .             else {\n  .                 message &lt;- c(i = \"In index: {i}.\")\n  .                 if (!is.null(names) && !is.na(names[[i]]) && \n  .                   names[[i]] != \"\") {\n  .                   name &lt;- names[[i]]\n  .                   message &lt;- c(message, i = \"With name: {name}.\")\n  .                 }\n  .                 else {\n  .                   name &lt;- NULL\n  .                 }\n  .                 cli::cli_abort(message, location = i, name = name, \n  .                   parent = cnd, call = error_call, class = \"purrr_error_indexed\")\n  .             }\n  .         }), call_with_cleanup(map2_impl, environment(), .type, \n  .             .progress, n, names, i), .f(.x[[i]], .y[[i]], ...), \n  .         read_predictions(fname, dates, variables), .handleSimpleError(`&lt;fn&gt;`, \n  .             \"L\\\"ange von 'dimnames' [3] ungleich der Arrayausdehnung\", \n  .             base::quote(dimnames(predictions) &lt;- list(type = c(\"mean\", \n  .                 \"sd\", probs), date = as.character(dates), variable = variables))), \n  .         h(simpleError(msg, call)), cli::cli_abort(message, location = i, \n  .             name = name, parent = cnd, call = error_call, class = \"purrr_error_indexed\"), \n  .         rlang::abort(message, ..., call = call, use_cli_format = TRUE, \n  .             .frame = .frame)), parent = c(0L, 1L, 2L, 3L, 4L, \n  .     5L, 6L, 7L, 6L, 9L, 10L, 4L, 12L, 13L, 13L, 15L, 16L, 17L, \n  .     18L, 19L, 13L, 13L, 13L, 23L, 24L, 0L, 0L, 0L, 0L, 0L, 0L, \n  .     31L, 32L, 32L, 34L, 35L, 0L, 37L, 38L, 39L, 38L, 38L, 42L, \n  .     0L, 44L, 45L, 46L), visible = c(TRUE, TRUE, TRUE, TRUE, TRUE, \n  .     TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, \n  .     TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, \n  .     TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, \n  .     TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, \n  .     FALSE, FALSE), namespace = c(\"IRkernel\", NA, \"IRkernel\", \n  .     NA, \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \n  .     \"evaluate\", \"evaluate\", \"evaluate\", \"evaluate\", \"base\", \"base\", \n  .     \"base\", \"base\", \"base\", \"base\", \"base\", \"evaluate\", \"base\", \n  .     \"base\", NA, \"dplyr\", \"tidyr\", \"dplyr\", \"dplyr\", \"dplyr\", \n  .     \"dplyr\", \"base\", \"dplyr\", NA, \"dplyr\", \"purrr\", \"purrr\", \n  .     \"purrr\", \"base\", \"purrr\", NA, NA, \"base\", \"purrr\", \"cli\", \n  .     \"rlang\"), scope = c(\"::\", NA, \"local\", NA, \"::\", \"local\", \n  .     \"local\", \"local\", \"local\", \"local\", \"local\", \"::\", \":::\", \n  .     \"local\", \"local\", \"::\", \"::\", \"local\", \"local\", \"local\", \n  .     \"::\", \"::\", \":::\", \"::\", \"::\", NA, \"::\", \"::\", \"::\", \"::\", \n  .     \":::\", \":::\", \"::\", \":::\", NA, \"local\", \"::\", \":::\", \":::\", \n  .     \"::\", \":::\", \"global\", \"global\", \"::\", \"local\", \"::\", \"::\"\n  .     ), error_frame = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, \n  .     FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, \n  .     FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, \n  .     FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, \n  .     FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, \n  .     FALSE, FALSE, FALSE, FALSE, FALSE)), row.names = c(NA, -47L\n  . ), version = 2L, class = c(\"rlang_trace\", \"rlib_trace\", \"tbl\", \n  . \"data.frame\")), parent = structure(list(message = \"L\\\"ange von 'dimnames' [3] ungleich der Arrayausdehnung\", \n  .     call = dimnames(predictions) &lt;- list(type = c(\"mean\", \"sd\", \n  .         probs), date = as.character(dates), variable = variables)), class = c(\"simpleError\", \n  . \"error\", \"condition\")), location = 1L, name = NULL, rlang = list(\n  .     inherit = TRUE), call = map2(fname, n_delay, read_age_predictions), \n  .     use_cli_format = TRUE), class = c(\"purrr_error_indexed\", \n  . \"rlang_error\", \"error\", \"condition\")))\n26. abort(message, class = error_class, parent = parent, call = error_call)\n27. signal_abort(cnd, .file)\n\n\n\n\ndf_predictions %&gt;%\n    distinct(variable) %&gt;%\n    pull()\n\n\nprob_hospp_delay_smooth1p_delay_smooth2p_delay_smooth3p_delay_smooth4p_delay_smooth5p_delay1p_delay2p_delay3p_delay4p_delay5weekday_0weekday_1p_delay_smooth6p_delay_smooth7p_delay_smooth8p_delay6p_delay7p_delay8\n\n\n    \n        Levels:\n    \n    \n    'prob_hosp''p_delay_smooth1''p_delay_smooth2''p_delay_smooth3''p_delay_smooth4''p_delay_smooth5''p_delay1''p_delay2''p_delay3''p_delay4''p_delay5''weekday_0''weekday_1''p_delay_smooth6''p_delay_smooth7''p_delay_smooth8''p_delay6''p_delay7''p_delay8'\n\n\n\n\np_smoothed_delays &lt;- df_predictions %&gt;%\n    select(age_group, date, variable, mean, `0.025`, `0.5`, `0.975`) %&gt;%\n    filter(str_detect(variable, \"p_delay_smooth\")) %&gt;%\n    mutate(variable = str_replace(variable, \"p_delay_smooth\", \"\")) %&gt;%\n    rename(k = variable) %&gt;%\n    mutate(k = as.numeric(k) - 1) %&gt;%\n    filter(k &lt;= 3) %&gt;%\n    ggplot(aes(x = date, y = mean * 100, color = age_group, group = age_group, linetype = age_group)) +\n    geom_line() +\n    # geom_ribbon(aes(ymin = `0.025` * 100, ymax = `0.975` * 100, fill = age_group), alpha = 0.2) +\n    labs(\n        title = \"\",\n        x = \"\",\n        y = \"$q_{t, k}$ [\\\\%]\",\n        color = \"\",\n        linetype = \"\"\n    ) +\n    scale_x_four_weekly() +\n    facet_wrap(~k, scales = \"free_y\", nrow = 4, labeller = labeller(k = function(x) str_glue(\"$ k = {x} $\")))\np_smoothed_delays\n\n\n\n\n\n\n\n\n\ndf_predictions %&gt;%\n    select(age_group, date, variable, mean, `0.025`, `0.5`, `0.975`) %&gt;%\n    filter(str_detect(variable, \"p_delay_smooth\")) %&gt;%\n    mutate(variable = str_replace(variable, \"p_delay_smooth\", \"\")) %&gt;%\n    rename(k = variable) %&gt;%\n    mutate(k = as.numeric(k) - 1) %&gt;%\n    filter(k &lt;= 6) %&gt;%\n    mutate(k = factor(k)) %&gt;%\n    select(age_group, date, k, mean) %&gt;%\n    complete(age_group, date, k, fill = list(mean = 0)) %&gt;%\n    ggplot(aes(date, mean, fill = k)) +\n    geom_area(position = position_stack(reverse = T)) +\n    facet_wrap(~age_group, nrow = 1) +\n    scale_fill_brewer(palette = \"RdYlGn\") +\n    labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\n\np_hospitalizations &lt;- df_predictions %&gt;%\n    select(age_group, date, variable, mean, `0.025`, `0.5`, `0.975`) %&gt;%\n    filter(variable == \"prob_hosp\") %&gt;%\n    ggplot(aes(x = date, y = mean * 100, color = age_group, group = age_group, linetype = age_group)) +\n    geom_line() +\n    geom_ribbon(aes(ymin = `0.025` * 100, ymax = `0.975` * 100, fill = age_group), alpha = 0.2) +\n    labs(\n        title = \"\",\n        x = \"\",\n        y = \" $p_{t}$ [\\\\%]\"\n    ) +\n    scale_x_four_weekly() +\n    scale_y_log10() +\n    theme(legend.position = \"none\")\np_hospitalizations\n\n\n\n\n\n\n\n\n\ndf_predictions %&gt;%\n    select(age_group, date, variable, mean, `0.025`, `0.5`, `0.975`) %&gt;%\n    filter(str_detect(variable, \"weekday\")) %&gt;%\n    ggplot(aes(x = wday(date, label = T, week_start = 1), mean, color = variable, group = variable)) +\n    stat_summary(fun = mean, geom = \"line\") +\n    facet_wrap(~age_group, nrow = 1) +\n    xlab(\"\") +\n    ylab(\"$W_\\\\cdot$\")\n\n\n\n\n\n\n\n\n\np_weekday &lt;- df_predictions %&gt;%\n    select(age_group, date, variable, mean, `0.025`, `0.5`, `0.975`) %&gt;%\n    filter(str_detect(variable, \"weekday\")) %&gt;%\n    ggplot(aes(x = date, y = mean, color = age_group, group = age_group, linetype = age_group)) +\n    geom_line() +\n    # geom_ribbon(aes(ymin = `0.025` * 100, ymax = `0.975` * 100, fill = age_group), alpha = 0.2) +\n    labs(\n        title = \"\",\n        x = \"\",\n        y = \"\",\n        linetype = \"\",\n        color = \"\"\n    ) +\n    scale_x_four_weekly() +\n    facet_wrap(~variable, nrow = 2, labeller = as_labeller(c(\"weekday_0\" = \"$W_{t,0}$\", \"weekday_1\" = \"$W_{t,1}$\")))\np_weekday\n\n\n\n\n\n\n\n\n\nh &lt;- 10\nw &lt;- 10\noptions(\n    repr.plot.width = w,\n    repr.plot.height = h,\n    repr.plot.res = 300\n)\n((p_hospitalizations / p_weekday) | p_smoothed_delays) + plot_layout(guides = \"collect\")\n# (p_hospitalizations / p_smoothed_delays / p_weekday) + plot_layout(guides = \"collect\")\n\n\nggsave_tikz(here(\"tikz/hospitalization_showcase_results.tex\"), width = w, height = h)\n\npdf: 2",
    "crumbs": [
      "4 Models",
      "4.3 Nowcasting hospitalizations",
      "Figures for the hospitalization model"
    ]
  },
  {
    "objectID": "4 Models/utilities.html#computation",
    "href": "4 Models/utilities.html#computation",
    "title": "Utilities for the models of this thesis",
    "section": "Computation",
    "text": "Computation\nFor \\(p \\in \\mathbf R^{k}_{&gt;0}\\) with \\(\\sum_{i = 1}^k p_{i} = 1\\), let \\(\\log q_i = \\log \\frac{p_{i}}{p_{k}}\\) for \\(i = 1, \\dots, k -1\\). Then \\[\n    p_{k} = \\frac{1}{1 + \\sum_{i = 1}^{k-1}q_{i}},\n\\] so $$ p_{i} = q_{i} p_{k} = .\n$$\nAnother parametrization takes consecutive conditonal probabilities, using logits to make the problem unconstrained.\nThus for \\(p\\in \\mathbf R^k\\) we have \\[\n    q_{i} = \\frac{p_{i}}{1 - \\sum_{j = 1}^{i - 1} p_{i}},\n\\] for \\(i = 1, \\dots, k - 1\\) (\\(q_k\\) is \\(1\\) and can be discarded).\nThen for \\(i = 1, \\dots, k\\) \\[\n    p_{i} = q_{i} \\prod_{j = 1}^{i - 1}(1 - q_j).\n\\]",
    "crumbs": [
      "4 Models",
      "Utilities for the models of this thesis"
    ]
  },
  {
    "objectID": "3 SSMs/08_comparison_figures.html",
    "href": "3 SSMs/08_comparison_figures.html",
    "title": "3.8 Comparison of Importance Sampling Methods - Figures",
    "section": "",
    "text": "library(here)\nsource(here(\"code/setup.R\"))\n\ndf_are_mu &lt;- read_csv(here(\"data/figures/are_mu.csv\"))\ndf_are_sigma2 &lt;- read_csv(here(\"data/figures/are_sigma2.csv\"))\n\ndf_rho_ce_sigma2 &lt;- read_csv(here(\"data/figures/rho_cem_sigma2.csv\"))\ndf_rho_eis_sigma2 &lt;- read_csv(here(\"data/figures/rho_eis_sigma2.csv\"))\ndf_rho_mu &lt;- read_csv(here(\"data/figures/rho_mu.csv\"))\n\nrename_scenario &lt;- function(df) {\n    df %&gt;% rename(\n        \"$\\\\mathcal N (0, 1)$\" = 2,\n        \"loc. mixture, $\\\\omega^2 = 0.1$\" = 3,\n        \"loc. mixture, $\\\\omega^2= 0.5$\" = 4,\n        \"loc. mixture, $\\\\omega^2 = 1$\" = 5,\n        \"scale mixture, $\\\\varepsilon^2 = 100$\" = 6,\n        \"scale mixture, $\\\\varepsilon^2 = 10$\" = 7,\n        \"scale mixture, $\\\\varepsilon^2 = 2$\" = 8\n    )\n}\n\nadd_type &lt;- function(df) {\n    df %&gt;% mutate(type_short = str_sub(name, 1, 3)) %&gt;%\n    mutate(type = case_when(\n        type_short == \"$\\\\m\" ~ \"normal\",\n        type_short == \"loc\" ~ \"location mixture\",\n        type_short == \"sca\" ~ \"scale mixture\"\n    )) %&gt;%\n    mutate(type = factor(type, levels=c(\"normal\", \"location mixture\", \"scale mixture\")))\n}\n\nhere() starts at /Users/stefan/workspace/work/phd/thesis",
    "crumbs": [
      "3 SSMs",
      "3.8 Comparison of Importance Sampling Methods - Figures"
    ]
  },
  {
    "objectID": "3 SSMs/08_comparison_figures.html#tikzare.tex",
    "href": "3 SSMs/08_comparison_figures.html#tikzare.tex",
    "title": "3.8 Comparison of Importance Sampling Methods - Figures",
    "section": "tikz/are.tex",
    "text": "tikz/are.tex\n\ndf_are_mu_fig &lt;- df_are_mu %&gt;%\n    rename_scenario %&gt;%\n    pivot_longer(-sigma2) %&gt;%\n    add_type()\n\ndf_are_sigma2_fig &lt;- df_are_sigma2 %&gt;%\n    rename_scenario() %&gt;%\n    pivot_longer(-mu) %&gt;%\n    add_type()\n    \n    \nplt_mu &lt;-  df_are_mu_fig %&gt;%\n    mutate(name = str_extract(name, \"\\\\$.*\\\\$\")) %&gt;%\n    ggplot(aes(sigma2, value, linetype=name, color = type)) +\n    geom_line() +\n    coord_cartesian(ylim = c(1e-2, 1e2))+\n    scale_y_log10(breaks = 10^(-2:2), labels = paste0(\"$10^{\",-2:2,\"}$\")) +\n    scale_linetype_discrete(name = \"\") +\n    scale_color_discrete(name=\"\") +\n    labs(x = \"$\\\\sigma^2$\", y = \"$\\\\frac{V_{EIS}}{V_{CE}}$\")\n\nplt_sigma2 &lt;-  df_are_sigma2_fig %&gt;%\n    mutate(name = str_extract(name, \"\\\\$.*\\\\$\")) %&gt;%\n    ggplot(aes(mu, value, linetype=name, color = type)) +\n    geom_line() +\n    coord_cartesian(ylim = c(1e-2, 1e2))+\n    scale_y_log10(breaks = 10^(-2:2), labels = paste0(\"$10^{\",-2:2,\"}$\")) +\n    scale_linetype_discrete(name = \"\") +\n    scale_color_discrete(name=\"\") +\n    labs(x = \"$\\\\mu$\", y = \"$\\\\frac{V_{EIS}}{V_{CE}}$\")\n\n(plt_mu | plt_sigma2) + plot_layout(guides = \"collect\") &\n  theme(legend.position='bottom')\n\nggsave_tikz(here(\"tikz/are.tex\"))\n\nWarning message in scale_y_log10(breaks = 10^(-2:2), labels = paste0(\"$10^{\", -2:2, :\n\"log-10 transformation introduced infinite values.\"\nWarning message in scale_y_log10(breaks = 10^(-2:2), labels = paste0(\"$10^{\", -2:2, :\n\"log-10 transformation introduced infinite values.\"\nWarning message in scale_y_log10(breaks = 10^(-2:2), labels = paste0(\"$10^{\", -2:2, :\n\"log-10 transformation introduced infinite values.\"\nWarning message in scale_y_log10(breaks = 10^(-2:2), labels = paste0(\"$10^{\", -2:2, :\n\"log-10 transformation introduced infinite values.\"\n\n\npdf: 2",
    "crumbs": [
      "3 SSMs",
      "3.8 Comparison of Importance Sampling Methods - Figures"
    ]
  },
  {
    "objectID": "3 SSMs/08_comparison_figures.html#tikzrho.tex",
    "href": "3 SSMs/08_comparison_figures.html#tikzrho.tex",
    "title": "3.8 Comparison of Importance Sampling Methods - Figures",
    "section": "tikz/rho.tex",
    "text": "tikz/rho.tex\n\ndf_rho_ce_fig &lt;- df_rho_ce_sigma2 %&gt;%\n    rename_scenario %&gt;%\n    pivot_longer(-mu) %&gt;%\n    add_type() %&gt;%\n    mutate(name = str_extract(name, \"\\\\$.*\\\\$\")) %&gt;%\n    rename(\"rho\" = value) %&gt;%\n    mutate(\"EF\" = 1/rho * 100)\ndf_rho_eis_fig &lt;- df_rho_eis_sigma2 %&gt;%\n    rename_scenario %&gt;%\n    pivot_longer(-mu) %&gt;%\n    add_type() %&gt;%\n    mutate(name = str_extract(name, \"\\\\$.*\\\\$\")) %&gt;%\n    rename(\"rho\" = value) %&gt;%\n    mutate(\"EF\" = 1/rho * 100)\n\nscales_fig &lt;- list(\n    scale_linetype_discrete(name = \"\"),\n    scale_color_discrete(name=\"\")\n)\n\n\np1 &lt;- df_rho_ce_fig %&gt;%\n    ggplot(aes(mu, EF, color = type, linetype = name)) +\n    geom_line() +\n    labs(x=\"$\\\\mu$\", y=\"EF [\\\\%]\") +\n    scales_fig +\n    ggtitle(\"CE-method\")\np2 &lt;- df_rho_eis_fig %&gt;%\n    ggplot(aes(mu, EF, color = type, linetype = name)) +\n    geom_line() +\n    labs(x=\"$\\\\mu$\", y=\"EF [\\\\%]\") +\n    scales_fig+\n    ggtitle(\"EIS\")\n\np3 &lt;- select(rename(df_rho_ce_fig, \"CE\" = \"EF\"), -rho) %&gt;%\n    inner_join(select(rename(df_rho_eis_fig, \"EIS\"=\"EF\"), -rho)) %&gt;%\n    mutate(`relative EF` = CE/EIS) %&gt;%\n    ggplot(aes(mu, `relative EF`, color = type, linetype = name)) +\n    geom_line()+\n    coord_cartesian(ylim = c(NA, 1.5)) +\n    scales_fig + \n    xlab(\"$\\\\mu$\") +\n    ylab(\"relative EF (CE/EIS)\")\n\n\n(p1 | p2 | p3) + plot_layout(guides = \"collect\") & theme(legend.position = \"bottom\")\n\nggsave_tikz(here(\"tikz/rho.tex\"))\n\nJoining with `by = join_by(mu, name, type_short, type)`\n\n\npdf: 2",
    "crumbs": [
      "3 SSMs",
      "3.8 Comparison of Importance Sampling Methods - Figures"
    ]
  },
  {
    "objectID": "3 SSMs/08_comparison_figures.html#tikzrho_mu.tex",
    "href": "3 SSMs/08_comparison_figures.html#tikzrho_mu.tex",
    "title": "3.8 Comparison of Importance Sampling Methods - Figures",
    "section": "tikz/rho_mu.tex",
    "text": "tikz/rho_mu.tex\n\ndf_rho_mu %&gt;%\n    rename_scenario %&gt;%\n    pivot_longer(-sigma2) %&gt;%\n    add_type() %&gt;%\n    mutate(name = str_extract(name, \"\\\\$.*\\\\$\")) %&gt;%\n    rename(\"rho\" = value) %&gt;%\n    mutate(\"EF\" = 1/rho * 100) %&gt;%\n    ggplot(aes(sigma2, EF, color=type, linetype = name)) +\n    geom_line() +\n    labs(x= \"$\\\\sigma^2$\", y=\"EF [\\\\%]\") +\n    scale_color_discrete(name=\"\") +\n    scale_linetype_discrete(name=\"\") +\n    theme(legend.position = \"bottom\")\n\nggsave_tikz(here(\"tikz/rho_mu.tex\"))\n\nWarning message:\n\"Removed 5 rows containing missing values or values outside the scale range\n(`geom_line()`).\"\nWarning message:\n\"Removed 5 rows containing missing values or values outside the scale range\n(`geom_line()`).\"\n\n\npdf: 2",
    "crumbs": [
      "3 SSMs",
      "3.8 Comparison of Importance Sampling Methods - Figures"
    ]
  },
  {
    "objectID": "3 SSMs/08_comparison_figures.html#tikzcem_eis_sigma2.tex",
    "href": "3 SSMs/08_comparison_figures.html#tikzcem_eis_sigma2.tex",
    "title": "3.8 Comparison of Importance Sampling Methods - Figures",
    "section": "tikz/cem_eis_sigma2.tex",
    "text": "tikz/cem_eis_sigma2.tex\n\ncem_sigma2 &lt;- read_csv(here(\"data/figures/cem_sigma2.csv\")) %&gt;%\n    rename_scenario %&gt;%\n    pivot_longer(-mu) %&gt;%\n    add_type() %&gt;%\n    mutate(name = str_extract(name, \"\\\\$.*\\\\$\")) %&gt;%\n    rename(sigma2 = value) %&gt;%\n    mutate(method = \"CE\")\n    \neis_sigma2 &lt;- read_csv(here(\"data/figures/eis_sigma2.csv\")) %&gt;%\n    rename_scenario %&gt;%\n    pivot_longer(-mu) %&gt;%\n    add_type() %&gt;%\n    mutate(name = str_extract(name, \"\\\\$.*\\\\$\")) %&gt;%\n    rename(sigma2 = value) %&gt;%\n    mutate(method = \"EIS\")\n\ncem_sigma2 %&gt;%\n    rbind(eis_sigma2) %&gt;%\n    ggplot(aes(mu, sigma2, color=type, linetype = name)) +\n    geom_line() +\n    facet_wrap(~method, scales = \"free_y\") +\n    coord_cartesian(ylim = c(0, 3)) +\n    labs(x = \"$\\\\mu$\", y = \"$\\\\sigma^2$\") +\n    scale_color_discrete(name=\"\") +\n    scale_linetype_discrete(name=\"\") +\n    theme(legend.position = \"bottom\")\n\nggsave_tikz(here(\"tikz/cem_eis_sigma2.tex\"))\n\npdf: 2",
    "crumbs": [
      "3 SSMs",
      "3.8 Comparison of Importance Sampling Methods - Figures"
    ]
  },
  {
    "objectID": "3 SSMs/08_comparison_figures.html#tikzgssm_eps.tex",
    "href": "3 SSMs/08_comparison_figures.html#tikzgssm_eps.tex",
    "title": "3.8 Comparison of Importance Sampling Methods - Figures",
    "section": "tikz/gssm_eps.tex",
    "text": "tikz/gssm_eps.tex\n\ndf_eps &lt;- read_csv(here(\"data/figures/gsmm_eps.csv\"))\n\np1 &lt;- df_eps %&gt;%\n    select(epsilon, sigma2_cem, sigma2_eis) %&gt;%\n    pivot_longer(-epsilon, names_pattern = \"sigma2_(.*)\", values_to = \"sigma2\") %&gt;%\n    mutate(name = ifelse(name == \"cem\", \"CE\", \"EIS\")) %&gt;%\n    ggplot(aes(epsilon ** 2, sigma2 - (1/2 + 1/2/epsilon**2), color = name)) +\n    geom_line() +\n    scale_x_log10() +\n    #scale_y_log10() +\n    scale_color_discrete(name=\"\") +\n    labs(x = \"$\\\\varepsilon^2$\", y = \"$\\\\hat \\\\sigma^2 - (\\\\frac 1 2 + \\\\frac 1 {2\\\\varepsilon^2})$\") +\n    ylim(-10, 5)\n\np2 &lt;- df_eps %&gt;%\n    select(epsilon, starts_with(\"rho\")) %&gt;%\n    pivot_longer(-epsilon, names_pattern = \"rho_(.*)\", values_to = \"rho\") %&gt;%\n    mutate(name = ifelse(name == \"cem\", \"CE\", \"EIS\")) %&gt;%\n    ggplot(aes(epsilon ** 2, 1/rho * 100, color = name)) +\n    geom_line() +\n    scale_x_log10() +\n    labs(x = \"$\\\\varepsilon^2$\", y = \"EF [\\\\%]\") +\n    scale_color_discrete(name=\"\") + \n    ylim (0, 100)\n    \np3 &lt;- df_eps %&gt;%\n    select(epsilon, are) %&gt;%\n    ggplot(aes(epsilon ** 2, are)) +\n    geom_line() +\n    scale_x_log10() +\n    labs(x = \"$\\\\varepsilon^2$\", y = \"$V_{EIS} / V_{CE}$\") +\n    theme(legend.position = \"bottom\")\n\n(p1 / p2/ p3) + plot_layout(guides = \"collect\") & theme(legend.position = \"bottom\")\nggsave_tikz(here(\"tikz/gsmm_eps.tex\"))\n\nWarning message:\n\"Removed 110 rows containing missing values or values outside the scale range\n(`geom_line()`).\"\nWarning message:\n\"Removed 136 rows containing missing values or values outside the scale range\n(`geom_line()`).\"\nWarning message:\n\"Removed 110 rows containing missing values or values outside the scale range\n(`geom_line()`).\"\nWarning message:\n\"Removed 136 rows containing missing values or values outside the scale range\n(`geom_line()`).\"\n\n\npdf: 2",
    "crumbs": [
      "3 SSMs",
      "3.8 Comparison of Importance Sampling Methods - Figures"
    ]
  },
  {
    "objectID": "3 SSMs/compare_ce_eis_mse_figures.html",
    "href": "3 SSMs/compare_ce_eis_mse_figures.html",
    "title": "3.9 Comparison of importance sampling methods - Figures",
    "section": "",
    "text": "library(here)\nsource(here(\"code/setup.R\"))\n\ndf_mse &lt;- read_csv(here(\"data/figures/03_state_space_models/compare_ce_eis_mse.csv\"))\ndf_ef &lt;- read_csv(here(\"data/figures/03_state_space_models/ef_large_N.csv\"))\ndf_dimensions &lt;- read_csv(here(\"data/figures/03_state_space_models/ef_dimensions.csv\"))\n\nhere() starts at /Users/stefan/workspace/work/phd/thesis\n\n\n\n\ndf_mse %&gt;%\n    ggplot(aes(N, ef, group = interaction(N, method))) +\n    geom_boxplot(aes(color = method)) +\n    scale_x_log10() +\n    labs(x=\"N\", y=\"Efficiency factor\")\n\nggsave_tikz(here(\"tikz/ef_comparison_medium_N.tex\"))\n\npdf: 2\n\n\n\n\n\n\n\n\n\n\nNs &lt;- df_mse %&gt;% \n    distinct(N) %&gt;%\n    pull\n\nmse_plot &lt;- df_mse %&gt;%\n    filter(method != \"LA\") %&gt;%\n    mutate(mse = mean_bias + mean_var + var_bias + var_var) %&gt;%\n    select(method, N, mse) %&gt;%\n    group_by(method, N) %&gt;%\n    summarise(lower = t.test(mse)$conf.int[1], upper = t.test(mse)$conf.int[2], middle = mean(mse)) %&gt;%\n    ggplot(aes(N, group = interaction(N, method))) +\n    #geom_boxplot(aes(color = method)) +\n    geom_point(aes(y=middle, color=method)) +\n    scale_x_log10(breaks = Ns[c(T,F)], minor_breaks = Ns[c(F,T)]) +\n    scale_y_log10() +\n    scale_shape_discrete(name=\"\") +\n    scale_color_discrete(name=\"\") +\n    labs(x=\"N\", y=\"$\\\\widehat{\\\\mathrm{aMSE}}$\")\n\n\nbias_var_plot &lt;- df_mse %&gt;%\n    filter(method != \"LA\") %&gt;%\n    select(method, N, mean_bias:var_var) %&gt;%\n    pivot_longer(-c(method, N)) %&gt;%\n    mutate(parameter = ifelse(str_starts(name, \"mean\"), \"$\\\\mu$\", \"$\\\\sigma^2$\")) %&gt;% \n    mutate(type=ifelse(str_ends(name, \"bias\"), \"aBias$^2$\", \"aVar\")) %&gt;%\n    mutate(combination = paste0(method, \" - \", type)) %&gt;%\n    select(N, value, method, combination, type, parameter) %&gt;%\n    group_by(N, method, combination, type, parameter) %&gt;%\n    summarise(lower = t.test(value)$conf.int[1], upper = t.test(value)$conf.int[2], middle = mean(value)) %&gt;%\n    ggplot(aes(N, middle)) +\n    #geom_boxplot(aes(color=method, group=interaction(N, combination), linetype = type)) +\n    #geom_errorbar(aes(ymin = lower, ymax = upper, color=method, group=interaction(N, combination), linetype = type)) +\n    geom_point(aes(x = N * ifelse(type == \"aBias$^2$\", 1.1, 1/1.01), y=middle, color=method, pch=type)) +\n    #geom_smooth(aes(color=method), method='lm', se=FALSE, alpha = .1) + \n    scale_y_log10() +\n    scale_x_log10(breaks = Ns[c(T,F)], minor_breaks = Ns[c(F,T)]) +\n    facet_wrap(~parameter) +\n    scale_shape_discrete(name=\"\") +\n    scale_color_discrete(name=\"\") +\n    xlab(\"N\") + ylab(\"\")\n\n(mse_plot / bias_var_plot) + plot_layout(guides = \"collect\")\nggsave_tikz(here(\"tikz/mse_bias_var_decomposition.tex\"))\n\npdf: 2\n\n\n\n\n\n\n\n\n\n\ndf_ef %&gt;%\n    ggplot(aes(y=ef, color=method)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\n\nN &lt;- df_dimensions %&gt;%\n    pull(N) %&gt;%\n    unique\n    \nM &lt;- df_dimensions %&gt;%\n    count(N,n,m, method) %&gt;%\n    pull(nn) %&gt;%\n    min\n\nns &lt;- df_dimensions %&gt;%\n    distinct(n) %&gt;%\n    pull(n)\n\ncaption &lt;- str_glue(\"$\\\\alpha = \\\\frac 1 2, r = \\\\frac 1 {{10}}, N = 10^{{ {round(log(N, 10))} }}, M={M}$\")\n\ndf_dimensions %&gt;%\n    ggplot(aes(x=n, y=ef, color=method)) +\n    geom_boxplot(aes(group = interaction(n, method)), width=.1) +\n    scale_x_log10(breaks=ns) +\n    labs(x=\"n\", y=\"Efficiency factor [\\\\%]\", caption=caption) +\n    facet_wrap(~m, label=label_both)  +\n    scale_color_discrete(name=\"\") + \n    theme(legend.position = \"bottom\")\n\nggsave_tikz(here(\"tikz/ef_time_dimension.tex\"))\n\nStoring counts in `nn`, as `n` already present in input\ni Use `name = \"new_name\"` to pick a new name.\n\n\npdf: 2",
    "crumbs": [
      "3 SSMs",
      "3.9 Comparison of importance sampling methods - Figures"
    ]
  },
  {
    "objectID": "3 SSMs/05_gaussian_is.html",
    "href": "3 SSMs/05_gaussian_is.html",
    "title": "3.5 Gaussian Importance Sampling",
    "section": "",
    "text": "library(here)\nsource(here(\"setup.R\"))\n\nhere() starts at /Users/stefan/workspace/work/phd/thesis\n\n\n\n\nprob_failure_ce &lt;- function(N, o2) {\n    pchisq(q = N * (1 + 1 / o2), N - 1, lower.tail = FALSE)\n}\n\nNs &lt;- round(10^(seq(1, 3, length.out = 5)))\no2 &lt;- 10^(seq(0, 2, length.out = 101))\n\ndf &lt;- expand.grid(N = Ns, o2 = o2) %&gt;%\n    mutate(prob_failure = map2_dbl(N, o2, prob_failure_ce)) %&gt;%\n    mutate(N = factor(N))\n\ndf %&gt;%\n    ggplot(aes(o2, prob_failure, color = N)) +\n    geom_line() +\n    scale_x_log10() +\n    # scale_y_log10(limits = c(1e-10, 1/2)) +\n    labs(\n        y = \"$1 - F_{\\\\chi^2_{N - 1}(N)} \\\\approx \\\\mathbf P ( \\\\hat \\\\Gamma &gt; 1 )$\",\n        x = \"$\\\\omega^2$\"\n    )\nggsave_tikz(here(\"tikz/ce_prob_failure.tex\"), width = 6, height = 3)\n\npdf: 2",
    "crumbs": [
      "3 SSMs",
      "3.5 Gaussian Importance Sampling"
    ]
  },
  {
    "objectID": "2 Epidemiology/04_desiderata.html",
    "href": "2 Epidemiology/04_desiderata.html",
    "title": "2.4 Desiderata for epidemiological models",
    "section": "",
    "text": "library(here)\nsource(here(\"setup.R\"))\n\n\nw_df &lt;- read_csv(here(\"data/processed/generation_time.csv\"))\nw &lt;- w_df$w\n\nestimate_R &lt;- function(I, w) {\n    I_ext &lt;- c(rep(0, length(w) - 1), I)\n    I / stats::filter(I_ext, c(0, w), sides = 1, method = \"convolution\")[-(1:(length(w) - 1))]\n}\n\nestimate_rho &lt;- function(I) {\n    I / lag(I, 7)\n}\n\nfct_events &lt;- ordered(c(\"outbreak\", \"christmas\"))\n\nstopifnot(length(estimate_R(rpois(100, 10), w)) == 100)\n\nrki &lt;- read_csv(here(\"data/processed/rki_county.csv\"))\n\n\nrki %&gt;%\n    group_by(date) %&gt;%\n    summarize(cases = sum(cases)) %&gt;%\n    mutate(cases_7 = rollmean(cases, k = 7, fill = NA)) %&gt;%\n    mutate(R = estimate_R(cases, w)) %&gt;%\n    mutate(R_7 = estimate_R(cases_7, w)) %&gt;%\n    mutate(rho = estimate_rho(cases)) %&gt;%\n    mutate(rho_7 = estimate_rho(cases_7)) %&gt;%\n    filter(date &gt;= \"2020-04-01\") %&gt;%\n    select(date, R:rho_7) %&gt;%\n    pivot_longer(-date) %&gt;%\n    mutate(type = ifelse(str_starts(name, \"R\"), \"$\\\\hat R$\", \"$\\\\hat \\\\rho^7$\")) %&gt;%\n    mutate(timescale = ifelse(str_ends(name, \"_7\"), \"7-day avg.\", \"daily\")) %&gt;%\n    ggplot(aes(date, value, color = timescale, alpha = timescale, group = timescale)) +\n    geom_line() +\n    facet_wrap(~type, scales = \"free_y\", nrow = 2) +\n    scale_color_manual(values = c(\"daily\" = \"black\", \"7-day avg.\" = pal_npg()(1))) +\n    scale_alpha_manual(values = c(\"daily\" = .4, \"7-day avg.\" = 1)) +\n    coord_cartesian(ylim = c(0, 3.2)) +\n    scale_x_date(date_minor_breaks = \"1 month\", limits = c(ymd(\"2020-04-01\"), ymd(\"2021-03-01\")), date_breaks = \"3 months\", date_labels = \"%b %Y\") +\n    theme(legend.position = \"bottom\") +\n    geom_mark_rect(\n        aes(\n            x = date, y = value,\n            filter = (date &lt;= ymd(\"2020-07-01\") & date &gt;= ymd(\"2020-06-01\")), group = type, linetype = fct_events[1]\n        ),\n        inherit.aes = FALSE\n    ) +\n    geom_mark_rect(\n        aes(\n            x = date, y = value,\n            filter = (date &gt;= ymd(\"2020-12-07\") & date &lt;= ymd(\"2021-01-14\")),\n            group = type, linetype = fct_events[2]\n        ),\n        inherit.aes = FALSE\n    ) +\n    scale_linetype_manual(values = c(\"outbreak\" = \"dashed\", \"christmas\" = \"dotted\")) +\n    labs(x = \"\", y = \"\", color = \"incidences\", alpha = \"incidences\", linetype = \"\")\n# geom_magnify( # Toennies\n#    from = list(ymd(\"2020-06-01\"), ymd(\"2020-07-01\"), .6, 2.2),\n#    to = list(ymd(\"2020-08-01\"), ymd(\"2020-10-14\"), 2.1, 3),\n#    axes = \"x\"\n# )\n# geom_magnify( # christmas\n#    from = list(ymd(\"2020-12-07\"), ymd(\"2021-01-14\"), .5, 1.5),\n#    to = list(ymd(\"2021-03-01\"), ymd(\"2021-07-01\"), 2.2, 3.9),\n#    axes = \"x\"\n# )\n\nggsave_tikz(here(\"tikz/rho_and_R_naive.tex\"))\n\nWarning message:\n“Removed 1590 rows containing missing values or values outside the scale range\n(`geom_line()`).”\nWarning message:\n“Removed 1590 rows containing missing values or values outside the scale range\n(`geom_line()`).”\n\n\npdf: 2",
    "crumbs": [
      "2 Epidemiology",
      "2.4 Desiderata for epidemiological models"
    ]
  },
  {
    "objectID": "2 Epidemiology/03_data.html",
    "href": "2 Epidemiology/03_data.html",
    "title": "2.3 Available data and its quality",
    "section": "",
    "text": "library(here)\nsource(here(\"setup.R\"))\nlibrary(zoo)\nlibrary(ggmagnify)\nlibrary(latex2exp)\nlibrary(scales)\n\nhere() starts at /Users/stefan/workspace/work/phd/thesis",
    "crumbs": [
      "2 Epidemiology",
      "2.3 Available data and its quality"
    ]
  },
  {
    "objectID": "2 Epidemiology/03_data.html#rki-case-and-death-data",
    "href": "2 Epidemiology/03_data.html#rki-case-and-death-data",
    "title": "2.3 Available data and its quality",
    "section": "RKI case and death data",
    "text": "RKI case and death data\n\ntikz/cases_germany.tex\n\nrki_county &lt;- here(\"data/processed/RKI_county.csv\") %&gt;%\n    read_csv()\n\ndf_weekly &lt;- rki_county %&gt;%\n    group_by(date) %&gt;%\n    summarize(cases = sum(cases), deaths = sum(deaths)) %&gt;%\n    mutate(cases_7 = rollmean(cases, k = 7, fill = NA), deaths_7 = rollmean(deaths, k = 7, fill = NA))\n\np_cases &lt;- df_weekly %&gt;%\n    select(-deaths_7) %&gt;%\n    pivot_longer(cols = c(cases, cases_7), names_to = \"type\", values_to = \"value\") %&gt;%\n    mutate(type = ifelse(type == \"cases\", \"daily\", \"7-day avg.\")) %&gt;%\n    ggplot(aes(date, value, color = type, alpha = type)) +\n    geom_line() +\n    scale_y_continuous(labels = scales::comma) +\n    scale_color_manual(values = c(\"daily\" = \"black\", \"7-day avg.\" = pal_npg()(1))) +\n    scale_alpha_manual(values = c(\"daily\" = .4, \"7-day avg.\" = 1)) +\n    labs(x = \"\", y = \"\\\\# reported cases\", alpha = \"\", color = \"\", title = \"A\") +\n    geom_magnify( # christmas\n        from = list(ymd(\"2020-12-01\"), ymd(\"2021-02-01\"), .1 * 1e5, .35 * 1e5),\n        to =   list(ymd(\"2021-03-01\"), ymd(\"2022-01-01\"), 1e5, 2.8e5),\n        axes = \"x\"\n    ) +\n    geom_magnify( # Toennies\n        from = list(ymd(\"2020-05-01\"), ymd(\"2020-08-01\"), 0, .1e4),\n        to = list(ymd(\"2020-01-01\"), ymd(\"2020-11-01\"), 1e5, 2.8e5),\n        axes = \"x\"\n    )\n\np_deaths &lt;- df_weekly %&gt;%\n    select(-cases_7) %&gt;%\n    pivot_longer(cols = c(deaths, deaths_7), names_to = \"type\", values_to = \"value\") %&gt;%\n    mutate(type = ifelse(type == \"deaths\", \"daily\", \"7-day avg.\")) %&gt;%\n    ggplot(aes(date, value, color = type, alpha = type)) +\n    geom_line() +\n    scale_color_manual(values = c(\"daily\" = \"black\", \"7-day avg.\" = pal_npg()(1))) +\n    scale_alpha_manual(values = c(\"daily\" = .4, \"7-day avg.\" = 1)) +\n    labs(x = \"reporting date (Meldedatum)\", y = \"\\\\# reported deaths\", alpha = \"\", color = \"\", title = \"B\")\n# geom_magnify( # christmas\n#    from = list(ymd(\"2020-11-15\"), ymd(\"2021-02-01\"), 600, 1000),\n#    to = list(ymd(\"2021-03-10\"), ymd(\"2021-10-10\"), 400, 1100),\n#    axes = \"x\"\n# )\n\n(p_cases / p_deaths) + plot_layout(guides = \"collect\") & theme(legend.position = \"bottom\", legend.box = \"horizontal\")\nggsave_tikz(here(\"tikz/cases_germany.tex\"))\n\nWarning message:\n“Removed 6 rows containing missing values or values outside the scale range\n(`geom_line()`).”\nWarning message:\n“Removed 6 rows containing missing values or values outside the scale range\n(`geom_line()`).”\nWarning message:\n“Removed 6 rows containing missing values or values outside the scale range\n(`geom_line()`).”\nWarning message:\n“Removed 6 rows containing missing values or values outside the scale range\n(`geom_line()`).”\n\n\npdf: 2\n\n\n\n\n\n\n\n\n\n\n\nInfection date vs. reporting date\n\nrki_raw &lt;- read_csv(here(\"data/raw/RKI.csv\"))\n\nrki_raw %&gt;%\n    group_by(IstErkrankungsbeginn) %&gt;%\n    summarize(cases = sum(AnzahlFall * (NeuerFall &gt;= 0))) %&gt;%\n    mutate(prop = cases / sum(cases) * 100)\n\nrki_raw %&gt;%\n    filter(IstErkrankungsbeginn == 1) %&gt;%\n    group_by(Refdatum) %&gt;%\n    summarize(cases = sum(AnzahlFall * (NeuerFall &gt;= 0))) %&gt;%\n    ggplot(aes(Refdatum, cases)) +\n    geom_line() +\n    labs(title = \"Only dates of symptom onset\")\n\n\nA tibble: 2 x 3\n\n\nIstErkrankungsbeginn\ncases\nprop\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0\n29422629\n75.80404\n\n\n1\n9391434\n24.19596\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nreporting triangle\nWe use the reporting triangle for the number of cases, i.e. on any day \\(t\\) the number of cases \\[I_{s,t}\\] that are reported associated with date \\(s &lt; t\\).\nWe begin our analysis on April 1st 2020, when data have become stable enough to warrant an analysis.\n\nfull_rep_tri &lt;- read_csv(here(\"data/raw/rki_cases_deaths_delays.csv\")) %&gt;%\n    select(t = rki_date, s = county_date, I = cases) %&gt;%\n    filter(s &gt;= ymd(\"2020-04-01\"))\n\n\nfull_rep_tri %&gt;%\n    pull(s) %&gt;%\n    range()\n\n\n2020-04-012023-10-12\n\n\n\nfull_rep_tri %&gt;%\n    mutate(I = ifelse(I &lt; 1, NA, I)) %&gt;%\n    ggplot(aes(x = s, y = t, fill = I)) +\n    geom_tile() +\n    scale_fill_viridis_c(trans = \"log10\") +\n    labs(x = \"s\", y = \"t\", fill = TeX(\"$I_{s,t}$\"))\n\n\n\n\n\n\n\n\nHow often is \\(I_{s,t} &gt; I_{s,T}\\)?\n\nrel_diffs &lt;- full_rep_tri %&gt;%\n    group_by(s) %&gt;%\n    arrange(t) %&gt;%\n    mutate(rel_diff = (I - tail(I, 1)) / tail(I, 1)) %&gt;%\n    ungroup() %&gt;%\n    filter(rel_diff &gt; 0) %&gt;%\n    mutate(rel_diff_pct = rel_diff * 100)\n\nrel_diffs %&gt;%\n    ggplot(aes(x = rel_diff)) +\n    geom_histogram(bins = 100) +\n    scale_x_log10()\n\nrel_diffs %&gt;%\n    summarize(\n        q90 = quantile(rel_diff_pct, .9),\n        q95 = quantile(rel_diff_pct, .95),\n        q99 = quantile(rel_diff_pct, .99),\n        q999 = quantile(rel_diff_pct, .999)\n    )\n\nrel_diffs %&gt;%\n    arrange(rel_diff_pct) %&gt;%\n    tail(20)\n\n\nA tibble: 1 x 4\n\n\nq90\nq95\nq99\nq999\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0.4065041\n0.6497726\n1.840491\n16.1435\n\n\n\n\n\n\nA tibble: 20 x 5\n\n\nt\ns\nI\nrel_diff\nrel_diff_pct\n\n\n&lt;date&gt;\n&lt;date&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n2021-01-17\n2020-12-24\n58712\n1.779924\n177.9924\n\n\n2021-01-17\n2021-01-03\n24542\n1.781594\n178.1594\n\n\n2021-01-17\n2020-11-30\n36988\n1.781889\n178.1889\n\n\n2021-01-17\n2020-12-31\n54618\n1.792331\n179.2331\n\n\n2021-01-17\n2020-12-21\n56628\n1.792996\n179.2996\n\n\n2021-01-17\n2021-01-02\n27996\n1.799040\n179.9040\n\n\n2021-01-17\n2021-01-10\n23721\n1.800921\n180.0921\n\n\n2021-01-17\n2021-01-04\n40161\n1.805323\n180.5323\n\n\n2021-01-17\n2021-01-05\n77088\n1.808306\n180.8306\n\n\n2021-01-17\n2020-12-22\n81282\n1.808736\n180.8736\n\n\n2021-01-17\n2020-12-29\n77477\n1.812436\n181.2436\n\n\n2021-01-17\n2021-01-07\n73534\n1.816963\n181.6963\n\n\n2021-01-17\n2021-01-12\n63763\n1.822996\n182.2996\n\n\n2021-01-17\n2021-01-08\n69526\n1.833055\n183.3055\n\n\n2021-01-17\n2020-12-14\n54632\n1.874158\n187.4158\n\n\n2021-01-17\n2020-12-28\n45134\n1.875510\n187.5510\n\n\n2021-01-17\n2021-01-01\n29620\n1.891731\n189.1731\n\n\n2021-01-17\n2020-12-25\n38099\n1.898364\n189.8364\n\n\n2021-01-17\n2020-12-26\n33642\n1.912223\n191.2223\n\n\n2021-01-17\n2021-01-11\n40595\n2.008374\n200.8374\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntikz/reporting_delays_cases.tex\n\ndata_april &lt;- full_rep_tri %&gt;%\n    filter(s &gt;= ymd(\"2021-12-01\"), s &lt; ymd(\"2022-01-01\")) %&gt;%\n    filter(t &gt;= ymd(\"2021-12-01\"), t &lt; ymd(\"2022-01-01\"))\np_reptri_case &lt;- data_april %&gt;%\n    ggplot(aes(x = s, y = t, fill = I)) +\n    geom_tile() +\n    scale_fill_viridis_c(trans = \"log10\", labels = label_comma()) +\n    labs(x = \"$s$\", y = \"$t$\", fill = \"$I_{s,t}$\")\n\np_maginal_case &lt;- data_april %&gt;%\n    ggplot(aes(x = s, y = I, color = t, group = factor(t))) +\n    geom_line() +\n    theme(axis.title.x = element_blank(), axis.ticks.x = element_blank(), axis.text.x = element_blank()) +\n    labs(x = \"\", y = \"$I_{s,t}$\", color = \"data date\") +\n    scale_color_viridis_c(option = \"H\", trans = \"date\")\n\n(p_maginal_case / p_reptri_case) + plot_layout(heights = c(1, 2))",
    "crumbs": [
      "2 Epidemiology",
      "2.3 Available data and its quality"
    ]
  },
  {
    "objectID": "2 Epidemiology/03_data.html#dataprocessedrki_4day_rt.csv",
    "href": "2 Epidemiology/03_data.html#dataprocessedrki_4day_rt.csv",
    "title": "2.3 Available data and its quality",
    "section": "data/processed/RKI_4day_rt.csv",
    "text": "data/processed/RKI_4day_rt.csv\nWe are only interested in the number of newly reported cases on each day. Let \\[\n    I_{s,t} = \\sum_{\\tau = 0}^{t - s} i_{s, \\tau}\n\\] be the distribution of reported on date \\(t\\) for date \\(s\\). We will assume that \\(i_{s,\\tau} \\geq 0\\) always holds. To ensure this, let \\(T\\) be the last date observed and set \\[\n    \\tilde I_{s,t} = \\min \\{\\max \\{I_{s,s}, \\dots, I_{s,t}\\}, I_{s,T}\\}.\n\\] \\(\\tilde I_{s,t}\\) is a running maximum that is cut-off at the final value \\(I_{s,T}\\) (to deal with big rearrangements of cases, due to missingness or faulty data).\n\nrep_tri_cummax &lt;- full_rep_tri %&gt;%\n    arrange(t) %&gt;%\n    group_by(s) %&gt;%\n    mutate(\n        I_tilde = pmin(cummax(I), tail(I, 1)),\n    ) %&gt;%\n    ungroup()\n\nincrements &lt;- rep_tri_cummax %&gt;%\n    group_by(s) %&gt;%\n    mutate(\n        i = I_tilde - lag(I_tilde, default = 0),\n    ) %&gt;%\n    ungroup()\n\nstopifnot(all(increments$i &gt;= 0))\n\nincrements %&gt;%\n    ggplot(aes(x = s, y = t - s, fill = i)) +\n    geom_tile() +\n    scale_fill_viridis_c(trans = \"log10\", na.value = rgb(0, 0, 0, 0)) +\n    ylim(0, 30)\n\n\nmax_tau &lt;- 4\nincrements %&gt;%\n    mutate(tau = as.numeric(t - s)) %&gt;%\n    filter(tau &lt;= max_tau) %&gt;%\n    filter(s &gt;= min(s) + max_tau) %&gt;%\n    select(s, tau, i) %&gt;%\n    pivot_wider(names_from = tau, values_from = i, values_fill = 0) %&gt;%\n    rename(county_date = s) %&gt;%\n    write_csv(here(\"data/processed/RKI_4day_rt.csv\"))\n\nWarning message in scale_fill_viridis_c(trans = \"log10\", na.value = rgb(0, 0, 0, :\n“log-10 transformation introduced infinite values.”\nWarning message:\n“Removed 795691 rows containing missing values or values outside the scale range\n(`geom_tile()`).”\n\n\n\n\n\n\n\n\n\nWe see that most delays are short. Let us investigate the proportion of reported cases with delay \\(\\tau = t - s\\):\n\\[\n    p_{\\tau} = \\frac{\\sum_{t - s = \\tau} i_{s, t}}{\\sum_{t,s} i_{s,t}}\n\\]\nand we will look at the survival function \\[\n    \\hat S(\\tau) =  1- \\sum_{\\tau' \\leq \\tau} p_{\\tau'}\n\\] which measures the fraction of cases reported after \\(\\tau\\) days of delay.\n\ntikz/survival_function_rep_tri_incidences.tex\n\ntotal_S &lt;- increments %&gt;%\n    mutate(tau = as.numeric(t - s)) %&gt;%\n    group_by(tau) %&gt;%\n    summarize(total = sum(i)) %&gt;%\n    ungroup() %&gt;%\n    mutate(p = total / sum(total)) %&gt;%\n    mutate(S = 1 - cumsum(p))\n\nquarter_S &lt;- increments %&gt;%\n    mutate(tau = as.numeric(t - s)) %&gt;%\n    group_by(quarter = floor_date(s, \"quarter\"), tau) %&gt;%\n    summarize(total = sum(i)) %&gt;%\n    mutate(p = total / sum(total)) %&gt;%\n    mutate(S = 1 - cumsum(p)) %&gt;%\n    ungroup()\np_boxplots &lt;- increments %&gt;%\n    mutate(tau = as.numeric(t - s)) %&gt;%\n    group_by(s) %&gt;%\n    arrange(t) %&gt;%\n    mutate(p = i / sum(i)) %&gt;%\n    mutate(S = 1 - cumsum(p)) %&gt;%\n    ungroup() %&gt;%\n    filter(tau &lt;= 10) %&gt;%\n    filter(S &gt; 1e-7) %&gt;%\n    ggplot(aes(x = factor(tau), y = S, group = tau)) +\n    geom_boxplot() +\n    scale_y_log10(breaks = c(1, .01, .0001), labels = c(\"1\", \"0.01\", \"0.0001\")) +\n    geom_hline(aes(yintercept = y, linetype = name), data = tibble(y = c(.05, .01), name = factor(2:3, labels = c(\"5\\\\%\", \"1\\\\%\")))) +\n    scale_linetype_manual(values = c(\"dashed\", \"dotted\")) +\n    labs(x = \"$\\\\tau$\", y = \"$\\\\hat S_{s}(\\\\tau)$\", title = \"A\", linetype = \"\")\n\np_hat_S &lt;- quarter_S %&gt;%\n    filter(tau &lt;= 10) %&gt;%\n    # mutate(label = ifelse(tau %in% c(4,8), paste(round(S * 100, 1), \"\\\\%\") , NA)) %&gt;%\n    mutate(ord_fct = ordered(quarter)) %&gt;%\n    # relevel ord_fct to use levels(fct) &lt;- paste0(year(fct), \"-Q\", quarter(fct))\n    mutate(ord_fct = fct_relevel(ord_fct, paste0(year(ord_fct), \"-Q\", quarter(ord_fct)))) %&gt;%\n    mutate(group = as.numeric(ord_fct)) %&gt;%\n    mutate(x = tau + (group - 8) / 25) %&gt;%\n    ggplot(aes(x = x, y = S, color = group)) +\n    geom_point(size = .5) +\n    geom_segment(aes(x = x, xend = x, y = S, yend = 0), linewidth = .3) +\n    geom_hline(aes(yintercept = y, linetype = name), data = tibble(y = c(.05, .01), name = factor(2:3, labels = c(\"5\\\\%\", \"1\\\\%\")))) +\n    scale_linetype_manual(values = c(\"dashed\", \"dotted\")) +\n    scale_x_continuous(breaks = 0:10) +\n    scale_y_log10(breaks = c(.1, .01, .001), labels = c(\"0.1\", \"0.01\", \"0.001\")) +\n    labs(x = \"$\\\\tau$\", y = \"$\\\\hat S_q(\\\\tau)$\", color = \"\", linetype = \"\", title = \"B\") +\n    scale_color_viridis_c(breaks = c(2, 8, 14), labels = c(\"2020-Q3\", \"2022-Q1\", \"2023-Q2\")) +\n    guides(color = guide_colorbar())\n\np_boxplots / p_hat_S + plot_layout(heights = c(1, 1), guides = \"collect\")\n\nggsave_tikz(here(\"tikz/survival_function_rep_tri_incidences.tex\"))\n\nWarning message:\n“There were 3 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `ord_fct = fct_relevel(ord_fct, paste0(year(ord_fct), \"-Q\",\n  quarter(ord_fct)))`.\nCaused by warning:\n! tz(): Don't know how to compute timezone for object of class ordered/factor; returning \"UTC\".\nℹ Run `dplyr::last_dplyr_warnings()` to see the 2 remaining warnings.”\nWarning message in scale_y_log10(breaks = c(0.1, 0.01, 0.001), labels = c(\"0.1\", :\n“log-10 transformation introduced infinite values.”\nWarning message in scale_y_log10(breaks = c(0.1, 0.01, 0.001), labels = c(\"0.1\", :\n“log-10 transformation introduced infinite values.”\n\n\npdf: 2",
    "crumbs": [
      "2 Epidemiology",
      "2.3 Available data and its quality"
    ]
  },
  {
    "objectID": "2 Epidemiology/03_data.html#rki-hospitalization-data",
    "href": "2 Epidemiology/03_data.html#rki-hospitalization-data",
    "title": "2.3 Available data and its quality",
    "section": "RKI Hospitalization data",
    "text": "RKI Hospitalization data\nDenote by \\[\n    H^{a}_{s,t}\n\\] the number of hospitalisations in age-group \\(a\\) whose case reporting date is \\(s\\) and whose hospitalization reporting date is \\(t\\).\n\nhospitalisations_raw &lt;- read_csv(here(\"data/raw/all_hosp_age.csv\")) %&gt;%\n    select(-location) %&gt;%\n    rename(a = age_group, s = case_date, t = hosp_date, H = value)\n\n\ndf_hosp_plot &lt;- hospitalisations_raw %&gt;%\n    filter(s &gt;= ymd(\"2021-12-01\"), s &lt; ymd(\"2022-01-01\")) %&gt;%\n    filter(t &gt;= ymd(\"2021-12-01\"), t &lt; ymd(\"2022-01-01\")) %&gt;%\n    group_by(s, t) %&gt;%\n    summarize(H = sum(H))\n\np_reptri_hosp &lt;- df_hosp_plot %&gt;%\n    ggplot(aes(s, t, fill = H)) +\n    geom_tile() +\n    scale_fill_viridis_c(trans = \"log10\") +\n    labs(fill = \"$H_{s,t}$\", x = \"$s$\", y = \"$t$\")\n\np_marginal_hosp &lt;- df_hosp_plot %&gt;%\n    ggplot(aes(s, H, color = t, group = t)) +\n    geom_line() +\n    theme(axis.title.x = element_blank(), axis.ticks.x = element_blank(), axis.text.x = element_blank()) +\n    labs(x = \"\", y = \"$H_{s,t}$\", color = \"data date\") +\n    scale_color_viridis_c(option = \"H\", trans = \"date\")\n\np_marginal_hosp / p_reptri_hosp\n\n\n\n\n\n\n\n\n\n(((p_maginal_case + ggtitle(\"A\")) / p_reptri_case) | ((p_marginal_hosp + ggtitle(\"B\")) / p_reptri_hosp)) + plot_layout(heights = c(1, 2), guides = \"collect\")\n\nggsave_tikz(here(\"tikz/reporting_delays_cases.tex\"))\n\npdf: 2\n\n\n\n\n\n\n\n\n\n\nrep_tri_hosp_cummax &lt;- hospitalisations_raw %&gt;%\n    arrange(t) %&gt;%\n    group_by(s, a) %&gt;%\n    mutate(\n        H_tilde = pmin(cummax(H), tail(H, 1)),\n        h = H_tilde - lag(H_tilde, default = 0),\n    ) %&gt;%\n    ungroup() %&gt;%\n    mutate(tau = t - s)\n\n\np_s_plus_tau_constant &lt;- rep_tri_hosp_cummax %&gt;%\n    filter(tau &lt;= 31) %&gt;%\n    filter(s &gt;= ymd(\"2021-12-01\")) %&gt;%\n    filter(s &lt; ymd(\"2022-01-01\")) %&gt;%\n    group_by(s, tau) %&gt;%\n    summarize(h = sum(h)) %&gt;%\n    ggplot(aes(s, tau, fill = h)) +\n    geom_tile() +\n    # facet_wrap(~a) +\n    scale_fill_viridis_c(trans = \"log10\", na.value = \"grey80\") +\n    coord_fixed() +\n    labs(title = \"B\", x = \"reporting date $s$\", y = \"hosp. delay $\\\\tau$ [days]\", fill = \"$h_{s, \\\\tau}$\") +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 10))\n\np_delayed_reporting_double &lt;- rep_tri_hosp_cummax %&gt;%\n    filter(tau &gt; 0, tau &lt; 5) %&gt;%\n    filter(s &gt;= ymd(\"2021-11-30\")) %&gt;%\n    filter(s &lt; ymd(\"2022-01-01\")) %&gt;%\n    group_by(s, tau) %&gt;%\n    summarize(h = sum(h)) %&gt;%\n    mutate(tau = factor(tau)) %&gt;%\n    ggplot(aes(s, y = h, color = tau, group = tau)) +\n    geom_line() +\n    labs(color = \"$\\\\tau$\", x = \"\", y = \"increment $h_{s, \\\\tau}$\", title = \"A\") +\n    # theme(axis.title.x = element_blank(), axis.ticks.x = element_blank(), axis.text.x = element_blank())\n    theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 10))\n\np_survival &lt;- rep_tri_hosp_cummax %&gt;%\n    filter(s == ymd(\"2021-12-01\")) %&gt;%\n    group_by(a) %&gt;%\n    mutate(cum_p = cumsum(h) / sum(h)) %&gt;%\n    ungroup() %&gt;%\n    ggplot(aes(tau, 1 - cum_p, color = a)) +\n    geom_vline(xintercept = 28 * seq(4), linetype = 2, color = \"gray\") +\n    geom_step() +\n    coord_cartesian(xlim = c(NA, 120), ylim = c(0, .2)) +\n    labs(x = \"hospitalization delay $\\\\tau$ [days]\", y = \"empirical survival function $\\\\hat S^a(\\\\tau)$\", title = \"C\") +\n    scale_color_discrete(name = \"\") + # ) +labels = as_labeller(age_group_labels)) +\n    annotate(\"text\", x = 28 * seq(4) + 8, y = .205, label = paste0(seq(4), \" weeks\"), size = 2, color = \"gray\")\n\n((p_delayed_reporting_double / p_s_plus_tau_constant)) | p_survival\n\nggsave_tikz(here(\"tikz/double_weekday_effect_hosp.tex\"))\n\nDon't know how to automatically pick scale for object of type &lt;difftime&gt;.\nDefaulting to continuous.\nWarning message in scale_fill_viridis_c(trans = \"log10\", na.value = \"grey80\"):\n“log-10 transformation introduced infinite values.”\nDon't know how to automatically pick scale for object of type &lt;difftime&gt;.\nDefaulting to continuous.\nDon't know how to automatically pick scale for object of type &lt;difftime&gt;.\nDefaulting to continuous.\nWarning message in scale_fill_viridis_c(trans = \"log10\", na.value = \"grey80\"):\n“log-10 transformation introduced infinite values.”\nDon't know how to automatically pick scale for object of type &lt;difftime&gt;.\nDefaulting to continuous.\n\n\npdf: 2",
    "crumbs": [
      "2 Epidemiology",
      "2.3 Available data and its quality"
    ]
  },
  {
    "objectID": "2 Epidemiology/03_data.html#tests",
    "href": "2 Epidemiology/03_data.html#tests",
    "title": "2.3 Available data and its quality",
    "section": "Tests",
    "text": "Tests\n\n# add leading zeros to week, i.e. W2-1 -&gt; W02-1\nstr_replace(\"2021-W2-1\", \"W([0-9])-\", \"W0\\\\1-\")\n\n'2021-W02-1'\n\n\n\nlibrary(ISOweek)\ntests &lt;- read_csv(\"https://github.com/robert-koch-institut/SARS-CoV-2-PCR-Testungen_in_Deutschland/raw/main/SARS-CoV-2-PCR-Testungen_in_Deutschland.csv\")\n\ntests %&gt;%\n    mutate(date = paste0(date, \"-1\")) %&gt;%\n    mutate(date = str_replace(date, \"W([0-9])-\", \"W0\\\\1-\")) %&gt;%\n    mutate(date = ISOweek2date(date)) %&gt;%\n    ggplot(aes(date, tests_positive_ratio * 100)) +\n    geom_line()\n\ntests %&gt;%\n    mutate(date = paste0(date, \"-1\")) %&gt;%\n    mutate(date = str_replace(date, \"W([0-9])-\", \"W0\\\\1-\")) %&gt;%\n    mutate(date = ISOweek2date(date)) %&gt;%\n    ggplot(aes(date, tests_total)) +\n    geom_line()",
    "crumbs": [
      "2 Epidemiology",
      "2.3 Available data and its quality"
    ]
  },
  {
    "objectID": "2 Epidemiology/03_data.html#generation-time-distribution-used",
    "href": "2 Epidemiology/03_data.html#generation-time-distribution-used",
    "title": "2.3 Available data and its quality",
    "section": "Generation time distribution used",
    "text": "Generation time distribution used\n\nw &lt;- c((0:3) / 3, 1, (5:1) / 5)\nw &lt;- w / sum(w)\n\nmean_w &lt;- sum(seq_along(w) * w)\n\ntibble(tau = seq_along(w), w = w) %&gt;%\n    ggplot(aes(tau, w)) +\n    geom_point(size = 4) +\n    geom_segment(aes(xend = tau, yend = 0)) +\n    geom_vline(data = tibble(mean = mean_w), mapping = aes(xintercept = mean, linetype = \"$\\\\bar w$\")) +\n    scale_x_continuous(breaks = 0:length(w)) +\n    scale_linetype_manual(values = \"dashed\") +\n    labs(x = \"$\\\\tau$\", y = \"$w_\\\\tau$\", linetype = \"\")\n\nggsave_tikz(here(\"tikz/generation_time.tex\"), height = 3)\nwrite_csv(tibble(tau = seq_along(w), w = w), here(\"data/processed/generation_time.csv\"))\n\npdf: 2",
    "crumbs": [
      "2 Epidemiology",
      "2.3 Available data and its quality"
    ]
  },
  {
    "objectID": "3 SSMs/04_is_figures.html",
    "href": "3 SSMs/04_is_figures.html",
    "title": "3.4 Importance sampling: Figures",
    "section": "",
    "text": "library(tidyverse)\nlibrary(here)\nsource(here(\"code/setup.R\"))\n\n\nw &lt;- function(x, eps) 1 / 2 * (1 + dnorm(x, 0, eps^(-2)) / dnorm(x))\ness &lt;- function(x, eps) sum(w(x, eps))^2 / sum(w(x, eps)^2)\nef &lt;- function(x, eps) ess(x, eps) / length(x) * 100\n\nM &lt;- 100\nNs &lt;- c(100, 1000, 10000)\neps2 &lt;- c(.01, .1, .5)\nset.seed(454334534)\nsimulations &lt;- \nexpand_grid(eps2 = eps2, N = Ns) %&gt;%\n    group_by(eps2, N) %&gt;%\n    mutate(ef = list(replicate(M, ef(rnorm(N), sqrt(eps2))))) %&gt;%\n    unnest(cols = c(ef)) \n\nsimulations %&gt;%\n    ggplot(aes(N, ef, color = factor(eps2), group = interaction(eps2, N))) +\n    geom_boxplot() +\n    scale_x_log10(breaks = Ns) +\n    ylim(0, 100) +\n    labs(x = \"N\", y = \"EF[\\\\%]\", color = \"$\\\\varepsilon^2$\")\n\nggsave_tikz(here(\"tikz/ess_failure.tex\"), width = 6, height = 4)\nsimulations %&gt;% \n    filter(N == 1000, eps2 == .5) %&gt;%\n    pull(ef) %&gt;%\n    summary()\n\npdf: 2\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.3307 41.5327 58.5151 53.3383 71.1417 83.3823",
    "crumbs": [
      "3 SSMs",
      "3.4 Importance sampling: Figures"
    ]
  },
  {
    "objectID": "3 SSMs/04_is_figures.html#example-3.2-failure-of-ess",
    "href": "3 SSMs/04_is_figures.html#example-3.2-failure-of-ess",
    "title": "3.4 Importance sampling: Figures",
    "section": "",
    "text": "library(tidyverse)\nlibrary(here)\nsource(here(\"code/setup.R\"))\n\n\nw &lt;- function(x, eps) 1 / 2 * (1 + dnorm(x, 0, eps^(-2)) / dnorm(x))\ness &lt;- function(x, eps) sum(w(x, eps))^2 / sum(w(x, eps)^2)\nef &lt;- function(x, eps) ess(x, eps) / length(x) * 100\n\nM &lt;- 100\nNs &lt;- c(100, 1000, 10000)\neps2 &lt;- c(.01, .1, .5)\nset.seed(454334534)\nsimulations &lt;- \nexpand_grid(eps2 = eps2, N = Ns) %&gt;%\n    group_by(eps2, N) %&gt;%\n    mutate(ef = list(replicate(M, ef(rnorm(N), sqrt(eps2))))) %&gt;%\n    unnest(cols = c(ef)) \n\nsimulations %&gt;%\n    ggplot(aes(N, ef, color = factor(eps2), group = interaction(eps2, N))) +\n    geom_boxplot() +\n    scale_x_log10(breaks = Ns) +\n    ylim(0, 100) +\n    labs(x = \"N\", y = \"EF[\\\\%]\", color = \"$\\\\varepsilon^2$\")\n\nggsave_tikz(here(\"tikz/ess_failure.tex\"), width = 6, height = 4)\nsimulations %&gt;% \n    filter(N == 1000, eps2 == .5) %&gt;%\n    pull(ef) %&gt;%\n    summary()\n\npdf: 2\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.3307 41.5327 58.5151 53.3383 71.1417 83.3823",
    "crumbs": [
      "3 SSMs",
      "3.4 Importance sampling: Figures"
    ]
  },
  {
    "objectID": "3 SSMs/compare_ce_eis_mse.html#imports",
    "href": "3 SSMs/compare_ce_eis_mse.html#imports",
    "title": "3.9 Comparison of importance sampling methods",
    "section": "imports",
    "text": "imports\n\nimport jax\nfrom jax import vmap\nimport jax.numpy as jnp\nimport jax.random as jrn\nfrom functools import partial\nfrom jaxtyping import Array, Float\nfrom pyprojroot.here import here\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom isssm.lcssm import nb_lcssm, simulate_lcssm\nfrom isssm.mode_estimation import mode_estimation\nfrom isssm.modified_efficient_importance_sampling import modified_efficient_importance_sampling\nfrom isssm.ce_method import (\n    ce_cholesky_precision, \n    marginals,\n    simulate as simulate_ce,\n    ce_log_weights,\n    forward_model_markov_process\n)\nfrom isssm.importance_sampling import ess_pct\nfrom isssm.importance_sampling import lcssm_importance_sampling\nfrom isssm.kalman import kalman, smoother\nfrom tensorflow_probability.substrates.jax.distributions import WishartTriL as Wishart\nfrom isssm.lcssm import poisson_lcssm\nfrom isssm.kalman import kalman\nfrom isssm.importance_sampling import log_weights_t\nfrom isssm.glssm import vmatmul\nfrom tensorflow_probability.substrates.jax.distributions import (\n    MultivariateNormalFullCovariance as MVN,\n)\nfrom ipywidgets import IntProgress\nfrom IPython.display import display\n\nkey = jrn.PRNGKey(342234)\n\n2024-05-23 10:50:14.837516: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-23 10:50:14.837540: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-23 10:50:14.837546: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered",
    "crumbs": [
      "3 SSMs",
      "3.9 Comparison of importance sampling methods"
    ]
  },
  {
    "objectID": "3 SSMs/compare_ce_eis_mse.html#efficient-calculation-of-weights",
    "href": "3 SSMs/compare_ce_eis_mse.html#efficient-calculation-of-weights",
    "title": "3.9 Comparison of importance sampling methods",
    "section": "Efficient calculation of weights",
    "text": "Efficient calculation of weights\n\ndef log_weights_ssm(y, x0, A, B, Sigma, z, Omega, dist, xi, N, key):\n    \"\"\"Calculate the efficiency factor while sampling only the marginal distributions, keeping memory impact minimal\"\"\"\n    x_filt, Xi_filt, x_pred, Xi_pred = kalman(z, x0, Sigma, Omega, A, B)\n\n    def _sample_and_weights(carry, inputs):\n        X_smooth_next, log_w, key = carry\n        x_filt, Xi_filt, Xi_pred, A_t, B_t, y_t, xi_t, z_t, Omega_t = inputs\n\n        G = Xi_filt @ jnp.linalg.solve(Xi_pred, A_t).T\n\n        cond_expectation = x_filt + vmatmul(G, X_smooth_next - (A_t @ x_filt)[None])\n        cond_covariance = Xi_filt - G @ Xi_pred @ G.T\n\n        key, subkey = jrn.split(key)\n        new_samples = MVN(cond_expectation, cond_covariance).sample(seed=subkey)\n\n        s_t = vmap(jnp.matmul, (None, 0))(B_t, new_samples)\n        log_w = log_w + vmap(\n            partial(\n                log_weights_t, y_t=y_t, xi_t=xi_t, dist=dist, z_t=z_t, Omega_t=Omega_t\n            )\n        )(s_t)\n\n        return (new_samples, log_w, key), jnp.empty((0,))\n\n    key, subkey = jrn.split(key)\n\n    initial_samples = MVN(x_filt[-1], Xi_filt[-1]).sample(N, seed=subkey)\n    initial_signals = vmap(jnp.matmul, (None, 0))(B[-1], initial_samples)\n    initial_log_weights = vmap(\n        partial(\n            log_weights_t,\n            y_t=y[-1],\n            xi_t=xi[-1],\n            dist=dist,\n            z_t=z[-1],\n            Omega_t=Omega[-1],\n        )\n    )(initial_signals)\n\n    (_, log_weights, _), _ = jax.lax.scan(\n        _sample_and_weights,\n        (initial_samples, initial_log_weights, subkey),\n        (\n            x_filt[:-1],\n            Xi_filt[:-1],\n            Xi_pred[1:],\n            A,\n            B[:-1],\n            y[:-1],\n            xi[:-1],\n            z[:-1],\n            Omega[:-1],\n        ),\n        reverse=True,\n    )\n\n    return log_weights\n\n#ess_pct(log_weights_ssm(Y, x0, A_t, B, Sigma, z_la, Omega_la, dist, xi, N_true, subkey)), results_la.mean()[\"ef\"]",
    "crumbs": [
      "3 SSMs",
      "3.9 Comparison of importance sampling methods"
    ]
  },
  {
    "objectID": "3 SSMs/compare_ce_eis_mse.html#simulation-code",
    "href": "3 SSMs/compare_ce_eis_mse.html#simulation-code",
    "title": "3.9 Comparison of importance sampling methods",
    "section": "simulation code",
    "text": "simulation code\n\ndef eis_marginals(x0, A, Sigma, B, z, Omega):\n    x_filt, Xi_filt, x_pred, Xi_pred = kalman(z, x0, Sigma, Omega, A, B)\n    x_smooth, Xi_smooth = smoother(x_filt, Xi_filt, x_pred, Xi_pred, A)\n    \n    marg_means = x_smooth\n    marg_vars = vmap(jnp.diag)(Xi_smooth)\n\n    return marg_means, marg_vars\n\ndef eis_single_outcome(key, N, Y, x0, A, Sigma, B, dist, xi, z_la, Omega_la):\n    key, subkey = jrn.split(key)\n    z_eis, Omega_eis = modified_efficient_importance_sampling(\n        Y, x0, A, Sigma, B, xi, dist, z_la, Omega_la, 1, N, subkey\n    )\n\n    key, subkey = jrn.split(key)\n    _, log_weights = lcssm_importance_sampling(\n        Y, x0, A, Sigma, B, dist, xi, z_eis, Omega_eis, N, subkey\n    )\n\n    return *(eis_marginals(x0, A, Sigma, B, z_eis, Omega_eis)), ess_pct(log_weights)\n\ndef ce_single_outcome(key, N, Y, x0, A, Sigma, B, dist, xi, z_la, Omega_la):\n    key, subkey = jrn.split(key)\n    \n    samples, log_weights = lcssm_importance_sampling(Y, x0, A, Sigma, B, dist, xi, z_la, Omega_la, N, subkey)\n\n    initial_mean, (initial_diag, initial_off_diag) = forward_model_markov_process(z_la, x0, A, B, Sigma, Omega_la)\n\n    (diag, off_diag, mean), _, _ = ce_cholesky_precision(Y, x0, A, Sigma, B, xi, dist, initial_mean, initial_diag, initial_off_diag, 1, N, subkey)\n\n    key, subkey = jrn.split(key)\n    samples = simulate_ce(\n        diag, off_diag, subkey, N\n    )\n\n    log_weights = vmap(partial(ce_log_weights,\n        y=Y, full_diag=diag, off_diag=off_diag, mean=mean, x0=x0, A=A, Sigma=Sigma, B=B, dist=dist, xi=xi\n    ))(samples)\n\n    return *(marginals(mean, diag, off_diag)), ess_pct(log_weights)\n\ndef bias_var(\n    samples: Float[Array, \"M ...\"],\n    true: Float[Array, \"...\"]\n):\n    bias_sr = (samples - true)**2\n    var = (samples - samples.mean(axis=0)) ** 2\n    return bias_sr, var\n\ndef eis_outcomes(N, key, M, true_z, true_Omega, Y, x0, A, Sigma, B, dist, xi, z_la, Omega_la):\n    key, *subkeys = jrn.split(key, M+1)\n    subkeys = jnp.array(subkeys)\n\n    true_mean, true_var = eis_marginals(x0, A, Sigma, B, true_z, true_Omega)\n\n    single_run = partial(eis_single_outcome, Y=Y, N=N, x0=x0, A=A, Sigma=Sigma, B=B, dist=dist, xi=xi, z_la=z_la, Omega_la=Omega_la)\n    marg_means, marg_vars, ef = vmap(single_run)(subkeys)\n        \n    mean_bias, mean_var = bias_var(marg_means, true_mean)\n    var_bias, var_var = bias_var(marg_vars, true_var)\n\n    return pd.DataFrame({\n        \"N\": int(N),\n        \"N_mse\": M,\n        \"mean_bias\": mean_bias.mean(axis=(-2, -1)),\n        \"mean_var\": mean_var.mean(axis=(-2,-1)),\n        \"var_bias\": var_bias.mean(axis=(-2,-1)),\n        \"var_var\": var_var.mean(axis=(-2,-1)),\n        \"ef\": ef,\n    })\n\ndef ce_outcomes(N, key, M, true_mean, true_diag, true_off_diag, Y, x0, A, Sigma, B, dist, xi, z_la, Omega_la):\n    key, *subkeys = jrn.split(key, M+1)\n    subkeys = jnp.array(subkeys)\n\n    true_mean, true_var = marginals(true_mean, true_diag, true_off_diag)\n\n    single_run = partial(ce_single_outcome, N=N, Y=Y,x0=x0, A=A, Sigma=Sigma, B=B, dist=dist, xi=xi, z_la=z_la, Omega_la=Omega_la)\n\n    marg_means, marg_vars, ef = vmap(single_run)(subkeys)\n        \n    mean_bias, mean_var = bias_var(marg_means, true_mean)\n    var_bias, var_var = bias_var(marg_vars, true_var)\n\n    return pd.DataFrame({\n        \"N\": int(N),\n        \"N_mse\": M,\n        \"mean_bias\": mean_bias.mean(axis=(-2,-1)),\n        \"mean_var\": mean_var.mean(axis=(-2,-1)),\n        \"var_bias\": var_bias.mean(axis=(-2,-1)),\n        \"var_var\": var_var.mean(axis=(-2,-1)),\n        \"ef\": ef,\n    })",
    "crumbs": [
      "3 SSMs",
      "3.9 Comparison of importance sampling methods"
    ]
  },
  {
    "objectID": "3 SSMs/compare_ce_eis_mse.html#setup",
    "href": "3 SSMs/compare_ce_eis_mse.html#setup",
    "title": "3.9 Comparison of importance sampling methods",
    "section": "setup",
    "text": "setup\n\nx0 = jnp.zeros(1)\nn = 20\nalpha = .5\nA_t = alpha * jnp.tile(jnp.eye(1)[None,:,:], (n, 1, 1))\nSigma = (1 - alpha ** 2) * jnp.tile(jnp.eye(1)[None,:,:], (n + 1, 1, 1))\n\n# NB observations \n\nnb_model = nb_lcssm(\n    x0,\n    A_t,\n    Sigma,\n    jnp.tile(jnp.eye(1)[None, :,:], (n + 1, 1, 1)),\n    .1\n)\nx0, A_t, Sigma, B, dist, xi = nb_model\n\nkey, subkey = jrn.split(key)\n(X,), (Y,) = simulate_lcssm(*nb_model, 1, subkey)\n\ns_init = vmap(jnp.dot)(B, X)\nX_smooth, z_la, Omega_la = mode_estimation(Y, x0, A_t, Sigma, B, dist, xi, s_init, 10)\n\n# more will result in OOMs\nN_true = int(1.5 * 1e6)\nM = 100\nNs = jnp.logspace(1,3.5,10).astype(jnp.int32)",
    "crumbs": [
      "3 SSMs",
      "3.9 Comparison of importance sampling methods"
    ]
  },
  {
    "objectID": "3 SSMs/compare_ce_eis_mse.html#la-simulation",
    "href": "3 SSMs/compare_ce_eis_mse.html#la-simulation",
    "title": "3.9 Comparison of importance sampling methods",
    "section": "LA simulation",
    "text": "LA simulation\n\nkey, subkey_run = jrn.split(key)\n\ndef la_single_outcome(key, N, Y, x0, A, Sigma, B, dist, xi, z, Omega):\n    _, log_weights = lcssm_importance_sampling(Y, x0, A, Sigma, B, dist, xi, z, Omega, N, key)\n    return ess_pct(log_weights)\n\ndef la_outcomes(N, key, M, Y, x0, A, Sigma, B, dist, xi, z_la, Omega_la):\n    key, *subkeys = jrn.split(key, M+1)\n    subkeys = jnp.array(subkeys)\n\n    single_run = partial(la_single_outcome, N=N, Y=Y,x0=x0, A=A, Sigma=Sigma, B=B, dist=dist, xi=xi, z=z_la, Omega=Omega_la)\n\n    ef = vmap(single_run)(subkeys)\n\n    return pd.DataFrame({\n        \"N\": int(N),\n        \"N_mse\": M,\n        \"mean_bias\": None,\n        \"mean_var\": None,\n        \"var_bias\": None,\n        \"var_var\": None,\n        \"ef\": ef,\n    })\n\nresults_la = pd.concat([\n    la_outcomes(N, subkey_run, M, Y, x0, A_t, Sigma, B, dist, xi, z_la, Omega_la)\n    for N in Ns\n]).reset_index(drop=True)",
    "crumbs": [
      "3 SSMs",
      "3.9 Comparison of importance sampling methods"
    ]
  },
  {
    "objectID": "3 SSMs/compare_ce_eis_mse.html#eis-simulation",
    "href": "3 SSMs/compare_ce_eis_mse.html#eis-simulation",
    "title": "3.9 Comparison of importance sampling methods",
    "section": "EIS simulation",
    "text": "EIS simulation\n\nkey, subkey = jrn.split(key)\ntrue_z_eis, true_omega_eis = modified_efficient_importance_sampling(\n    Y, x0, A_t, Sigma, B, xi, dist, z_la, Omega_la, 1, N_true, subkey,\n)\n\n\nsingle_N_run_eis = partial(\n    eis_outcomes, \n    Y=Y, key=subkey_run, M=M, true_z=true_z_eis, true_Omega=true_omega_eis, x0=x0, A=A_t, Sigma=Sigma, B=B, dist=dist, xi=xi, z_la=z_la, Omega_la=Omega_la\n)\n\nresults_eis = pd.concat([\n    single_N_run_eis(N) for N in Ns\n]).reset_index(drop=True)",
    "crumbs": [
      "3 SSMs",
      "3.9 Comparison of importance sampling methods"
    ]
  },
  {
    "objectID": "3 SSMs/compare_ce_eis_mse.html#ce-simulation",
    "href": "3 SSMs/compare_ce_eis_mse.html#ce-simulation",
    "title": "3.9 Comparison of importance sampling methods",
    "section": "CE simulation",
    "text": "CE simulation\n\nkey, subkey = jrn.split(key)\nsamples_la, log_weights_la = lcssm_importance_sampling(\n    Y, x0, A_t, Sigma, B, dist, xi, z_la, Omega_la, N_true, subkey\n)\n\nkey, subkey = jrn.split(key)\ninitial_mean, (initial_diag, initial_off_diag) = forward_model_markov_process(z_la, x0, A_t, B, Sigma, Omega_la)\n# getting XLA error otherwise\nwith jax.default_device(jax.devices(\"cpu\")[0]):\n    (true_diag, true_off_diag, true_mu), (_, log_weights_ce), _ = ce_cholesky_precision(\n        Y, x0, A_t, Sigma, B, xi, dist, initial_mean, initial_diag, initial_off_diag, 1, N_true, subkey\n    )\n\n\nsingle_N_run_ce = partial(\n    ce_outcomes, \n    Y=Y, key=subkey_run, N_mse=M, true_mean=true_mu, true_diag=true_diag, true_off_diag=true_off_diag, x0=x0, A=A_t, Sigma=Sigma, B=B, dist=dist, xi=xi, z_la=z_la, Omega_la=Omega_la\n)\n\nresults_ce = pd.concat([\n    single_N_run_ce(N) for N in Ns\n]).reset_index(drop=True)",
    "crumbs": [
      "3 SSMs",
      "3.9 Comparison of importance sampling methods"
    ]
  },
  {
    "objectID": "3 SSMs/compare_ce_eis_mse.html#inspect-true-solutions",
    "href": "3 SSMs/compare_ce_eis_mse.html#inspect-true-solutions",
    "title": "3.9 Comparison of importance sampling methods",
    "section": "inspect true solutions",
    "text": "inspect true solutions\n\n_, log_weights_eis = lcssm_importance_sampling(Y, x0, A_t, Sigma, B, dist, xi, true_z_eis, true_omega_eis, N_true, subkey)\ness_pct(log_weights_la), ess_pct(log_weights_ce), ess_pct(log_weights_eis)\n\n(Array(99.18389385, dtype=float64),\n Array(99.77554809, dtype=float64),\n Array(99.78012264, dtype=float64))\n\n\n\nce_true_mu, ce_true_var = marginals(true_mu, true_diag, true_off_diag)\neis_true_mu, eis_true_var = eis_marginals(x0, A_t, Sigma, B, true_z_eis, true_omega_eis)\nla_mu, la_var = eis_marginals(x0, A_t, Sigma, B, z_la, Omega_la)\n\n\nfig, axs = plt.subplots(1,2, figsize=(14,6))\n\naxs[0].set_title(\"Mean\")\naxs[0].plot(la_mu[:,0], label=\"LA\")\naxs[0].plot(ce_true_mu[:,0], label=\"CE\")\naxs[0].plot(eis_true_mu[:,0], label=\"EIS\")\naxs[0].legend()\naxs[1].set_title(\"marginal variances\")\naxs[1].plot(la_var[:,0], label=\"LA\")\naxs[1].plot(ce_true_var[:,0], label=\"CE\")\naxs[1].plot(eis_true_var[:,0], label=\"EIS\")\naxs[1].legend()\nplt.show()",
    "crumbs": [
      "3 SSMs",
      "3.9 Comparison of importance sampling methods"
    ]
  },
  {
    "objectID": "3 SSMs/compare_ce_eis_mse.html#simulation-results",
    "href": "3 SSMs/compare_ce_eis_mse.html#simulation-results",
    "title": "3.9 Comparison of importance sampling methods",
    "section": "simulation results",
    "text": "simulation results\n\nimport seaborn as sns\nresults_eis\n\n\n\n\n\n\n\n\nN\nN_mse\nmean_bias\nmean_var\nvar_bias\nvar_var\nef\n\n\n\n\n0\n10\n100\n2.493071e-04\n9.245159e-05\n1.601086e-04\n1.737175e-04\n99.526637\n\n\n1\n10\n100\n2.039639e-04\n1.445876e-05\n7.675667e-05\n7.279853e-05\n99.883174\n\n\n2\n10\n100\n1.813451e-04\n3.406173e-05\n9.023990e-05\n7.823291e-05\n99.813548\n\n\n3\n10\n100\n1.874421e-04\n1.589622e-05\n1.033920e-04\n1.200959e-04\n99.879167\n\n\n4\n10\n100\n3.143126e-04\n4.965315e-05\n1.233423e-04\n1.303480e-04\n96.281030\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n3162\n100\n2.874495e-07\n2.178808e-07\n1.267212e-06\n1.210763e-06\n99.772152\n\n\n996\n3162\n100\n1.912870e-08\n2.473438e-08\n1.308759e-06\n1.037496e-06\n99.774445\n\n\n997\n3162\n100\n1.843290e-07\n1.542400e-07\n1.603958e-06\n1.537533e-06\n99.782021\n\n\n998\n3162\n100\n9.460597e-08\n7.615242e-08\n7.292288e-07\n7.513534e-07\n99.781592\n\n\n999\n3162\n100\n4.352821e-07\n3.879304e-07\n4.281331e-07\n5.694264e-07\n99.783446\n\n\n\n\n1000 rows × 7 columns\n\n\n\n\nresults = pd.concat([\n    results_la.assign(method=\"LA\"),\n    results_eis.assign(method=\"EIS\"), \n    results_ce.assign(method=\"CE\")\n])\nresults.to_csv(here() / \"data/figures/03_state_space_models/compare_ce_eis_mse.csv\", index=False)\n\n/tmp/ipykernel_2164959/409058690.py:1: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  results = pd.concat([\n\n\n\nresults_eis[\"N\"] = results_eis[\"N\"].astype(jnp.float64)\nsns.boxplot(results_eis, x=\"N\", y=\"ef\")",
    "crumbs": [
      "3 SSMs",
      "3.9 Comparison of importance sampling methods"
    ]
  },
  {
    "objectID": "3 SSMs/compare_ce_eis_mse.html#optimum-analysis-efficiency-factors",
    "href": "3 SSMs/compare_ce_eis_mse.html#optimum-analysis-efficiency-factors",
    "title": "3.9 Comparison of importance sampling methods",
    "section": "Optimum analysis: efficiency factors",
    "text": "Optimum analysis: efficiency factors\n\ndf_ef = pd.DataFrame({\"N\": {}, \"method\": {}, \"ef\": {}})\n(n_iter_la, n_iter_eis, n_iter_ce) = (10, 10, 10)\n\nkey, *subkeys_large = jrn.split(key, 3)\nsubkeys_large = jnp.array(subkeys_large)\n\nfor subkey in subkeys_large:\n    n = 10\n    N_true = int(1.5 * 1e6 / n)\n    x0 = jnp.zeros(1)\n    alpha = 0.5\n    A = alpha * jnp.tile(jnp.eye(1)[None, :, :], (n, 1, 1))\n    Sigma = (1 - alpha**2) * jnp.tile(jnp.eye(1)[None, :, :], (n + 1, 1, 1))\n\n    # NB observations\n    model = nb_lcssm(x0, A, Sigma, jnp.tile(jnp.eye(1)[None, :, :], (n + 1, 1, 1)), 0.1)\n    # model = poisson_lcssm(\n    #    x0,\n    #    A,\n    #    Sigma,\n    #    jnp.tile(jnp.eye(1)[None, :,:], (n + 1, 1, 1)),\n    # )\n    x0, A, Sigma, B, dist, xi = model\n\n    key, subkey = jrn.split(key)\n    (X,), (Y,) = simulate_lcssm(*model, 1, subkey)\n    X_smooth, z_la, Omega_la = mode_estimation(\n        Y, x0, A, Sigma, B, dist, xi, X, n_iter_la\n    )\n    true_z_eis, true_omega_eis = modified_efficient_importance_sampling(\n        Y,\n        x0,\n        A,\n        Sigma,\n        B,\n        xi,\n        dist,\n        z_la,\n        Omega_la,\n        n_iter_eis,\n        N_true,\n        subkey,\n    )\n\n    log_weights_eis = log_weights_ssm(Y, x0, A, B, Sigma, true_z_eis, true_omega_eis, dist, xi, N_true, subkey)\n\n    log_weights_la = log_weights_ssm(Y, x0, A, B, Sigma, z_la, Omega_la, dist, xi, N_true, subkey)\n    #_, log_weights_la = lcssm_importance_sampling(\n    #    Y, x0, A, Sigma, B, dist, xi, z_la, Omega_la, N_true, subkey\n    #)\n\n    key, subkey = jrn.split(key)\n\n    initial_mean, (initial_diag, initial_off_diag) = forward_model_markov_process(\n        z_la, x0, A, B, Sigma, Omega_la\n    )\n    _, (_, log_weights_ce), _ = ce_cholesky_precision(\n        Y,\n        x0,\n        A,\n        Sigma,\n        B,\n        xi,\n        dist,\n        initial_mean,\n        initial_diag,\n        initial_off_diag,\n        n_iter_ce,\n        N_true,\n        subkey,\n    )\n\n    df_ef = pd.concat(\n        [\n            df_ef,\n            pd.DataFrame(\n                {\n                    \"N\": N_true,\n                    \"method\": [\"LA\", \"EIS\", \"CE\"],\n                    \"ef\": [\n                        ess_pct(log_weights_la),\n                        ess_pct(log_weights_eis),\n                        ess_pct(log_weights_ce),\n                    ],\n                }\n            ),\n        ]\n    )\n\ndf_ef.to_csv(here() / \"data/figures/03_state_space_models/ef_large_N.csv\", index=False)\n\n/tmp/ipykernel_2164959/2327031721.py:73: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n  df_ef = pd.concat(\n\n\n\ndf_state_dimensions = pd.DataFrame({\n    \"N\": {},\n    \"m\": {},\n    \"n\": {},\n    \"method\": {},\n    \"ef\": {}\n})\n\n(n_iter_la, n_iter_eis, n_iter_ce) = (100, 100, 100)\n\nM = 3\nkey, *subkeys_large = jrn.split(key, M + 1)\nsubkeys_large = jnp.array(subkeys_large)\n\nms = jnp.array([2,4,8,16,32])\nn = 10\n\nprogress_bar = IntProgress(min=0, max = M * len(ms))\ndisplay(progress_bar)\n\nfor j in range(M):\n    key = subkeys_large[j]\n    for m in ms:\n        progress_bar.description = f\"m={m}({j+1}/{M})\"\n\n        # prevent OOM, smaller N for larger n\n        N_true = int(1e6)\n        x0 = jnp.zeros(m)\n        alpha = .5\n        A = alpha * jnp.tile(jnp.eye(m)[None,:,:], (n, 1, 1))\n        key, subkey = jrn.split(key)\n        innovation_cov = Wishart(df=1.1 * m, scale_tril=jnp.eye(m)).sample(seed=subkey)\n        Sigma = 1/(1 - alpha ** 2) * jnp.tile(innovation_cov[None,:,:], (n + 1, 1, 1))\n\n        # NB observations \n        model = nb_lcssm(\n            x0,\n            A,\n            Sigma,\n            jnp.tile(jnp.eye(m)[None, :,:], (n + 1, 1, 1)),\n            .1\n        )\n        x0, A, Sigma, B, dist, xi = model\n\n        key, subkey = jrn.split(key)\n        (X,), (Y,) = simulate_lcssm(*model, 1, subkey)\n\n        s_init = vmap(jnp.dot)(B, X)\n        X_smooth, z_la, Omega_la = mode_estimation(Y, x0, A, Sigma, B, dist, xi, s_init, n_iter_la)\n\n        key, subkey = jrn.split(key)\n        true_z_eis, true_omega_eis = modified_efficient_importance_sampling(\n            Y, x0, A, Sigma, B, xi, dist, z_la, Omega_la, n_iter_ce, N_true, subkey,\n        )\n\n        key, subkey = jrn.split(key)\n\n        log_weights_la = log_weights_ssm(Y, x0, A, B, Sigma, z_la, Omega_la, dist, xi, N_true, subkey)\n        log_weights_eis = log_weights_ssm(Y, x0, A, B, Sigma, true_z_eis, true_omega_eis, dist, xi, N_true, subkey)\n        # large simulations don't fit on GPU (yet)\n        with jax.default_device(jax.devices(\"cpu\")[0]):\n            initial_mean, (initial_diag, initial_off_diag) = forward_model_markov_process(\n                z_la, x0, A, B, Sigma, Omega_la\n            )\n        #with jax.default_device(jax.devices(\"cpu\")[0]):\n            _, (_, log_weights_ce), _ = ce_cholesky_precision(\n                Y,\n                x0,\n                A,\n                Sigma,\n                B,\n                xi,\n                dist,\n                initial_mean,\n                initial_diag,\n                initial_off_diag,\n                n_iter_ce,\n                N_true,\n                subkey,\n            )\n\n        df_state_dimensions = pd.concat([\n                df_state_dimensions,\n                pd.DataFrame({\n                    \"N\": N_true,\n                    \"m\": m,\n                    \"j\": j,\n                    \"n\": n,\n                    \"method\": [\"LA\", \"EIS\", \"CE\"],\n                    \"ef\": [ess_pct(log_weights_la), ess_pct(log_weights_eis), ess_pct(log_weights_ce)]\n                })\n            ])\n        \n        df_state_dimensions.to_csv(here() / \"data/figures/03_state_space_models/ef_state_dimensions.csv\", index=False)\n        progress_bar.value += 1\n\n\n\n\n/tmp/ipykernel_2366657/867018030.py:81: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n  df_state_dimensions = pd.concat([\n\n\n\nfrom isssm.lcssm import poisson_lcssm\ndf_dimensions = pd.DataFrame({\n    \"N\": {},\n    \"m\": {},\n    \"n\": {},\n    \"method\": {},\n    \"ef\": {}\n})\n\n(n_iter_la, n_iter_eis, n_iter_ce) = (100, 100, 100)\n\nM = 100\nkey, *subkeys_large = jrn.split(key, M + 1)\nsubkeys_large = jnp.array(subkeys_large)\n\nns = jnp.array([10, 20, 40, 80, 160])\n\nfor j in range(M):\n    key = subkeys_large[j]\n    for n in ns:\n\n        # prevent OOM, smaller N for larger n\n        N_true = int(1e6)\n        x0 = jnp.zeros(1)\n        alpha = .5\n        A = alpha * jnp.tile(jnp.eye(1)[None,:,:], (n, 1, 1))\n        Sigma = (1 - alpha ** 2) * jnp.tile(jnp.eye(1)[None,:,:], (n + 1, 1, 1))\n\n        # NB observations \n        model = nb_lcssm(\n            x0,\n            A,\n            Sigma,\n            jnp.tile(jnp.eye(1)[None, :,:], (n + 1, 1, 1)),\n            .1\n        )\n        #model = poisson_lcssm(\n        #    x0,\n        #    A,\n        #    Sigma,\n        #    jnp.tile(jnp.eye(1)[None, :,:], (n + 1, 1, 1)),\n        #)\n        x0, A, Sigma, B, dist, xi = model\n\n        key, subkey = jrn.split(key)\n        (X,), (Y,) = simulate_lcssm(*model, 1, subkey)\n        n,m = X.shape\n\n        s_init = vmap(jnp.dot)(B, X)\n        X_smooth, z_la, Omega_la = mode_estimation(Y, x0, A, Sigma, B, dist, xi, s_init, n_iter_la)\n\n        key, subkey = jrn.split(key)\n        true_z_eis, true_omega_eis = modified_efficient_importance_sampling(\n            Y, x0, A, Sigma, B, xi, dist, z_la, Omega_la, n_iter_ce, N_true, subkey,\n        )\n\n        key, subkey = jrn.split(key)\n\n        log_weights_la = log_weights_ssm(Y, x0, A, B, Sigma, z_la, Omega_la, dist, xi, N_true, subkey)\n        log_weights_eis = log_weights_ssm(Y, x0, A, B, Sigma, true_z_eis, true_omega_eis, dist, xi, N_true, subkey)\n        # large simulations don't fit on GPU (yet)\n        with jax.default_device(jax.devices(\"cpu\")[0]):\n            initial_mean, (initial_diag, initial_off_diag) = forward_model_markov_process(\n                z_la, x0, A, B, Sigma, Omega_la\n            )\n        #with jax.default_device(jax.devices(\"cpu\")[0]):\n            _, (_, log_weights_ce), _ = ce_cholesky_precision(\n                Y,\n                x0,\n                A,\n                Sigma,\n                B,\n                xi,\n                dist,\n                initial_mean,\n                initial_diag,\n                initial_off_diag,\n                n_iter_ce,\n                N_true,\n                subkey,\n            )\n\n        df_dimensions = pd.concat([\n                df_dimensions,\n                pd.DataFrame({\n                    \"N\": N_true,\n                    \"m\": m,\n                    \"j\": j,\n                    \"n\": n,\n                    \"method\": [\"LA\", \"EIS\", \"CE\"],\n                    \"ef\": [ess_pct(log_weights_la), ess_pct(log_weights_eis), ess_pct(log_weights_ce)]\n                })\n            ])\n        \n        df_dimensions.to_csv(here() / \"data/figures/03_state_space_models/ef_dimensions.csv\", index=False)\n\n2024-05-23 07:14:47.758780: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-23 07:14:47.758875: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-23 07:14:47.758909: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\n\n\n---------------------------------------------------------------------------\n\nKeyboardInterrupt                         Traceback (most recent call last)\n\nCell In[1], line 1\n\n----&gt; 1 from isssm.lcssm import poisson_lcssm\n\n      2 df_dimensions = pd.DataFrame({\n\n      3     \"N\": {},\n\n      4     \"m\": {},\n\n   (...)\n\n      7     \"ef\": {}\n\n      8 })\n\n     10 (n_iter_la, n_iter_eis, n_iter_ce) = (100, 100, 100)\n\n\n\nFile ~/workspace/work/packages/isssm/isssm/lcssm.py:8\n\n      6 # %% ../nbs/20_lcssm.ipynb 5\n\n      7 from jaxtyping import Float, Array, PRNGKeyArray\n\n----&gt; 8 from isssm.glssm import log_probs_x\n\n     10 # %% ../nbs/20_lcssm.ipynb 6\n\n     11 from isssm.glssm import simulate_states\n\n\n\nFile ~/workspace/work/packages/isssm/isssm/glssm.py:13\n\n     11 from jaxtyping import Float, Array, PRNGKeyArray\n\n     12 from jax.lax import scan\n\n---&gt; 13 from tensorflow_probability.substrates.jax.distributions import MultivariateNormalFullCovariance as MVN\n\n     15 # %% ../nbs/00_glssm.ipynb 5\n\n     16 vmatmul = vmap(jnp.matmul, (None, 0))\n\n\n\nFile ~/.conda/envs/research/lib/python3.11/site-packages/tensorflow_probability/substrates/jax/__init__.py:41\n\n     39 from tensorflow_probability.python.version import __version__\n\n     40 # from tensorflow_probability.substrates.jax.google import staging  # DisableOnExport  # pylint:disable=line-too-long\n\n---&gt; 41 from tensorflow_probability.substrates.jax import bijectors\n\n     42 from tensorflow_probability.substrates.jax import distributions\n\n     43 from tensorflow_probability.substrates.jax import experimental\n\n\n\nFile ~/.conda/envs/research/lib/python3.11/site-packages/tensorflow_probability/substrates/jax/bijectors/__init__.py:19\n\n     15 \"\"\"Bijective transformations.\"\"\"\n\n     17 # pylint: disable=unused-import,wildcard-import,line-too-long,g-importing-member\n\n---&gt; 19 from tensorflow_probability.substrates.jax.bijectors.absolute_value import AbsoluteValue\n\n     20 from tensorflow_probability.substrates.jax.bijectors.ascending import Ascending\n\n     21 # from tensorflow_probability.substrates.jax.bijectors.batch_normalization import BatchNormalization\n\n\n\nFile ~/.conda/envs/research/lib/python3.11/site-packages/tensorflow_probability/substrates/jax/bijectors/absolute_value.py:17\n\n      1 # Copyright 2018 The TensorFlow Probability Authors.\n\n      2 #\n\n      3 # Licensed under the Apache License, Version 2.0 (the \"License\");\n\n   (...)\n\n     13 # limitations under the License.\n\n     14 # ============================================================================\n\n     15 \"\"\"AbsoluteValue bijector.\"\"\"\n\n---&gt; 17 from tensorflow_probability.python.internal.backend.jax.compat import v2 as tf\n\n     19 from tensorflow_probability.substrates.jax.bijectors import bijector\n\n     20 from tensorflow_probability.substrates.jax.internal import assert_util\n\n\n\nFile ~/.conda/envs/research/lib/python3.11/site-packages/tensorflow_probability/python/internal/backend/jax/__init__.py:18\n\n     15 \"\"\"Experimental Numpy backend.\"\"\"\n\n     17 from tensorflow_probability.python.internal.backend.jax import __internal__\n\n---&gt; 18 from tensorflow_probability.python.internal.backend.jax import bitwise\n\n     19 from tensorflow_probability.python.internal.backend.jax import compat\n\n     20 from tensorflow_probability.python.internal.backend.jax import config\n\n\n\nFile ~/.conda/envs/research/lib/python3.11/site-packages/tensorflow_probability/python/internal/backend/jax/bitwise.py:19\n\n     15 \"\"\"Numpy bitwise ops.\"\"\"\n\n     17 import numpy as onp; import jax.numpy as np\n\n---&gt; 19 from tensorflow_probability.python.internal.backend.jax import _utils as utils\n\n     21 __all__ = [\n\n     22     'bitwise_xor',\n\n     23     'left_shift',\n\n     24 ]\n\n     27 bitwise_xor = utils.copy_docstring(\n\n     28     'tf.bitwise.bitwise_xor',\n\n     29     lambda x, y, name=None: np.bitwise_xor(x, y))  # pylint: disable=unused-argument\n\n\n\nFile ~/.conda/envs/research/lib/python3.11/site-packages/tensorflow_probability/python/internal/backend/jax/_utils.py:25\n\n     22 from tensorflow_probability.python.internal.backend.jax import nest\n\n     24 try:\n\n---&gt; 25   from tensorflow.python.ops import array_ops  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top,unused-import\n\n     26   from tensorflow.python.ops import random_ops  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top,unused-import\n\n     27   import tensorflow.compat.v1 as tf1  # pylint: disable=g-import-not-at-top,unused-import\n\n\n\nFile ~/.conda/envs/research/lib/python3.11/site-packages/tensorflow/__init__.py:38\n\n     35 import sys as _sys\n\n     36 import typing as _typing\n\n---&gt; 38 from tensorflow.python.tools import module_util as _module_util\n\n     39 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\n\n     41 # Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\n\n\n\nFile ~/.conda/envs/research/lib/python3.11/site-packages/tensorflow/python/__init__.py:36\n\n     27 import traceback\n\n     29 # We aim to keep this file minimal and ideally remove completely.\n\n     30 # If you are adding a new file with @tf_export decorators,\n\n     31 # import it in modules_with_exports.py instead.\n\n     32 \n\n     33 # go/tf-wildcard-import\n\n     34 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\n\n---&gt; 36 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\n\n     38 # pylint: enable=wildcard-import\n\n     39 \n\n     40 # from tensorflow.python import keras\n\n     41 # from tensorflow.python.layers import layers\n\n     42 from tensorflow.python.saved_model import saved_model\n\n\n\nFile ~/.conda/envs/research/lib/python3.11/site-packages/tensorflow/python/pywrap_tensorflow.py:26\n\n     21 from tensorflow.python.platform import self_check\n\n     23 # TODO(mdan): Cleanup antipattern: import for side effects.\n\n     24 \n\n     25 # Perform pre-load sanity checks in order to produce a more actionable error.\n\n---&gt; 26 self_check.preload_check()\n\n     28 # pylint: disable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\n\n     30 try:\n\n     31   # This import is expected to fail if there is an explicit shared object\n\n     32   # dependency (with_framework_lib=true), since we do not need RTLD_GLOBAL.\n\n\n\nFile ~/.conda/envs/research/lib/python3.11/site-packages/tensorflow/python/platform/self_check.py:63, in preload_check()\n\n     50       raise ImportError(\n\n     51           \"Could not find the DLL(s) %r. TensorFlow requires that these DLLs \"\n\n     52           \"be installed in a directory that is named in your %%PATH%% \"\n\n   (...)\n\n     56           \"https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads\"\n\n     57           % \" or \".join(missing))\n\n     58 else:\n\n     59   # Load a library that performs CPU feature guard checking.  Doing this here\n\n     60   # as a preload check makes it more likely that we detect any CPU feature\n\n     61   # incompatibilities before we trigger them (which would typically result in\n\n     62   # SIGILL).\n\n---&gt; 63   from tensorflow.python.platform import _pywrap_cpu_feature_guard\n\n     64   _pywrap_cpu_feature_guard.InfoAboutUnusedCPUFeatures()\n\n\n\nKeyboardInterrupt:",
    "crumbs": [
      "3 SSMs",
      "3.9 Comparison of importance sampling methods"
    ]
  },
  {
    "objectID": "3 SSMs/04_is.html",
    "href": "3 SSMs/04_is.html",
    "title": "3.4 Importance sampling",
    "section": "",
    "text": "from jax import vmap, numpy as jnp\nimport matplotlib.pyplot as plt\nfrom IPython.display import display",
    "crumbs": [
      "3 SSMs",
      "3.4 Importance sampling"
    ]
  },
  {
    "objectID": "3 SSMs/04_is.html#lemma-3.5-mathbb-e-exp-u-1-implies-small-mu",
    "href": "3 SSMs/04_is.html#lemma-3.5-mathbb-e-exp-u-1-implies-small-mu",
    "title": "3.4 Importance sampling",
    "section": "Lemma 3.5 (\\(\\mathbb E \\exp U = 1\\) implies small \\(\\mu\\))",
    "text": "Lemma 3.5 (\\(\\mathbb E \\exp U = 1\\) implies small \\(\\mu\\))\n\ndef p(a,b):\n    #return (1 - jnp.exp(b)) / (jnp.exp(a) - jnp.exp(b))\n    return (jnp.exp(-b) - 1) / (jnp.exp(a - b) - 1)\n\ndef mu(a,b):\n    return a * p(a,b) + b * (1 - p(a,b))\n\ndef s2(a,b):\n    return p(a,b) * (1 - p(a,b)) * (b - a)**2\n\ndef lower_bound(a,b):\n    return jnp.log(1 - jnp.exp(b) * s2(a,b) / 2)\n\ndef upper_bound(a,b):\n    return jnp.log(1 - jnp.exp(a) * s2(a,b) / 2)\n\n\na = jnp.linspace(-1, -.1, 101)\nb = jnp.linspace(1.1, 2, 2)\n\nouter = lambda fun: vmap(vmap(fun, (None, 0)), (0, None))\n\nmus = outer(mu)(a,b)\nlower = outer(lower_bound)(a,b)\nupper = outer(upper_bound)(a,b)\n\nfor i in range(b.shape[0]):\n    plt.plot(a, mus[:,i], label=f'b={b[i]}', color = 'C'+str(i))\n    plt.plot(a, lower[:,i], linestyle='--', label=f'b={b[i]}', color = 'C'+str(i))\n    plt.plot(a, upper[:,i], linestyle='--', label=f'b={b[i]}', color = 'C'+str(i))\nplt.show()\n\n\n\n\n\n\n\n\n\n(jnp.nan_to_num(lower, nan=-jnp.inf) &lt; mus).all(), (mus &lt; upper).all()\n\n(Array(True, dtype=bool), Array(True, dtype=bool))",
    "crumbs": [
      "3 SSMs",
      "3.4 Importance sampling"
    ]
  },
  {
    "objectID": "4 Models/4.3 Nowcasting hospitalizations/model.html",
    "href": "4 Models/4.3 Nowcasting hospitalizations/model.html",
    "title": "Model",
    "section": "",
    "text": "NowcastHub period: Monday 22 November 2021 to Friday 29 April 2022\ndate of final evaluation: 8 August 2022\ndates[0], dates[-1]\n\n('2021-04-05', '2023-10-04')\na_index = 0\nh = h_by_age[a_index]\nI = I_by_age[a_index]\nunique_a[a_index]\n\n'00-04'\n[jnp.max(h_by_age[i].sum(axis=-1) / I_by_age[i]) for i in range(len(unique_a))]\n\n[Array(0.88235294, dtype=float64),\n Array(0.42857143, dtype=float64),\n Array(0.20802005, dtype=float64),\n Array(0.31587057, dtype=float64),\n Array(0.74633431, dtype=float64),\n Array(1.09724473, dtype=float64),\n Array(0.63493344, dtype=float64)]\nnp1 = 158\ni_start = 231\ni_end = i_start + np1\n\nn_delay = 4\ny = h[i_start : i_start + np1, :n_delay]\n\np0_hat = y[:7].sum() / I[i_start : i_start + 7].sum()\nn_weekday = 2\naux = (np1, n_delay, n_weekday, I[i_start : i_start + np1])\ntheta_manual = jnp.log(\n    jnp.array(\n        [1**2, 1**2, 0.1**2, 0.1**2, p0_hat]  # s2_p  # s2_q  # s2_W  # p0  # s2_0\n    )\n)\n\nmodel_manual = hospitalization_model(theta_manual, aux)\nplt.plot(y / I[i_start : i_start + np1][:, None])\nplt.plot(y)\nplt.show()\nfrom ssm4epi.models.util import visualize_pgssm\n\nvisualize_pgssm(model_manual)\nfrom isssm.estimation import initial_theta\n\ntheta0_result = initial_theta(y, hospitalization_model, theta_manual, aux, 100)\ntheta0 = theta0_result.x\ntheta0_result\n\n  message: Optimization terminated successfully.\n  success: True\n   status: 0\n      fun: 3.1679917947612553\n        x: [-8.859e+00 -4.440e+00 -1.806e+01 -1.757e+00 -4.245e+00]\n      nit: 47\n      jac: [ 6.375e-06 -1.721e-06  4.811e-07  7.139e-06 -7.210e-06]\n hess_inv: [[ 5.299e+01 -8.239e-01 ...  8.839e+00  5.897e+00]\n            [-8.239e-01  2.388e+01 ...  1.343e+00 -3.406e-02]\n            ...\n            [ 8.839e+00  1.343e+00 ...  8.132e+01  4.571e+00]\n            [ 5.897e+00 -3.406e-02 ...  4.571e+00  1.008e+02]]\n     nfev: 550\n     njev: 50\nmodel0 = hospitalization_model(theta0, aux)\nsds = jnp.exp(theta0[:4] / 2)\np0 = jnp.exp(theta0[4])\nplt.scatter(jnp.arange(4), sds)\nplt.scatter(4, p0)\nplt.xticks(jnp.arange(5), [\"s_p\", \"s_q\", \"s_W\", \"s_0\", \"p0\"])\nplt.show()\nfrom isssm.laplace_approximation import laplace_approximation as LA\nfrom isssm.modified_efficient_importance_sampling import (\n    modified_efficient_importance_sampling as MEIS,\n)\nfrom isssm.importance_sampling import pgssm_importance_sampling, ess_pct\nimport jax.random as jrn\n\nproposal_la, info_la = LA(y, model0, 100)\nkey = jrn.PRNGKey(423423423)\nkey, subkey = jrn.split(key)\nproposal_meis, info_meis = MEIS(\n    y, model0, proposal_la.z, proposal_la.Omega, 100, 10000, subkey\n)\nkey, subkey = jrn.split(key)\nsamples, log_weights = pgssm_importance_sampling(\n    y, model0, proposal_meis.z, proposal_meis.Omega, 1000, subkey\n)\ness_pct(log_weights)\n\nArray(67.45223992, dtype=float64)\nvisualize_model_fit(samples, log_weights, model0, i_start, np1, y, a_index)",
    "crumbs": [
      "4 Models",
      "4.3 Nowcasting hospitalizations",
      "Model"
    ]
  },
  {
    "objectID": "4 Models/4.3 Nowcasting hospitalizations/model.html#missigness",
    "href": "4 Models/4.3 Nowcasting hospitalizations/model.html#missigness",
    "title": "Model",
    "section": "Missigness",
    "text": "Missigness\nDeviate from standard setup: now missing indices of \\(B\\) differ from those of \\(Y\\).\ntest: make everything in the last week of observations missing, except initial observation\n\ny_nan = make_y_nan(y)\n# need fewer data for missingness\nnp1_miss = 100\ny_nan = y_nan[-np1_miss:]\nmissing_y_indices = jnp.isnan(y_nan)\nmissing_s_indicies = jnp.concatenate(\n    (jnp.full((np1_miss, 1), False, dtype=bool), missing_y_indices[:, :-1]), axis=-1\n)\naux_miss = (np1_miss, n_delay, n_weekday, I[i_start + (np1 - np1_miss) : i_start + np1])\n\ntheta_manual_missing = theta0\n_, y_miss = account_for_nans(\n    hospitalization_model(theta0, aux_miss),\n    y_nan,\n    missing_y_indices,\n    missing_s_indicies,\n)\n_model_miss = lambda theta, aux: account_for_nans(\n    hospitalization_model(theta, aux), y_nan, missing_y_indices, missing_s_indicies\n)[0]\n\ntheta0_missing_result = initial_theta(\n    y_miss, _model_miss, theta_manual_missing, aux_miss, 10\n)\ntheta0_missing = theta0_missing_result.x\ntheta0_missing_result\n\n  message: Optimization terminated successfully.\n  success: True\n   status: 0\n      fun: 3.113603913668618\n        x: [-9.414e+00 -4.847e+00 -1.806e+01 -1.861e+00 -4.561e+00]\n      nit: 21\n      jac: [-1.629e-06 -2.281e-07  2.666e-07  6.895e-06 -1.010e-06]\n hess_inv: [[ 8.709e+01 -1.311e+00 ...  5.298e+00  5.793e+00]\n            [-1.311e+00  2.688e+01 ... -2.789e-01  2.457e-01]\n            ...\n            [ 5.298e+00 -2.789e-01 ...  5.530e+00  1.557e+01]\n            [ 5.793e+00  2.457e-01 ...  1.557e+01  5.650e+01]]\n     nfev: 253\n     njev: 23\n\n\n\nmodel_miss0 = _model_miss(theta0_missing, aux_miss)\nproposal_la, info_la = LA(y_miss, model_miss0, 100)\n# key, subkey = jrn.split(key)\n# proposal_meis, info_meis = MEIS(\n#    y_miss,\n#    model_miss0,\n#    proposal_la.z,\n#    proposal_la.Omega,\n#    100,\n#    10000,\n#    subkey,\n# )\nkey, subkey = jrn.split(key)\nsamples, log_weights = pgssm_importance_sampling(\n    y_miss,\n    model_miss0,\n    proposal_la.z,\n    proposal_la.Omega,\n    10000,\n    subkey,\n)\n\ness_pct(log_weights)\n\nArray(0.00314612, dtype=float64)\n\n\n\nvisualize_model_fit(\n    samples,\n    log_weights,\n    model_miss0,\n    i_start + (np1 - np1_miss),\n    np1_miss,\n    y_miss,\n    a_index,\n)\n\n\n\n\n\n\n\n\n\npost = mc_integration(samples, log_weights)\npost_state = mc_integration(\n    vmap(state_mode, (None, 0))(model_miss0, samples), log_weights\n)\n\nfig, axs = plt.subplots(3, 2, figsize=(10, 10))\naxs = axs.flatten()\naxs[0].plot(jnp.exp(post[:, 0]) * I[i_start + (np1 - np1_miss) : i_start + np1])\naxs[0].plot(y[-(np1_miss):].sum(axis=-1))\naxs[1].plot(jnp.exp(post[:, 0]))\n\naxs[2].plot(from_consecutive_logits(post[:, 1:]), linestyle=\"--\")\naxs[2].plot(from_consecutive_logits(post_state[:, 1:8]))\naxs[3].plot(post_state[:, 1:8])\naxs[4].plot(post_state[:, 8])\naxs[4].plot(post_state[:, 8 + 6])\n\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom isssm.importance_sampling import prediction\n\n\ndef f_nowcast(x, s, y):\n    return jnp.sum((missing_y_indices * y) + (1 - missing_y_indices) * y_miss, axis=-1)\n\n\nkey, subkey = jrn.split(key)\npreds = prediction(\n    f_nowcast,\n    y_miss,\n    proposal_la,\n    _model_miss(theta0_missing, aux_miss),\n    10000,\n    subkey,\n    jnp.array([0.025, 0.5, 0.975]),\n    hospitalization_model(theta0_missing, aux_miss),\n)\n\n\n# plt.plot(preds[0])\n# plt.plot(preds[2][1])\nplt.plot(jnp.clip(preds[2][0], 0.0), linestyle=\"--\", color=\"grey\")\nplt.plot(preds[2][2], linestyle=\"--\", color=\"grey\")\nplt.plot(y.sum(axis=-1)[-np1_miss:])\nplt.show()",
    "crumbs": [
      "4 Models",
      "4.3 Nowcasting hospitalizations",
      "Model"
    ]
  },
  {
    "objectID": "4 Models/4.3 Nowcasting hospitalizations/showcase.html",
    "href": "4 Models/4.3 Nowcasting hospitalizations/showcase.html",
    "title": "Application 1: Showcase",
    "section": "",
    "text": "from jax import config\n\nimport pickle\n\nconfig.update(\"jax_enable_x64\", True)\nfrom ssm4epi.models.hospitalization import (\n    hospitalization_model,\n    dates,\n    unique_a,\n    h_by_age,\n    I_by_age,\n)\nfrom ssm4epi.models.util import from_consecutive_logits\nfrom isssm.importance_sampling import mc_integration\nfrom isssm.kalman import state_mode\nfrom jax import vmap\nfrom ssm4epi.patch import full_deps\nimport jax.numpy as jnp\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\nfrom isssm.estimation import initial_theta, mle_pgssm\nfrom isssm.laplace_approximation import laplace_approximation as LA\nfrom isssm.modified_efficient_importance_sampling import (\n    modified_efficient_importance_sampling as MEIS,\n)\nfrom isssm.importance_sampling import pgssm_importance_sampling, ess_pct\nimport jax.random as jrn\nRun hospitalization model on all data until final NCH evaluation date.\nfrom typing import NamedTuple\nfrom jaxtyping import Float, Array, PRNGKeyArray\nfrom scipy.optimize import OptimizeResult\nfrom isssm.typing import GLSSMProposal\n\n\nclass HospitalizationResult(NamedTuple):\n    a_index: int\n    h: Float[Array, \"...\"]\n    I: Float[Array, \"...\"]\n    y: Float[Array, \"np1 p\"]\n    theta0: Float[Array, \"5\"]\n    theta0_result: OptimizeResult\n    theta_hat: Float[Array, \"5\"]\n    theta_hat_result: OptimizeResult\n    proposal: GLSSMProposal\n    key: PRNGKeyArray\n    ess_pct: Float\nstart_date = \"2021-11-22\"\nend_date = \"2022-04-29\"\n\nn_iter = 100\nN_mle = 1000\nN_meis = 1000\nN_ess = 10000\nN_fit = 10000\nN_predict = 10000\ndef showcase_result(a_index: int, n_delay: int) -&gt; HospitalizationResult:\n    i_start = int(jnp.where(dates == start_date)[0][0])\n    np1 = int(jnp.where(dates == end_date)[0][0]) - i_start\n    h = h_by_age[a_index][:, :n_delay]\n    I = I_by_age[a_index][i_start : i_start + np1]\n    unique_a[a_index]\n    key = jrn.PRNGKey(4234809124)\n\n    y = h[i_start : i_start + np1]\n    aux = (np1, n_delay, 2, I)\n\n    p0_hat = y[:7].sum() / I[:7].sum()\n    theta_manual = jnp.log(jnp.array([1**2, 0.1**2, 0.1**2, 0.1**2, p0_hat]))\n\n    theta0_result = initial_theta(y, hospitalization_model, theta_manual, aux, n_iter)\n    theta0 = theta0_result.x\n    key, subkey = jrn.split(key)\n    mle_result = None\n    theta_hat = None\n    # mle_result = mle_pgssm(y, hospitalization_model, theta0, aux, n_iter, N_mle, subkey)\n    # theta_hat = mle_result.x\n    fitted_model = hospitalization_model(theta0, aux)\n\n    proposal_la, _ = LA(y, fitted_model, n_iter)\n    key, subkey = jrn.split(key)\n    proposal_meis, _ = MEIS(\n        y, fitted_model, proposal_la.z, proposal_la.Omega, n_iter, N_meis, subkey\n    )\n    key, subkey = jrn.split(key)\n    proposal = proposal_meis\n    _, log_weights = pgssm_importance_sampling(\n        y, fitted_model, proposal.z, proposal.Omega, N_ess, subkey\n    )\n\n    result = HospitalizationResult(\n        a_index,\n        h,\n        I,\n        y,\n        theta0,\n        theta0_result,\n        theta_hat,\n        mle_result,\n        proposal,\n        key,\n        ess_pct(log_weights),\n    )\n\n    with open(\n        here()\n        / \"data\"\n        / \"results\"\n        / \"4_hospitalizations\"\n        / \"showcase\"\n        / f\"result_A{a_index}_D{n_delay}.pkl\",\n        \"wb\",\n    ) as f:\n        pickle.dump(result, f)\n\n    print(f\"Finished results for a_index {a_index} and n_delay {n_delay}\")\n\n    return result\nfig, axs = plt.subplots(4, 2, figsize=(20, 10))\naxs = axs.flatten()\nfor a_index in range(len(unique_a)):\n    h = h_by_age[a_index]\n    I = I_by_age[a_index]\n    unique_a[a_index]\n    i_start = int(jnp.where(dates == start_date)[0][0])\n    np1 = int(jnp.where(dates == end_date)[0][0]) - i_start\n\n    y = h[i_start : i_start + np1]\n    axs[a_index].plot(y)\n    axs[a_index].set_title(f\"{unique_a[a_index]}\")\n\naxs[-1].plot(jnp.array(I_by_age).T[i_start : i_start + np1], label=unique_a)\naxs[-1].legend()\nplt.show()\nfrom ssm4epi.models.hospitalization import delays_per_age\n\nresults = [\n    showcase_result(a_index, n_delay) for _, (a_index, n_delay) in delays_per_age.iterrows()\n]\n\nFinished results for a_index 0 and n_delay 5\nFinished results for a_index 1 and n_delay 5\nFinished results for a_index 2 and n_delay 7\nFinished results for a_index 3 and n_delay 7\nFinished results for a_index 4 and n_delay 8\nFinished results for a_index 5 and n_delay 8\nFinished results for a_index 6 and n_delay 8\nfrom ssm4epi.models.hospitalization import visualize_model_fit\n\n\ndef visualize_result(result: HospitalizationResult):\n    i_start = int(jnp.where(dates == start_date)[0][0])\n    # I = result.I[i_start : i_start + np1]\n    key, subkey = jrn.split(result.key)\n    np1, n_delay = result.y.shape\n    aux = (result.y.shape[0], n_delay, 2, result.I)\n    model = hospitalization_model(result.theta0, aux)\n    samples, log_weights = pgssm_importance_sampling(\n        result.y,\n        model,\n        result.proposal.z,\n        result.proposal.Omega,\n        N_fit,\n        subkey,\n    )\n\n    visualize_model_fit(\n        samples, log_weights, model, i_start, np1, result.y, result.a_index\n    )\n\n\nvisualize_result(results[0])\nfrom isssm.importance_sampling import prediction\n\nkey = jrn.PRNGKey(2342312412)\nkey, subkey_prediction = jrn.split(key)\n\n\ndef f_pred(x, s, y):\n    np1, n_delay = y.shape\n    p = jnp.exp(s[:, :1])\n    delay_dist_smooth = from_consecutive_logits(x[:, 2 : (2 + n_delay - 1)])\n    delay_dist_observed = from_consecutive_logits(s[:, 1:])\n    weekday_1 = jnp.exp(x[:, (2 + n_delay - 1)][:, None])\n    weekday_2 = jnp.exp(x[:, (2 + n_delay - 1 + 6)][:, None])\n    average_delay = jnp.sum(\n        delay_dist_smooth * jnp.arange(n_delay), axis=-1, keepdims=True\n    )\n    return jnp.concatenate(\n        (\n            p,\n            delay_dist_smooth,\n            delay_dist_observed,\n            weekday_1,\n            weekday_2,\n            average_delay,\n        ),\n        axis=-1,\n    )\n\n\npercentiles_of_interest = jnp.array(\n    [0.01, 0.025, *(0.05 * jnp.arange(1, 20)), 0.975, 0.99]\n)\n\n\ndef stacked_prediction(f, result: HospitalizationResult):\n    proposal = result.proposal\n    result.theta0\n    fitted_model = hospitalization_model(\n        result.theta0, (result.y.shape[0], result.y.shape[1], 2, result.I)\n    )\n    mean, sd, quantiles = prediction(\n        f,\n        result.y,\n        proposal,\n        fitted_model,\n        N_predict,\n        subkey_prediction,\n        percentiles_of_interest,\n    )\n    return jnp.vstack((mean[None], sd[None], quantiles))\n\n\nfor i, result in enumerate(results):\n    jnp.save(\n        here()\n        / f\"data/results/4_hospitalizations/showcase/predictions_age_{unique_a[i]}.npy\",\n        stacked_prediction(f_pred, result),\n    )",
    "crumbs": [
      "4 Models",
      "4.3 Nowcasting hospitalizations",
      "Application 1: Showcase"
    ]
  },
  {
    "objectID": "4 Models/4.3 Nowcasting hospitalizations/showcase.html#dataresults4_hospitalizationsshowcasehyperparams.tex",
    "href": "4 Models/4.3 Nowcasting hospitalizations/showcase.html#dataresults4_hospitalizationsshowcasehyperparams.tex",
    "title": "Application 1: Showcase",
    "section": "data/results/4_hospitalizations/showcase/hyperparams.tex",
    "text": "data/results/4_hospitalizations/showcase/hyperparams.tex\n\nhyperparams_statement = f\"\"\"\nTo fit the model for all age groups, we use at most {n_iter} iterations for all occurences of the \\\\acrshort{{la}}, with a convergence threshold set to $10^{{-5}}$ relative difference in $z$ and $\\\\Omega$. We use the same method for \\\\acrshort{{eis}}, where we additionally use ${N_meis:,}$ samples to determine the optimal proposal, starting with the proposal given by th \\\\acrshort{{la}}.\n\nFor \\\\acrshort{{mle}}, we use ${N_mle:,}$ samples to determine the maximum likelihood estimate of the parameters, initializing at the initial guess given by \\\\Cref{{alg:mle}}.\nTo obtain prediction intervals of the states, signals and missing observations we use ${N_predict:,}$ samples. To estimate the \\\\acrshort{{ess}} we use ${N_ess:,}$ samples. Note that all sample sizes are given before constructing the three antithetic variables.\n\"\"\"\n\n# write to tex file\nwith open(\n    here() / \"data/results/4_hospitalizations/showcase/hyperparams.tex\", \"w\"\n) as f:\n    f.write(hyperparams_statement)\n\n\nimport pandas as pd\n\nwith open(here() / \"tables\" / \"hospitalization_showcase_ess.tex\", \"w\") as f:\n    table = pd.DataFrame(\n        {\n            \"Age group\": [f\"A{a}\" for a in unique_a],\n            \"EF [%]\": [f\"{result.ess_pct:.0f}\" for result in results],\n            \"weeks of delay\": n_delays,\n        }\n    ).to_latex(index=False)\n    f.write(table)\n\n/var/folders/9y/xdxkkt710kx5tf1j0p68y46r0000gn/T/ipykernel_19755/377309747.py:4: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n  table = pd.DataFrame(",
    "crumbs": [
      "4 Models",
      "4.3 Nowcasting hospitalizations",
      "Application 1: Showcase"
    ]
  },
  {
    "objectID": "4 Models/4.3 Nowcasting hospitalizations/nowcasting.html",
    "href": "4 Models/4.3 Nowcasting hospitalizations/nowcasting.html",
    "title": "Application 2: Nowcasting hospitalizations",
    "section": "",
    "text": "from jax import config\n\nconfig.update(\"jax_enable_x64\", True)\nfrom ssm4epi.models.hospitalization import (\n    hospitalization_model,\n    dates,\n    unique_a,\n    h_by_age,\n    I_by_age,\n)\nfrom ssm4epi.models.util import from_consecutive_logits\nfrom isssm.importance_sampling import mc_integration\nfrom isssm.kalman import state_mode\nfrom jax import vmap\nfrom ssm4epi.patch import full_deps\nimport jax.numpy as jnp\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\nfrom isssm.estimation import initial_theta, mle_pgssm\nfrom isssm.laplace_approximation import laplace_approximation as LA\nfrom isssm.modified_efficient_importance_sampling import (\n    modified_efficient_importance_sampling as MEIS,\n)\nfrom isssm.importance_sampling import pgssm_importance_sampling, ess_pct\nimport jax.random as jrn\nimport pandas as pd\n\n\nfrom typing import NamedTuple\nfrom jaxtyping import Float, Array, PRNGKeyArray\nfrom scipy.optimize import OptimizeResult\nfrom isssm.typing import GLSSMProposal\n\n\nclass PredictionResult(NamedTuple):\n    mean: Float[Array, \"...\"]\n    sd: Float[Array, \"...\"]\n    quantiles: Float[Array, \"k ...\"]\n    quantiles_of_interest: Float[Array, \"k\"]\n\n\nclass HospitalizationNowcastingResult(NamedTuple):\n    a_index: int\n    h: Float[Array, \"...\"]\n    I: Float[Array, \"...\"]\n    dates: pd.DatetimeIndex\n    y_miss: Float[Array, \"np1 p\"]\n    y_true: Float[Array, \"np1 p\"]\n    theta0: Float[Array, \"5\"]\n    theta0_result: OptimizeResult\n    proposal: GLSSMProposal\n    key: PRNGKeyArray\n    ess_pct: Float\n    y_sum_predict: PredictionResult\n\n\nstart_date = \"2021-11-22\"\nend_date = \"2022-04-29\"\n\nn_days_back = 100\nn_weekday = 2\n\nn_iter = 100\nN_meis = 10000\nN_ess = 10000\nN_fit = 10000\nN_predict = 10000\n\n\nclass HospitalizationResult(NamedTuple):\n    a_index: int\n    h: Float[Array, \"...\"]\n    I: Float[Array, \"...\"]\n    y: Float[Array, \"np1 p\"]\n    theta0: Float[Array, \"5\"]\n    theta0_result: OptimizeResult\n    theta_hat: Float[Array, \"5\"]\n    theta_hat_result: OptimizeResult\n    proposal: GLSSMProposal\n    key: PRNGKeyArray\n    ess_pct: Float\n\n\nimport pickle\nfrom isssm.util import degenerate_cholesky\nfrom ssm4epi.models.hospitalization import account_for_nans, make_y_nan\nfrom isssm.importance_sampling import prediction\n\n\ndef nowcast_hospitalizations(a_index, n_delay, date_to_nowcast):\n    s = int(jnp.where(dates == date_to_nowcast)[0][0])\n    i_start = s - n_days_back\n    np1 = n_days_back\n\n    h = h_by_age[a_index][:, :n_delay]\n    I = I_by_age[a_index][i_start : i_start + np1]\n    unique_a[a_index]\n\n    key = jrn.PRNGKey(4234809124)\n\n    y = h[i_start : i_start + np1]\n    aux = (np1, n_delay, n_weekday, I)\n\n    y_nan = make_y_nan(y)\n    missing_y_indices = jnp.isnan(y_nan)\n\n    with open(\n        here()\n        / \"data\"\n        / \"results\"\n        / \"4_hospitalizations\"\n        / \"showcase\"\n        / f\"result_A{a_index}_D{n_delay}.pkl\",\n        \"rb\",\n    ) as f:\n        result = pickle.load(f)\n\n    theta_manual = result.theta0\n\n    missing_s_indicies = jnp.concatenate(\n        (jnp.full((np1, 1), False, dtype=bool), missing_y_indices[:, :-1]), axis=-1\n    )\n\n    _, y_miss = account_for_nans(\n        hospitalization_model(theta_manual, aux),\n        y_nan,\n        missing_y_indices,\n        missing_s_indicies,\n    )\n    _model_miss = lambda theta, aux: account_for_nans(\n        hospitalization_model(theta, aux), y_nan, missing_y_indices, missing_s_indicies\n    )[0]\n\n    theta0_missing_result = initial_theta(\n        y_miss, _model_miss, theta_manual, aux, n_iter\n    )\n    theta0_missing = theta0_missing_result.x\n    model_miss0 = _model_miss(theta0_missing, aux)\n    proposal_la, info_la = LA(y_miss, model_miss0, n_iter)\n\n    cOmega = degenerate_cholesky(proposal_la.Omega)\n    Omega = cOmega @ cOmega.transpose((0, 2, 1))\n\n    proposal_la = GLSSMProposal(\n        u=proposal_la.u,\n        A=proposal_la.A,\n        D=proposal_la.D,\n        Sigma0=proposal_la.Sigma0,\n        Sigma=proposal_la.Sigma,\n        v=proposal_la.v,\n        B=proposal_la.B,\n        Omega=Omega,\n        z=proposal_la.z,\n    )\n\n    # key, subkey = jrn.split(key)\n    # proposal_meis, info_meis = MEIS(\n    #    y_miss,\n    #    model_miss0,\n    #    proposal_la.z,\n    #    proposal_la.Omega,\n    #    n_iter,\n    #    N_meis,\n    #    subkey,\n    # )\n\n    key, subkey = jrn.split(key)\n    _, log_weights = pgssm_importance_sampling(\n        y_miss,\n        model_miss0,\n        proposal_la.z,\n        proposal_la.Omega,\n        N_fit,\n        subkey,\n    )\n\n    def f_nowcast(x, s, y):\n        return jnp.sum(\n            (missing_y_indices * y) + (1 - missing_y_indices) * y_miss, axis=-1\n        )\n\n    key, subkey = jrn.split(key)\n    preds = prediction(\n        f_nowcast,\n        y_miss,\n        proposal_la,\n        _model_miss(theta0_missing, aux),\n        N_predict,\n        subkey,\n        jnp.array([0.025, 0.5, 0.975]),\n        hospitalization_model(theta0_missing, aux),\n    )\n\n    result = HospitalizationNowcastingResult(\n        a_index=a_index,\n        h=h,\n        I=I,\n        y_miss=y_miss,\n        y_true=y,\n        dates=dates[i_start : i_start + np1],\n        theta0=theta0_missing,\n        theta0_result=theta0_missing_result,\n        proposal=proposal_la,\n        key=key,\n        ess_pct=ess_pct(log_weights),\n        y_sum_predict=preds,\n    )\n\n    with open(\n        here()\n        / \"data\"\n        / \"results\"\n        / \"4_hospitalizations\"\n        / \"nowcast\"\n        / f\"nowcast_A{a_index}_D{n_delay}_{date_to_nowcast}.pkl\",\n        \"wb\",\n    ) as f:\n        pickle.dump(result, f)\n\n    return result\n\n\nfrom ssm4epi.models.hospitalization import delays_per_age\n\n\ninitial_date = \"2021-11-22\"\nresults = delays_per_age.apply(\n    lambda x: nowcast_hospitalizations(x[0], x[1], initial_date), axis=1\n)\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import dates as mdates\n\n\ndef visualize(result: HospitalizationNowcastingResult, ax: plt.Axes = None):\n    ax.set_title(f\"ESS: {result.ess_pct * 100 :.2f}%\")\n\n    dates = pd.to_datetime(result.dates)\n    y = result.y_true\n    ax.plot(dates, result.y_miss.sum(axis=-1), label=\"$y_{miss}$\")\n    ax.plot(dates, y.sum(axis=-1), label=\"$y_{true}$\")\n    # plt.plot(result.y_sum_predict[0], color=\"black\")\n    ax.plot(dates, result.y_sum_predict[2][0], \"--\", color=\"gray\")\n    ax.plot(dates, result.y_sum_predict[2][-1], \"--\", color=\"gray\")\n    ax.legend()\n    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%m/%d/%Y\"))\n    ax.xaxis.set_major_locator(mdates.MonthLocator())\n\n_, axs = plt.subplots(4, 2, figsize=(12, 12))\n\n[visualize(result, ax) for result, ax in zip(results, axs.flatten()[:-1])]",
    "crumbs": [
      "4 Models",
      "4.3 Nowcasting hospitalizations",
      "Application 2: Nowcasting hospitalizations"
    ]
  },
  {
    "objectID": "4 Models/4.2 Regional growth factor model/model.html#hyperparameters",
    "href": "4 Models/4.2 Regional growth factor model/model.html#hyperparameters",
    "title": "Model",
    "section": "Hyperparameters",
    "text": "Hyperparameters\n\nmpl.rcParams[\"figure.figsize\"] = (20, 6)",
    "crumbs": [
      "4 Models",
      "4.2 Regional growth factor model",
      "Model"
    ]
  },
  {
    "objectID": "4 Models/4.2 Regional growth factor model/model.html#states",
    "href": "4 Models/4.2 Regional growth factor model/model.html#states",
    "title": "Model",
    "section": "States",
    "text": "States\nLet \\(\\bar r_t = \\log \\bar \\rho_t\\) be the average growth factor (on the log scale) across all counties and \\(u_t^c\\) be the deviation of county \\(c\\)’s log-growth factor for \\(\\bar r_t\\). We model \\(\\bar r_t\\) following a random walk and \\(u^\\cdot_t\\) following an \\(\\text{VAR}(1)\\) process with transition matrix \\(\\alpha I\\) and spatial correlation \\(\\Omega\\).\nThus we have \\[\n\\begin{align*}\n\\bar r_{t + 1} &= \\bar r_{t} + \\varepsilon_{t + 1}^\\rho \\\\\nu^c_{t + 1} &= \\alpha u^c_t + \\varepsilon_{t + 1}^c\n\\end{align*}\n\\]\nwith \\(\\text{Var}(\\varepsilon_{t + 1}^\\rho) = \\sigma^2_\\rho\\) and \\(\\text{Cov}(\\varepsilon_{t + 1}) = \\Omega\\). The covariance matrix of the stationary distribution is \\(\\Sigma = \\frac {1} {1 - \\alpha^2} \\Omega\\).",
    "crumbs": [
      "4 Models",
      "4.2 Regional growth factor model",
      "Model"
    ]
  },
  {
    "objectID": "4 Models/4.2 Regional growth factor model/model.html#observations",
    "href": "4 Models/4.2 Regional growth factor model/model.html#observations",
    "title": "Model",
    "section": "Observations",
    "text": "Observations\nThe log growth factor in a region at time \\(t\\), \\(r^c_t\\), is given by the mean log growth factor \\(\\bar r_t\\) and the per-region deviation \\(u^c_t\\). Conditional on the log growth factors and past cases, cases are Negative Binomially distributed with shared overdispersion parameter \\(r\\) \\[\n\\begin{align*}\nr^c_t &= \\bar r_t + u^c_{t} \\\\\n\\lambda_t^c &= \\exp(r^c_t) \\sum_{d}p_{c,d} I_t^d \\\\\nI^c_{t + 1} | I^c_{t}, \\rho_t, u^c_t &\\sim \\text{NegBinom}(\\underbrace{\\lambda_t^c}_{\\text{mean}}, \\underbrace{r}_{\\text{overdispersion}})\n\\end{align*}\n\\]\nwhere the variance of the negative binomial distribution is \\(\\mu + \\frac{\\mu^2}{r}\\)",
    "crumbs": [
      "4 Models",
      "4.2 Regional growth factor model",
      "Model"
    ]
  },
  {
    "objectID": "4 Models/4.2 Regional growth factor model/model.html#spatial-correlations",
    "href": "4 Models/4.2 Regional growth factor model/model.html#spatial-correlations",
    "title": "Model",
    "section": "Spatial Correlations",
    "text": "Spatial Correlations\nSuppose we have in county \\(c\\) \\(S^c\\) many new infections generated, which may be attributed to the same county or another one, \\(c'\\) say. Let \\(p_{c,c'}\\) be the fraction of cases generated in county \\(c'\\) (instead of in county \\(c\\)) and \\(p_{c,c} = 1 - \\sum_{c' \\neq c} p_{c,c'}\\).\nLet \\(P = \\left(p_{c,c'}\\right)_{c = 1, \\dots, K, c' = 1,\\dots, K}\\).\nLet \\(\\tilde S^c = \\sum p_{c,d} S^d\\) be the number of cases generated in county \\(c\\).\nThen we are interested in\n\\[\n\\begin{align*}\n    \\text{Cov}(\\tilde S^c, \\tilde S^{c'}) = \\underbrace{\\sigma^2}_{\\text{Var}(S^c)} (\\sum_{d} p_{c,d} p_{c',d}) = \\sigma^2_{\\text{spat}} (P P^T)_{c, c'}\n\\end{align*}\n\\]\nTo obtain \\(p_{c,c'}\\) we use data on commuters \\(q_{c,c'}\\) the fraction of socially insured employees that have their center of life in county \\(c\\) but are registered to work in county \\(c'\\).\nTo account for non-working inhabitants (elderly, kids, …) we introduce a constant \\(C \\geq 1\\) s.t. \\[\np_{c,c'} = \\bar q + (1 - \\bar q)\\frac{\\mathbf 1 _{c \\neq c'} q_{c,c'}}{ \\sum_{d \\neq c} q_{c,d} + C q_{c,c}},\n\\] i.e. we blow up the proportion of “stay at home” by a constant \\(C\\) (that is the same for all counties) and add a constant “socket” of travel \\(\\bar q\\) between the counties.\nFinally we choose\n\\[\n\\Omega = \\sigma^2_{\\text{spat}}PP^T %+ \\sigma^2_{\\text{nugget}} I\n\\]",
    "crumbs": [
      "4 Models",
      "4.2 Regional growth factor model",
      "Model"
    ]
  },
  {
    "objectID": "4 Models/4.2 Regional growth factor model/model.html#parameters",
    "href": "4 Models/4.2 Regional growth factor model/model.html#parameters",
    "title": "Model",
    "section": "Parameters",
    "text": "Parameters\n\\[\n\\theta = \\left( \\text{logit}(\\alpha), \\log \\sigma^2_r, \\log \\sigma^2_{\\text{spat}}, C, \\log \\mu \\right)%\\log \\sigma^2_{\\text{nugget}}, \\log \\mu \\right)\n\\]",
    "crumbs": [
      "4 Models",
      "4.2 Regional growth factor model",
      "Model"
    ]
  },
  {
    "objectID": "4 Models/4.2 Regional growth factor model/model.html#final-model",
    "href": "4 Models/4.2 Regional growth factor model/model.html#final-model",
    "title": "Model",
    "section": "Final Model",
    "text": "Final Model\n\nplt.imshow(jnp.log(n_ij))\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\n\nP = _P(1.0, 0.5, n_ij, n_tot)\nplt.imshow(P)\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\n\npar_manual = jnp.array(\n    [\n        0.5,\n        0.01**2,\n        0.01**2,\n        0.5,\n        2,\n        100.0,\n    ]\n)\ntheta_manual = par_to_theta(par_manual)\nnp1 = 10\n# aux = (cases_full[: np1 + 1], n_ij, n_tot)\ndates = dates_full[14 + 1 : 14 + np1 + 1]\naux = make_aux(dates[0], cases_full, n_ij, n_tot, np1)\ny = aux[0][1:]\ny_nan = y.at[-1].set(jnp.nan)\nmissing_inds = jnp.isnan(y_nan)\n_, y_miss = account_for_nans(\n    growth_factor_model(theta_manual, aux), y_nan, missing_inds\n)\n_model_miss = lambda theta, aux: account_for_nans(\n    growth_factor_model(theta, aux), y_nan, missing_inds\n)[0]\n\n\nplt.plot(dates, y_nan.sum(axis=-1))\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom ssm4epi.models.util import visualize_pgssm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheta_manual = jnp.array([0.0, -2.063e00, -5.355e00, -4.511e-01, -5.711e-01, 7.932e-01])\nmodel = growth_factor_model(theta_manual, aux)\n\nvisualize_pgssm(model)\n\n\nfrom isssm.estimation import initial_theta\n\n  message: Maximum number of iterations has been exceeded.\n  success: False\n   status: 1\n      fun: 2.587296844430436\n        x: [ 5.318e-01 -2.083e+00 -5.314e+00 -2.451e+00 -4.464e-01\n             6.158e-01]\n      nit: 10\n      jac: [-2.857e-03  5.421e-04 -1.315e-03 -4.843e-04  2.249e-03\n             7.908e-04]\n hess_inv: [[ 1.801e+00  8.969e-03 ...  4.750e-01  1.249e-01]\n            [ 8.969e-03  1.016e+00 ...  1.133e-01 -7.293e-02]\n            ...\n            [ 4.750e-01  1.133e-01 ...  2.038e+00 -3.033e-01]\n            [ 1.249e-01 -7.293e-02 ... -3.033e-01  4.783e+00]]\n     nfev: 143\n     njev: 11\n\n\n\ntheta0_result = initial_theta(\n    y_miss, _model_miss, theta_manual, aux, n_iterations, options={\"maxiter\": 10}\n)\ntheta0 = theta0_result.x\ntheta0_result\n\n\ntheta_to_par(theta0)\n\nArray([0.62990242, 0.12461062, 0.0049228 , 0.07938039, 1.63991047,\n       1.85112263], dtype=float64)\n\n\n\nfitted_model = _model_miss(theta0, aux)",
    "crumbs": [
      "4 Models",
      "4.2 Regional growth factor model",
      "Model"
    ]
  },
  {
    "objectID": "4 Models/4.2 Regional growth factor model/model.html#la",
    "href": "4 Models/4.2 Regional growth factor model/model.html#la",
    "title": "Model",
    "section": "LA",
    "text": "LA\n\nfrom isssm.laplace_approximation import laplace_approximation as LA\n\nConvergenceInformation(converged=Array(False, dtype=bool), n_iter=Array(20, dtype=int64, weak_type=True), delta=Array(4.9595883e-12, dtype=float64))\n\n\n\nproposal_la, info_la = LA(y_miss, fitted_model, n_iterations)\ninfo_la\n\n\nfrom isssm.importance_sampling import pgssm_importance_sampling, ess_pct\n\nArray(0.27961409, dtype=float64)\n\n\n\nkey, subkey = jrn.split(key)\nsamples, lw = pgssm_importance_sampling(\n    y_miss,\n    fitted_model,\n    proposal_la.z,\n    proposal_la.Omega,\n    N_mle,\n    subkey,\n)\n\ness_pct(lw)",
    "crumbs": [
      "4 Models",
      "4.2 Regional growth factor model",
      "Model"
    ]
  },
  {
    "objectID": "4 Models/4.2 Regional growth factor model/model.html#meis",
    "href": "4 Models/4.2 Regional growth factor model/model.html#meis",
    "title": "Model",
    "section": "MEIS",
    "text": "MEIS\n\nfrom isssm.modified_efficient_importance_sampling import (\n    modified_efficient_importance_sampling as MEIS,\n)\n\nConvergenceInformation(converged=Array(False, dtype=bool), n_iter=Array(10, dtype=int64, weak_type=True), delta=Array(6.66369159e-06, dtype=float64))\n\n\n\nkey, subkey = jrn.split(key)\nproposal_meis, info_meis = MEIS(\n    y_miss, fitted_model, proposal_la.z, proposal_la.Omega, 10, int(1e3), subkey\n)\ninfo_meis\n\n\nfrom isssm.importance_sampling import pgssm_importance_sampling, ess_pct\n\nArray(99.92980398, dtype=float64)\n\n\n\nkey, subkey = jrn.split(key)\nsamples, lw = pgssm_importance_sampling(\n    y_miss,\n    fitted_model,\n    proposal_meis.z,\n    proposal_meis.Omega,\n    N_mle,\n    subkey,\n)\n\ness_pct(lw)",
    "crumbs": [
      "4 Models",
      "4.2 Regional growth factor model",
      "Model"
    ]
  },
  {
    "objectID": "4 Models/4.2 Regional growth factor model/model.html#predictions",
    "href": "4 Models/4.2 Regional growth factor model/model.html#predictions",
    "title": "Model",
    "section": "Predictions",
    "text": "Predictions\n\nfrom isssm.importance_sampling import mc_integration, prediction\n\n\nkey, subkey = jrn.split(key)\n\npreds = prediction(\n    lambda x, s, y: jnp.concatenate([y[-1].sum()[None], y[-1]]),\n    y_miss,\n    proposal_la,\n    fitted_model,\n    1000,\n    subkey,\n    percentiles_of_interest,\n    growth_factor_model(theta0, aux),\n)\n\n\nplt.plot(dates_full[1 : np1 + 1], y_nan.sum(axis=-1))\nplt.scatter(dates_full[np1], preds[0][0], color=\"red\")\nplt.scatter(dates_full[np1], preds[2][1, 0], color=\"grey\")\nplt.scatter(dates_full[np1], preds[2][-2, 0], color=\"grey\")\nplt.scatter(dates_full[np1], y[-1].sum(), color=\"green\", marker=\"x\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# find index of Gütersloh / neighboring Warendorf\njnp.where(ags_full == 5754), jnp.where(ags_full == 5570)\n\n((Array([98], dtype=int64),), (Array([96], dtype=int64),))\n\n\n\nkey, subkey = jrn.split(key)\ncounty_index = jrn.randint(subkey, (1,), 0, 400)[0]\n# Gütersloh\n# county_index = 98\n\n\nplt.title(f\"ags = {ags_full[county_index]}\")\nplt.plot(dates_full[1 : np1 + 1], y_nan[:, county_index])\nplt.scatter(dates_full[np1], preds[0][county_index + 1], color=\"red\")\nplt.scatter(dates_full[np1], preds[2][1, county_index + 1], color=\"grey\")\nplt.scatter(dates_full[np1], preds[2][-2, county_index + 1], color=\"grey\")\nplt.scatter(dates_full[np1], y[-1, county_index], color=\"green\", marker=\"x\")\nplt.show()",
    "crumbs": [
      "4 Models",
      "4.2 Regional growth factor model",
      "Model"
    ]
  },
  {
    "objectID": "4 Models/4.2 Regional growth factor model/data.html",
    "href": "4 Models/4.2 Regional growth factor model/data.html",
    "title": "Data",
    "section": "",
    "text": "Files taken from - https://statistik.arbeitsagentur.de/SiteGlobals/Forms/Suche/Einzelheftsuche_Formular.html?topic_f=beschaeftigung-sozbe-svb-kreise-merkmale (Sozialversicherungspflichtige Beschäftigte je Kreis / Bundesland) - https://statistik.arbeitsagentur.de/SiteGlobals/Forms/Suche/Einzelheftsuche_Formular.html?topic_f=beschaeftigung-sozbe-krpend (Pendler)\n\nlibrary(here)\nsource(here(\"setup.R\"))\n\nsuppressPackageStartupMessages({\n    library(readxl)\n    library(reshape2)\n})\n\nhere() starts at /Users/stefan/workspace/work/phd/thesis\n\n\n\n\n\n\nprocess_commuters &lt;- function(fname) {\n    suppressWarnings({\n        state &lt;- read_excel(fname, sheet = \"Auspendler Kreise\", range = \"A4:A4\", col_names = F, .name_repair = \"unique_quiet\")[[1]]\n\n        outcommuters &lt;- read_excel(fname, sheet = \"Auspendler Kreise\", skip = 6, .name_repair = \"unique_quiet\") %&gt;%\n            tail(-1) %&gt;%\n            select(\"ags_home\" = Wohnort, home = 2, \"ags_work\" = Arbeitsort, work = 4, total = Insgesamt) %&gt;%\n            fill(ags_home, home) %&gt;%\n            filter(!is.na(work)) %&gt;%\n            mutate(total = as.numeric(total))\n\n        incommuters &lt;- read_excel(fname, sheet = \"Einpendler Kreise\", skip = 6, .name_repair = \"unique_quiet\") %&gt;%\n            tail(-1) %&gt;%\n            select(\"ags_work\" = Arbeitsort, work = 2, \"ags_home\" = Wohnort, home = 4, total = Insgesamt) %&gt;%\n            fill(ags_work, work) %&gt;%\n            filter(!is.na(home)) %&gt;%\n            mutate(total = as.numeric(total))\n    })\n    list(\n        incommuters,\n        outcommuters\n    )\n}\ncommuters &lt;- here(\"data/raw\") %&gt;%\n    # use 2022 data, Eisenach is no longer a county\n    list.files(pattern = \"krpend*\", full.names = T) %&gt;%\n    map(process_commuters)\n\nincommuters &lt;- commuters %&gt;%\n    map(function(coms) coms[[1]]) %&gt;%\n    bind_rows()\noutcommuters &lt;- commuters %&gt;%\n    map(function(coms) coms[[2]]) %&gt;%\n    bind_rows()\n\n\n\n\n\nall_ags &lt;- distinct(\n    rbind(\n        select(incommuters, ags = ags_work, name = work),\n        select(incommuters, ags = ags_home, name = home),\n        select(outcommuters, ags = ags_work, name = work),\n        select(outcommuters, ags = ags_home, name = home)\n    ),\n    ags, name\n)\n\nis_county_ags &lt;- function(ags) str_detect(ags, \"^[0-9]{5}$\")\nis_state_ags &lt;- function(ags) str_detect(ags, \"^[0-9]{2}$\")\n\nall_county_ags &lt;- all_ags %&gt;%\n    filter(is_county_ags(ags))\nwrite_csv(all_county_ags, here(\"data/processed/ags_county_dict.csv\"))\n\nall_state_ags &lt;- all_ags %&gt;%\n    filter(is_state_ags(ags)) %&gt;%\n    filter(!name == \"Übrige Regierungsbezirke (Bundesland)\")\nwrite_csv(all_state_ags, here(\"data/processed/ags_state_dict.csv\"))\n\n\nstopifnot(all_state_ags %&gt;% nrow() == 16)\nstopifnot(all_county_ags %&gt;% nrow() == 400)\n\n\n\n\n\ncomplete_ags &lt;- all_county_ags$ags\n\noutcommuters_complete &lt;- outcommuters %&gt;%\n    select(ags_work, ags_home, total) %&gt;%\n    filter(is_county_ags(ags_work), is_county_ags(ags_home)) %&gt;%\n    complete(ags_work = complete_ags, ags_home = complete_ags, fill = list(total = 0))\n\nincommuters_complete &lt;- incommuters %&gt;%\n    select(ags_work, ags_home, total) %&gt;%\n    filter(is_county_ags(ags_work), is_county_ags(ags_home)) %&gt;%\n    complete(ags_work = complete_ags, ags_home = complete_ags, fill = list(total = 0))\n\nstopifnot(\n    \"Outcommuters are not unique\" = nrow(outcommuters_complete) == length(complete_ags)^2,\n    \"Incommuters are not unique\" = nrow(incommuters_complete) == length(complete_ags)^2\n)\nwrite_csv(\n    outcommuters_complete,\n    here(\"data/processed/outcommuters.csv\")\n)\nwrite_csv(\n    incommuters_complete,\n    here(\"data/processed/incommuters.csv\")\n)\n\n\ninner_join(\n    incommuters_complete,\n    outcommuters_complete,\n    by = c(\"ags_work\" = \"ags_home\", \"ags_home\" = \"ags_work\"),\n    suffix = c(\"_in\", \"_out\")\n) %&gt;%\n    group_by(state_work = str_sub(ags_work, 0, 2), state_home = str_sub(ags_home, 0, 2)) %&gt;%\n    summarize(\n        pct_no_exchange = mean(total_in + total_out == 0) * 100\n    ) %&gt;%\n    inner_join(rename(all_state_ags, work = name), by = c(\"state_work\" = \"ags\")) %&gt;%\n    inner_join(rename(all_state_ags, home = name), by = c(\"state_home\" = \"ags\")) %&gt;%\n    ggplot(aes(x = work, y = home, fill = pct_no_exchange)) +\n    geom_tile() +\n    scale_fill_viridis_c() +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 90)) +\n    labs(\n        x = \"Work state\",\n        y = \"Home state\",\n        fill = \"Percentage of counties without exchange\"\n    )\n\n\n\n\n\n\n\n\n\n\n\nHier die passenden Daten ab September ’21 nehmen; ab September 21 ist Eisenach kein eigener Kreis mehr (und kommt in den RKI Zahlen nicht mehr vor).\n\n\n\n\nwork_totals &lt;- read_excel(here(\"data/raw/svb-kreise-merkmale-d-0-202109-xlsx.xlsx\"), sheet = 3, skip = 6, .name_repair = \"unique_quiet\") %&gt;%\n    select(name = 1, ags = 2, total = 3) %&gt;%\n    filter(is_county_ags(ags))\nhome_totals &lt;- read_excel(here(\"data/raw/svb-kreise-merkmale-d-0-202109-xlsx.xlsx\"), sheet = 4, skip = 6, .name_repair = \"unique_quiet\") %&gt;%\n    select(name = 1, ags = 2, total = 3) %&gt;%\n    filter(is_county_ags(ags))\n\nwrite_csv(\n    work_totals,\n    here(\"data/processed/work_totals.csv\")\n)\nwrite_csv(\n    home_totals,\n    here(\"data/processed/home_totals.csv\")\n)\n\n\nstopifnot(work_totals %&gt;% nrow() == 400)\nstopifnot(home_totals %&gt;% nrow() == 400)\n\np_out &lt;- outcommuters_complete %&gt;%\n    arrange(ags_home, ags_work) %&gt;%\n    ggplot(aes(ags_home, ags_work, fill = (total + 1))) +\n    geom_tile() +\n    scale_fill_viridis_c(trans = \"log\") +\n    theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) +\n    theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) +\n    labs(x = \"Home\", y = \"Work\", fill = \"Outcommuters\")\n\np_in &lt;- incommuters_complete %&gt;%\n    arrange(ags_home, ags_work) %&gt;%\n    ggplot(aes(ags_home, ags_work, fill = (total + 1))) +\n    geom_tile() +\n    scale_fill_viridis_c(trans = \"log\") +\n    theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) +\n    theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) +\n    labs(x = \"Home\", y = \"Work\", fill = \"Incommuters\")\n\np_out | p_in",
    "crumbs": [
      "4 Models",
      "4.2 Regional growth factor model",
      "Data"
    ]
  },
  {
    "objectID": "4 Models/4.2 Regional growth factor model/data.html#commuting-data",
    "href": "4 Models/4.2 Regional growth factor model/data.html#commuting-data",
    "title": "Data",
    "section": "",
    "text": "Files taken from - https://statistik.arbeitsagentur.de/SiteGlobals/Forms/Suche/Einzelheftsuche_Formular.html?topic_f=beschaeftigung-sozbe-svb-kreise-merkmale (Sozialversicherungspflichtige Beschäftigte je Kreis / Bundesland) - https://statistik.arbeitsagentur.de/SiteGlobals/Forms/Suche/Einzelheftsuche_Formular.html?topic_f=beschaeftigung-sozbe-krpend (Pendler)\n\nlibrary(here)\nsource(here(\"setup.R\"))\n\nsuppressPackageStartupMessages({\n    library(readxl)\n    library(reshape2)\n})\n\nhere() starts at /Users/stefan/workspace/work/phd/thesis\n\n\n\n\n\n\nprocess_commuters &lt;- function(fname) {\n    suppressWarnings({\n        state &lt;- read_excel(fname, sheet = \"Auspendler Kreise\", range = \"A4:A4\", col_names = F, .name_repair = \"unique_quiet\")[[1]]\n\n        outcommuters &lt;- read_excel(fname, sheet = \"Auspendler Kreise\", skip = 6, .name_repair = \"unique_quiet\") %&gt;%\n            tail(-1) %&gt;%\n            select(\"ags_home\" = Wohnort, home = 2, \"ags_work\" = Arbeitsort, work = 4, total = Insgesamt) %&gt;%\n            fill(ags_home, home) %&gt;%\n            filter(!is.na(work)) %&gt;%\n            mutate(total = as.numeric(total))\n\n        incommuters &lt;- read_excel(fname, sheet = \"Einpendler Kreise\", skip = 6, .name_repair = \"unique_quiet\") %&gt;%\n            tail(-1) %&gt;%\n            select(\"ags_work\" = Arbeitsort, work = 2, \"ags_home\" = Wohnort, home = 4, total = Insgesamt) %&gt;%\n            fill(ags_work, work) %&gt;%\n            filter(!is.na(home)) %&gt;%\n            mutate(total = as.numeric(total))\n    })\n    list(\n        incommuters,\n        outcommuters\n    )\n}\ncommuters &lt;- here(\"data/raw\") %&gt;%\n    # use 2022 data, Eisenach is no longer a county\n    list.files(pattern = \"krpend*\", full.names = T) %&gt;%\n    map(process_commuters)\n\nincommuters &lt;- commuters %&gt;%\n    map(function(coms) coms[[1]]) %&gt;%\n    bind_rows()\noutcommuters &lt;- commuters %&gt;%\n    map(function(coms) coms[[2]]) %&gt;%\n    bind_rows()\n\n\n\n\n\nall_ags &lt;- distinct(\n    rbind(\n        select(incommuters, ags = ags_work, name = work),\n        select(incommuters, ags = ags_home, name = home),\n        select(outcommuters, ags = ags_work, name = work),\n        select(outcommuters, ags = ags_home, name = home)\n    ),\n    ags, name\n)\n\nis_county_ags &lt;- function(ags) str_detect(ags, \"^[0-9]{5}$\")\nis_state_ags &lt;- function(ags) str_detect(ags, \"^[0-9]{2}$\")\n\nall_county_ags &lt;- all_ags %&gt;%\n    filter(is_county_ags(ags))\nwrite_csv(all_county_ags, here(\"data/processed/ags_county_dict.csv\"))\n\nall_state_ags &lt;- all_ags %&gt;%\n    filter(is_state_ags(ags)) %&gt;%\n    filter(!name == \"Übrige Regierungsbezirke (Bundesland)\")\nwrite_csv(all_state_ags, here(\"data/processed/ags_state_dict.csv\"))\n\n\nstopifnot(all_state_ags %&gt;% nrow() == 16)\nstopifnot(all_county_ags %&gt;% nrow() == 400)\n\n\n\n\n\ncomplete_ags &lt;- all_county_ags$ags\n\noutcommuters_complete &lt;- outcommuters %&gt;%\n    select(ags_work, ags_home, total) %&gt;%\n    filter(is_county_ags(ags_work), is_county_ags(ags_home)) %&gt;%\n    complete(ags_work = complete_ags, ags_home = complete_ags, fill = list(total = 0))\n\nincommuters_complete &lt;- incommuters %&gt;%\n    select(ags_work, ags_home, total) %&gt;%\n    filter(is_county_ags(ags_work), is_county_ags(ags_home)) %&gt;%\n    complete(ags_work = complete_ags, ags_home = complete_ags, fill = list(total = 0))\n\nstopifnot(\n    \"Outcommuters are not unique\" = nrow(outcommuters_complete) == length(complete_ags)^2,\n    \"Incommuters are not unique\" = nrow(incommuters_complete) == length(complete_ags)^2\n)\nwrite_csv(\n    outcommuters_complete,\n    here(\"data/processed/outcommuters.csv\")\n)\nwrite_csv(\n    incommuters_complete,\n    here(\"data/processed/incommuters.csv\")\n)\n\n\ninner_join(\n    incommuters_complete,\n    outcommuters_complete,\n    by = c(\"ags_work\" = \"ags_home\", \"ags_home\" = \"ags_work\"),\n    suffix = c(\"_in\", \"_out\")\n) %&gt;%\n    group_by(state_work = str_sub(ags_work, 0, 2), state_home = str_sub(ags_home, 0, 2)) %&gt;%\n    summarize(\n        pct_no_exchange = mean(total_in + total_out == 0) * 100\n    ) %&gt;%\n    inner_join(rename(all_state_ags, work = name), by = c(\"state_work\" = \"ags\")) %&gt;%\n    inner_join(rename(all_state_ags, home = name), by = c(\"state_home\" = \"ags\")) %&gt;%\n    ggplot(aes(x = work, y = home, fill = pct_no_exchange)) +\n    geom_tile() +\n    scale_fill_viridis_c() +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 90)) +\n    labs(\n        x = \"Work state\",\n        y = \"Home state\",\n        fill = \"Percentage of counties without exchange\"\n    )\n\n\n\n\n\n\n\n\n\n\n\nHier die passenden Daten ab September ’21 nehmen; ab September 21 ist Eisenach kein eigener Kreis mehr (und kommt in den RKI Zahlen nicht mehr vor).\n\n\n\n\nwork_totals &lt;- read_excel(here(\"data/raw/svb-kreise-merkmale-d-0-202109-xlsx.xlsx\"), sheet = 3, skip = 6, .name_repair = \"unique_quiet\") %&gt;%\n    select(name = 1, ags = 2, total = 3) %&gt;%\n    filter(is_county_ags(ags))\nhome_totals &lt;- read_excel(here(\"data/raw/svb-kreise-merkmale-d-0-202109-xlsx.xlsx\"), sheet = 4, skip = 6, .name_repair = \"unique_quiet\") %&gt;%\n    select(name = 1, ags = 2, total = 3) %&gt;%\n    filter(is_county_ags(ags))\n\nwrite_csv(\n    work_totals,\n    here(\"data/processed/work_totals.csv\")\n)\nwrite_csv(\n    home_totals,\n    here(\"data/processed/home_totals.csv\")\n)\n\n\nstopifnot(work_totals %&gt;% nrow() == 400)\nstopifnot(home_totals %&gt;% nrow() == 400)\n\np_out &lt;- outcommuters_complete %&gt;%\n    arrange(ags_home, ags_work) %&gt;%\n    ggplot(aes(ags_home, ags_work, fill = (total + 1))) +\n    geom_tile() +\n    scale_fill_viridis_c(trans = \"log\") +\n    theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) +\n    theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) +\n    labs(x = \"Home\", y = \"Work\", fill = \"Outcommuters\")\n\np_in &lt;- incommuters_complete %&gt;%\n    arrange(ags_home, ags_work) %&gt;%\n    ggplot(aes(ags_home, ags_work, fill = (total + 1))) +\n    geom_tile() +\n    scale_fill_viridis_c(trans = \"log\") +\n    theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) +\n    theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) +\n    labs(x = \"Home\", y = \"Work\", fill = \"Incommuters\")\n\np_out | p_in",
    "crumbs": [
      "4 Models",
      "4.2 Regional growth factor model",
      "Data"
    ]
  },
  {
    "objectID": "4 Models/4.2 Regional growth factor model/data.html#rki-data",
    "href": "4 Models/4.2 Regional growth factor model/data.html#rki-data",
    "title": "Data",
    "section": "RKI data",
    "text": "RKI data\n\noptions(timeout = max(300, getOption(\"timeout\")))\nif (!file.exists(here(\"data/raw/RKI.csv\"))) {\n    download.file(\n        \"https://zenodo.org/records/10638065/files/Aktuell_Deutschland_SarsCov2_Infektionen.csv?download=1\", here(\"data/raw/RKI.csv\")\n    )\n}\n\nWHO declares end of COVID-19 health crisis on May 5th 2023 (https://www.who.int/director-general/speeches/detail/who-director-general-s-opening-remarks-at-the-media-briefing—5-may-2023). Measures in Germany ended shortly before (April 2023), so weu se May 5th as final date (can also see reporting drop sharply in spring 2023).\n\nfinal_date &lt;- ymd(\"2023-05-05\")\n\n\nrki_no_meta &lt;- read_csv(here(\"data/raw/RKI.csv\"))\nrki_metadata &lt;- read_csv(here(\"data/raw/RKI_meta.csv\"))\n\nrki_no_meta$IdLandkreis &lt;- str_pad(\n    rki_no_meta$IdLandkreis,\n    5,\n    \"left\",\n    \"0\"\n)\nrki &lt;- inner_join(rki_no_meta, rki_metadata, by = \"IdLandkreis\")\n\nrki$Meldedatum &lt;- as.Date(rki$Meldedatum)\nrki$Refdatum &lt;- as.Date(rki$Refdatum)\n\n\n# RKI also reports differences to yesterdays publication\n# current days cases / deaths are only those where \"Anzahl(Todes)Fall\" is &gt;= 0\nrki[rki$NeuerFall &lt; 0, \"AnzahlFall\"] &lt;- 0\nrki[rki$NeuerTodesfall &lt; 0, \"AnzahlTodesfall\"] &lt;- 0\n\nrki &lt;- filter(rki, Meldedatum &lt;= final_date)\n\n\nrki_county &lt;- rki\ncounty_cases &lt;- NULL\n\nberlin_ids &lt;- 11000:11012\nrki_county[rki_county$IdLandkreis %in% berlin_ids, c(\"Landkreis\", \"IdLandkreis\")] &lt;- list(\"LK Berlin\", \"11000\")\ncounty_cases &lt;- aggregate(cbind(cases = AnzahlFall, deaths = AnzahlTodesfall) ~ Meldedatum + Bundesland + Landkreis, data = rki_county, sum)\ncolnames(county_cases)[1:3] &lt;- c(\"reporting_date\", \"state\", \"county\")\ncounty_cases &lt;- complete(county_cases,\n    reporting_date = seq(min(reporting_date), max(reporting_date), by = \"1 day\"),\n    nesting(state, county),\n    fill = list(cases = 0, deaths = 0)\n)\n\n\ndata/processed/RKI_county.csv\n\ncounty_cases %&gt;%\n    inner_join(\n        rbind(\n            rki_metadata,\n            list(IdBundesland = 11, Bundesland = \"Berlin\", IdLandkreis = \"11000\", Landkreis = \"LK Berlin\")\n        ) %&gt;%\n            arrange(IdLandkreis),\n        by = c(\"county\" = \"Landkreis\")\n    ) %&gt;%\n    select(date = reporting_date, ags = IdLandkreis, cases, deaths) %&gt;%\n    write_csv(here(\"data/processed/RKI_county.csv\"))\n\n\n\ndata/processed/RKI_county_weekly.csv\nWe use the weekly format required in the ECDCs ForecastHub: &gt; Forecast horizons should use the Epidemiological Week (EW) format, defined by the US CDC. Each week starts on Sunday and ends on Saturday.\n\ncounty_cases %&gt;%\n    inner_join(\n        rbind(\n            rki_metadata,\n            list(IdBundesland = 11, Bundesland = \"Berlin\", IdLandkreis = \"11000\", Landkreis = \"LK Berlin\")\n        ) %&gt;%\n            arrange(IdLandkreis),\n        by = c(\"county\" = \"Landkreis\")\n    ) %&gt;%\n    select(date = reporting_date, ags = IdLandkreis, cases, deaths) %&gt;%\n    group_by(date = ceiling_date(date, \"week\", week_start = 7), ags) %&gt;%\n    summarize(cases = sum(cases), deaths = sum(deaths)) %&gt;%\n    write_csv(here(\"data/processed/RKI_county_weekly.csv\"))\n\n\nrki_ags &lt;- unique(rki_county$IdLandkreis)\ncommuters_ags &lt;- unique(outcommuters$ags_home)\n\nstopifnot(length(c(setdiff(commuters_ags, rki_ags), setdiff(rki_ags, commuters_ags))) == 0)",
    "crumbs": [
      "4 Models",
      "4.2 Regional growth factor model",
      "Data"
    ]
  },
  {
    "objectID": "4 Models/4.1 Removing reporting delays and weekday effects/figures.html",
    "href": "4 Models/4.1 Removing reporting delays and weekday effects/figures.html",
    "title": "Figures",
    "section": "",
    "text": "library(here)\nsource(here(\"setup.R\"))\n\nhere() starts at /Users/stefan/workspace/work/phd/thesis\n\n\n\n\n\n\ntheta_showcase &lt;- read_csv(here(\"data/results/4_showcase_model/thetas.csv\")) %&gt;%\n    rename(\"\\\\rho\" = 2)\ntheta_showcase\n\nvariances &lt;- theta_showcase %&gt;%\n    mutate(across(where(is.numeric), ~ exp(.x)))\n\nsds &lt;- variances %&gt;%\n    mutate(across(where(is.numeric), ~ sqrt(.x)))\n\nvar_log_M &lt;- variances %&gt;%\n    filter(method == \"MLE\") %&gt;%\n    pull(M)\n\nvar_M &lt;- exp(var_log_M) - 1\nsd_M &lt;- sqrt(var_M)\n\nc(\"var_M\" = var_M, \"sd_M\" = sd_M)\n\n\nA spec_tbl_df: 3 x 6\n\n\nmethod\n\\rho\nW\nq\nM\nW_q\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nmanual\n-13.815511\n-4.605170\n-1.386294\n-9.210340\n-4.6051702\n\n\ninitial\n-8.422745\n-7.476381\n-4.235409\n-3.988990\n-0.4154693\n\n\nMLE\n-8.422722\n-7.476400\n-4.225735\n-3.988944\n-0.4137567\n\n\n\n\n\nvar_M0.0186918024262615sd_M0.136717966728084\n\n\n\n\n\n# use scientific notation\nsds %&gt;%\n    rename_with(function(col) str_glue(\"$\\\\hat\\\\sigma_{{ {col} }}$\"), where(is.numeric)) %&gt;%\n    kable(format = \"latex\", format.args = list(scipen = 2, digits = 2), booktabs = T, escape = F) %&gt;%\n    cat(., file = here(\"tables/showcase-parameters.tex\"))\n\n\n\n\n\n\nrep_tri &lt;- read_csv(here(\"data/processed/RKI_4day_rt.csv\"))\n\n\n\n\n\ndf_showcase &lt;- read_predictions(\n    here(\"data/results/4_showcase_model/predictions.npy\"),\n    seq(ymd(\"2020-04-05\"), ymd(\"2020-09-01\"), by = \"1 day\"),\n    c(\"I\", \"$\\\\rho$\", \"M\", \"W\", \"running_W\", \"$p^s_1$\", \"$p^s_2$\", \"$p^s_3$\", \"$p^s_4$\", \"$p_1$\", \"$p_2$\", \"$p_3$\", \"$p_4$\", \"$W_{q_1}\", \"W_{q_2}\", \"W_{q_3}\")\n)\n\n\n\n\ntotal_df &lt;- rep_tri %&gt;%\n    mutate(total = `1` + `2` + `3` + `4`) %&gt;%\n    select(date = county_date, total)\n\nplt_I &lt;- df_showcase %&gt;%\n    filter(variable == \"I\") %&gt;%\n    ggplot(aes(date, mean)) +\n    geom_ribbon(aes(date, ymin = `0.025`, ymax = `0.975`), fill = \"darkgreen\", alpha = .3) +\n    geom_line(aes(linetype = \"mean prediction\")) +\n    geom_line(aes(y = total, linetype = \"total reported cases\"), data = filter(total_df, date &lt;= max(df_showcase$date))) +\n    labs(x = \"\", y = \"$I$\", linetype = \"\") +\n    scale_x_four_weekly()\n\nplt_rho &lt;- df_showcase %&gt;%\n    filter(variable == \"$\\\\rho$\") %&gt;%\n    ggplot(aes(date, mean)) +\n    geom_ribbon(aes(date, ymin = `0.025`, ymax = `0.975`), fill = \"darkgreen\", alpha = .3) +\n    geom_line() +\n    geom_hline(yintercept = 1, linetype = \"dashed\", color = \"grey80\", size = 2) +\n    # geom_text(x = min(df_showcase$date), y = 1.00, label = \"threshold for \\n exponential growth\", color = \"grey80\", vjust = -1, hjust = .15, size = 2) +\n    labs(x = \"\", y = \"$\\\\rho$\") +\n    scale_y_continuous(\n        sec.axis = sec_axis(~ .^7, name = \"$\\\\rho^7$\", breaks = round(c(.9, .95, 1., 1.05, 1.1, 1.15)^7, 2))\n    ) +\n    scale_x_four_weekly()\n\nplt_I / plt_rho + plot_layout(guides = \"collect\") & theme(legend.position = \"top\")\n\nggsave_tikz(here(\"tikz/showcase_prediction_intervals_I_rho.tex\"), height = default_height)\n\nWarning message:\n\"Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\ni Please use `linewidth` instead.\"\n\n\npdf: 2\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt_MW &lt;- df_showcase %&gt;%\n    filter((variable %in% c(\"M\", \"W\"))) %&gt;%\n    ggplot(aes(date, mean)) +\n    geom_ribbon(aes(date, ymin = `0.025`, ymax = `0.975`), fill = \"darkgreen\", alpha = .3) +\n    geom_line() +\n    facet_wrap(~variable, scales = \"free_y\", ncol = 1) +\n    labs(x = \"\", y = \"\")\n\nplt_p &lt;- df_showcase %&gt;%\n    filter(str_starts(variable, \"\\\\$p\")) %&gt;%\n    mutate(delay = str_extract(variable, \"\\\\d+\")) %&gt;%\n    select(date, mean, variable, delay) %&gt;%\n    mutate(variable = ifelse(str_detect(variable, \"s\"), \"smoothed\", \"signal\")) %&gt;%\n    ggplot(aes(date, 100 * mean, color = delay, linetype = variable)) +\n    geom_line() +\n    scale_y_continuous(breaks = 20 * 0:5, limits = c(0, 1) * 100) +\n    scale_linetype_manual(values = c(\"signal\" = \"dotted\", \"smoothed\" = \"solid\")) +\n    labs(color = \"$\\\\tau$\", x = \"\", y = \"probability of delay [\\\\%]\", linetype = \"\")\n\nplt_MW / plt_p + plot_layout(guides = \"collect\") & theme(legend.position = \"bottom\") & scale_x_four_weekly()\n# theme(axis.text.x = element_text(angle = 45, hjust = 1))\nggsave_tikz(here(\"tikz/showcase_prediction_intervals.tex\"), height = 1.5 * default_height)\n\npdf: 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf_showcase %&gt;%\n    filter(variable == \"W\") %&gt;%\n    pull(mean) %&gt;%\n    head(-1) %&gt;%\n    matrix(nrow = 7) %&gt;%\n    t() %&gt;%\n    matplot(type = \"l\")\nlegend(\"bottomright\", lty = 1:7, legend = c(\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"), col = 1:7, cex = 0.8, bty = \"n\")\n\nWarning message in matrix(., nrow = 7):\n\"Datenl\"ange [149] ist kein Teiler oder Vielfaches der Anzahl der Zeilen [7]\"\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf_showcase %&gt;%\n    filter(variable == \"running_W\") %&gt;%\n    select(mean, sd) %&gt;%\n    head(-3) %&gt;%\n    tail(-3) %&gt;%\n    summarize(mean(.$mean), mean(.$sd))\n\n\nA tibble: 1 x 2\n\n\nmean(.$mean)\nmean(.$sd)\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1.057787\n0.006446631\n\n\n\n\n\n\ndf_showcase %&gt;%\n    filter(variable == \"running_W\") %&gt;%\n    ggplot(aes(date, mean)) +\n    geom_line() +\n    geom_ribbon(aes(ymin = `0.025`, ymax = `0.975`), fill = \"darkgreen\", alpha = .3)\n\n\n\n\n\n\n\n\n\n\n\n\nmean_Ms &lt;- df_showcase %&gt;%\n    filter(variable == \"M\") %&gt;%\n    pull(mean)\n\nmean(mean_Ms)\n\nqqnorm(log(mean_Ms))\nqqline(log(mean_Ms))\n\n0.99978390991431",
    "crumbs": [
      "4 Models",
      "4.1 Removing reporting delays and weekday effects",
      "Figures"
    ]
  },
  {
    "objectID": "4 Models/4.1 Removing reporting delays and weekday effects/figures.html#showcase",
    "href": "4 Models/4.1 Removing reporting delays and weekday effects/figures.html#showcase",
    "title": "Figures",
    "section": "",
    "text": "library(here)\nsource(here(\"setup.R\"))\n\nhere() starts at /Users/stefan/workspace/work/phd/thesis\n\n\n\n\n\n\ntheta_showcase &lt;- read_csv(here(\"data/results/4_showcase_model/thetas.csv\")) %&gt;%\n    rename(\"\\\\rho\" = 2)\ntheta_showcase\n\nvariances &lt;- theta_showcase %&gt;%\n    mutate(across(where(is.numeric), ~ exp(.x)))\n\nsds &lt;- variances %&gt;%\n    mutate(across(where(is.numeric), ~ sqrt(.x)))\n\nvar_log_M &lt;- variances %&gt;%\n    filter(method == \"MLE\") %&gt;%\n    pull(M)\n\nvar_M &lt;- exp(var_log_M) - 1\nsd_M &lt;- sqrt(var_M)\n\nc(\"var_M\" = var_M, \"sd_M\" = sd_M)\n\n\nA spec_tbl_df: 3 x 6\n\n\nmethod\n\\rho\nW\nq\nM\nW_q\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nmanual\n-13.815511\n-4.605170\n-1.386294\n-9.210340\n-4.6051702\n\n\ninitial\n-8.422745\n-7.476381\n-4.235409\n-3.988990\n-0.4154693\n\n\nMLE\n-8.422722\n-7.476400\n-4.225735\n-3.988944\n-0.4137567\n\n\n\n\n\nvar_M0.0186918024262615sd_M0.136717966728084\n\n\n\n\n\n# use scientific notation\nsds %&gt;%\n    rename_with(function(col) str_glue(\"$\\\\hat\\\\sigma_{{ {col} }}$\"), where(is.numeric)) %&gt;%\n    kable(format = \"latex\", format.args = list(scipen = 2, digits = 2), booktabs = T, escape = F) %&gt;%\n    cat(., file = here(\"tables/showcase-parameters.tex\"))\n\n\n\n\n\n\nrep_tri &lt;- read_csv(here(\"data/processed/RKI_4day_rt.csv\"))\n\n\n\n\n\ndf_showcase &lt;- read_predictions(\n    here(\"data/results/4_showcase_model/predictions.npy\"),\n    seq(ymd(\"2020-04-05\"), ymd(\"2020-09-01\"), by = \"1 day\"),\n    c(\"I\", \"$\\\\rho$\", \"M\", \"W\", \"running_W\", \"$p^s_1$\", \"$p^s_2$\", \"$p^s_3$\", \"$p^s_4$\", \"$p_1$\", \"$p_2$\", \"$p_3$\", \"$p_4$\", \"$W_{q_1}\", \"W_{q_2}\", \"W_{q_3}\")\n)\n\n\n\n\ntotal_df &lt;- rep_tri %&gt;%\n    mutate(total = `1` + `2` + `3` + `4`) %&gt;%\n    select(date = county_date, total)\n\nplt_I &lt;- df_showcase %&gt;%\n    filter(variable == \"I\") %&gt;%\n    ggplot(aes(date, mean)) +\n    geom_ribbon(aes(date, ymin = `0.025`, ymax = `0.975`), fill = \"darkgreen\", alpha = .3) +\n    geom_line(aes(linetype = \"mean prediction\")) +\n    geom_line(aes(y = total, linetype = \"total reported cases\"), data = filter(total_df, date &lt;= max(df_showcase$date))) +\n    labs(x = \"\", y = \"$I$\", linetype = \"\") +\n    scale_x_four_weekly()\n\nplt_rho &lt;- df_showcase %&gt;%\n    filter(variable == \"$\\\\rho$\") %&gt;%\n    ggplot(aes(date, mean)) +\n    geom_ribbon(aes(date, ymin = `0.025`, ymax = `0.975`), fill = \"darkgreen\", alpha = .3) +\n    geom_line() +\n    geom_hline(yintercept = 1, linetype = \"dashed\", color = \"grey80\", size = 2) +\n    # geom_text(x = min(df_showcase$date), y = 1.00, label = \"threshold for \\n exponential growth\", color = \"grey80\", vjust = -1, hjust = .15, size = 2) +\n    labs(x = \"\", y = \"$\\\\rho$\") +\n    scale_y_continuous(\n        sec.axis = sec_axis(~ .^7, name = \"$\\\\rho^7$\", breaks = round(c(.9, .95, 1., 1.05, 1.1, 1.15)^7, 2))\n    ) +\n    scale_x_four_weekly()\n\nplt_I / plt_rho + plot_layout(guides = \"collect\") & theme(legend.position = \"top\")\n\nggsave_tikz(here(\"tikz/showcase_prediction_intervals_I_rho.tex\"), height = default_height)\n\nWarning message:\n\"Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\ni Please use `linewidth` instead.\"\n\n\npdf: 2\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt_MW &lt;- df_showcase %&gt;%\n    filter((variable %in% c(\"M\", \"W\"))) %&gt;%\n    ggplot(aes(date, mean)) +\n    geom_ribbon(aes(date, ymin = `0.025`, ymax = `0.975`), fill = \"darkgreen\", alpha = .3) +\n    geom_line() +\n    facet_wrap(~variable, scales = \"free_y\", ncol = 1) +\n    labs(x = \"\", y = \"\")\n\nplt_p &lt;- df_showcase %&gt;%\n    filter(str_starts(variable, \"\\\\$p\")) %&gt;%\n    mutate(delay = str_extract(variable, \"\\\\d+\")) %&gt;%\n    select(date, mean, variable, delay) %&gt;%\n    mutate(variable = ifelse(str_detect(variable, \"s\"), \"smoothed\", \"signal\")) %&gt;%\n    ggplot(aes(date, 100 * mean, color = delay, linetype = variable)) +\n    geom_line() +\n    scale_y_continuous(breaks = 20 * 0:5, limits = c(0, 1) * 100) +\n    scale_linetype_manual(values = c(\"signal\" = \"dotted\", \"smoothed\" = \"solid\")) +\n    labs(color = \"$\\\\tau$\", x = \"\", y = \"probability of delay [\\\\%]\", linetype = \"\")\n\nplt_MW / plt_p + plot_layout(guides = \"collect\") & theme(legend.position = \"bottom\") & scale_x_four_weekly()\n# theme(axis.text.x = element_text(angle = 45, hjust = 1))\nggsave_tikz(here(\"tikz/showcase_prediction_intervals.tex\"), height = 1.5 * default_height)\n\npdf: 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf_showcase %&gt;%\n    filter(variable == \"W\") %&gt;%\n    pull(mean) %&gt;%\n    head(-1) %&gt;%\n    matrix(nrow = 7) %&gt;%\n    t() %&gt;%\n    matplot(type = \"l\")\nlegend(\"bottomright\", lty = 1:7, legend = c(\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"), col = 1:7, cex = 0.8, bty = \"n\")\n\nWarning message in matrix(., nrow = 7):\n\"Datenl\"ange [149] ist kein Teiler oder Vielfaches der Anzahl der Zeilen [7]\"\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf_showcase %&gt;%\n    filter(variable == \"running_W\") %&gt;%\n    select(mean, sd) %&gt;%\n    head(-3) %&gt;%\n    tail(-3) %&gt;%\n    summarize(mean(.$mean), mean(.$sd))\n\n\nA tibble: 1 x 2\n\n\nmean(.$mean)\nmean(.$sd)\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1.057787\n0.006446631\n\n\n\n\n\n\ndf_showcase %&gt;%\n    filter(variable == \"running_W\") %&gt;%\n    ggplot(aes(date, mean)) +\n    geom_line() +\n    geom_ribbon(aes(ymin = `0.025`, ymax = `0.975`), fill = \"darkgreen\", alpha = .3)\n\n\n\n\n\n\n\n\n\n\n\n\nmean_Ms &lt;- df_showcase %&gt;%\n    filter(variable == \"M\") %&gt;%\n    pull(mean)\n\nmean(mean_Ms)\n\nqqnorm(log(mean_Ms))\nqqline(log(mean_Ms))\n\n0.99978390991431",
    "crumbs": [
      "4 Models",
      "4.1 Removing reporting delays and weekday effects",
      "Figures"
    ]
  },
  {
    "objectID": "4 Models/4.1 Removing reporting delays and weekday effects/figures.html#christmas",
    "href": "4 Models/4.1 Removing reporting delays and weekday effects/figures.html#christmas",
    "title": "Figures",
    "section": "Christmas",
    "text": "Christmas\n\nparameters\n\ntheta_christmas &lt;- read_csv(here(\"data/results/4_christmas_model/thetas.csv\")) %&gt;%\n    rename(\"\\\\rho\" = 2)\ntheta_christmas_miss &lt;- read_csv(here(\"data/results/4_christmas_model/thetas_miss.csv\")) %&gt;%\n    rename(\"\\\\rho\" = 2)\n\nsds_christmas &lt;- theta_christmas %&gt;%\n    mutate(across(where(is.numeric), ~ exp(.x / 2)))\nsds_christmas_miss &lt;- theta_christmas_miss %&gt;%\n    mutate(across(where(is.numeric), ~ exp(.x / 2)))\n\n\ntables/christmas-parameters.tex\n\n# use scientific notation\nsds_christmas %&gt;%\n    mutate(model = \"full\") %&gt;%\n    bind_rows(sds_christmas_miss %&gt;% mutate(model = \"missing\")) %&gt;%\n    select(-model) %&gt;%\n    rename_with(function(col) str_glue(\"$\\\\hat\\\\sigma_{{ {col} }}$\"), where(is.numeric)) %&gt;%\n    kable(format = \"latex\", format.args = list(scipen = 2, digits = 2), booktabs = T, escape = F) %&gt;%\n    pack_rows(\"all observations\", 1, 3) %&gt;%\n    pack_rows(\"Christmas removed\", 4, 6) %&gt;%\n    cat(., file = here(\"tables/christmas-parameters.tex\"))\n\n\n\n\nPredictions\n\ndf_christmas &lt;- read_predictions(\n    here(\"data/results/4_christmas_model/predictions.npy\"),\n    seq(ymd(\"2020-10-01\"), ymd(\"2021-02-28\"), by = \"1 day\"),\n    c(\"I\", \"$\\\\rho$\", \"M\", \"W\", \"running_W\", \"$p^s_1$\", \"$p^s_2$\", \"$p^s_3$\", \"$p^s_4$\", \"$p_1$\", \"$p_2$\", \"$p_3$\", \"$p_4$\", \"$W_{q_1}\", \"W_{q_2}\", \"W_{q_3}\", \"y_christmas\")\n)\n\ndf_christmas_missing &lt;- read_predictions(\n    here(\"data/results/4_christmas_model/predictions_miss.npy\"),\n    seq(ymd(\"2020-10-01\"), ymd(\"2021-02-28\"), by = \"1 day\"),\n    c(\"I\", \"$\\\\rho$\", \"M\", \"W\", \"running_W\", \"$p^s_1$\", \"$p^s_2$\", \"$p^s_3$\", \"$p^s_4$\", \"$p_1$\", \"$p_2$\", \"$p_3$\", \"$p_4$\", \"$W_{q_1}\", \"W_{q_2}\", \"W_{q_3}\", \"y_christmas\")\n)\n\n\nmissing_indices &lt;- 80:109\ndates_missing &lt;- sort(unique(df_christmas$date))[missing_indices]\nrange(dates_missing)\n\n\n2020-12-192021-01-17\n\n\n\n\npredictive distribution of total number of cases\n\nimputed &lt;- df_christmas_missing %&gt;%\n    filter(variable == \"y_christmas\") %&gt;%\n    head(1) %&gt;%\n    select(-date, -variable) %&gt;%\n    pivot_longer(everything()) %&gt;%\n    deframe()\n\nremoved &lt;- total_df %&gt;%\n    filter(date %in% dates_missing) %&gt;%\n    summarize(sum(total)) %&gt;%\n    pull()\n\nf_digit &lt;- function(x) format(round(x, -3), big.mark = \",\")\nstr_glue(\"Removed {format(removed, big.mark=',')} cases\")\nstr_glue(\"Imputed cases: {f_digit(imputed['mean'])} +- {f_digit(imputed['sd'])}, 95% PI {f_digit(imputed['0.025'])} - {f_digit(imputed['0.975'])} \")\n\n'Removed 551,031 cases'\n\n\n'Imputed cases: 618,000 +- 59,000, 95% PI 511,000 - 743,000 '\n\n\n\ndf_plot &lt;- rbind(\n    mutate(df_christmas, model = \"all observations\"),\n    mutate(df_christmas_missing, model = \"Christmas removed\")\n) %&gt;%\n    select(model, date, variable, mean, `0.025`, `0.975`) %&gt;%\n    filter(variable %in% c(\"I\", \"$\\\\rho$\"))\n\ntotal_df_smoothed &lt;- total_df %&gt;%\n    mutate(total = rollmean(total, 7, 0, True, align = \"center\")) %&gt;%\n    filter(date %in% unique(df_plot$date)) %&gt;%\n    mutate(model = \"weekly average cases\")\n\nplt_I &lt;- df_plot %&gt;%\n    filter(variable == \"I\") %&gt;%\n    ggplot(aes(date, mean, color = model)) +\n    geom_rect(xmin = min(dates_missing), xmax = max(dates_missing), ymin = -Inf, ymax = Inf, fill = \"gray\", alpha = .01, inherit.aes = F) +\n    geom_line(aes(date, total), data = total_df_smoothed) +\n    # geom_ribbon(aes(ymin = `0.025`, ymax = `0.975`, fill = model), alpha = .3) +\n    geom_line(aes(y = `0.025`, color = model), linetype = \"dotted\") +\n    geom_line(aes(y = `0.975`, color = model), linetype = \"dotted\") +\n    geom_line() +\n    labs(x = \"\", y = \"$I$\", fill = \"\", color = \"\") +\n    scale_color_manual(values = c(\"all observations\" = pal_npg()(3)[1], \"Christmas removed\" = pal_npg()(3)[2], \"weekly average cases\" = \"black\"))\n\nplt_rho &lt;- df_plot %&gt;%\n    filter(variable == \"$\\\\rho$\") %&gt;%\n    ggplot(aes(date, mean, color = model)) +\n    # geom_rect(xmin = min(dates_missing), xmax = max(dates_missing), ymin = -Inf, ymax = Inf, fill = \"gray80\", alpha = .01, inherit.aes = F) +\n    # geom_ribbon(aes(ymin = `0.025`, ymax = `0.975`, fill = model), alpha = .3) +\n    geom_line(aes(y = `0.025`, color = model), linetype = \"dotted\", show.legend = F) +\n    geom_line(aes(y = `0.975`, color = model), linetype = \"dotted\", show.legend = F) +\n    geom_line(show.legend = F) +\n    labs(x = \"\", y = \"$\\\\rho$\", fill = \"\", color = \"\")\n\n\nplt_I / plt_rho + plot_layout(guides = \"collect\") & theme(legend.position = \"bottom\") & scale_x_four_weekly()\n\nggsave_tikz(here(\"tikz/christmas_prediction_intervals_I_rho.tex\"))\n\npdf: 2\n\n\n\n\n\n\n\n\n\n\nmedian in christmas period smaller than mean for missing model\n\ndf_christmas_missing %&gt;%\n    filter(variable == \"I\") %&gt;%\n    select(date, mean, `0.5`) %&gt;%\n    pivot_longer(-date) %&gt;%\n    ggplot(aes(date, value, color = name)) +\n    geom_line()\n\n\n\n\n\n\n\n\n\n\ntikz/christmas_delay_probs.tex\n\ndf_plot &lt;- rbind(\n    mutate(df_christmas, model = \"all observations\"),\n    mutate(df_christmas_missing, model = \"Christmas removed\")\n) %&gt;%\n    select(model, date, variable, mean, `0.025`, `0.975`) %&gt;%\n    filter(str_starts(variable, \"\\\\$p\\\\^s\")) %&gt;%\n    mutate(delay = str_extract(variable, \"\\\\d+\")) %&gt;%\n    select(-variable)\n\n\ndf_plot %&gt;%\n    ggplot(aes(date, 100 * mean, color = delay, linetype = model)) +\n    geom_line() +\n    ylim(0, 100) +\n    scale_x_four_weekly() +\n    labs(x = \"\", y = \"probability of delay [\\\\%]\", color = \"$\\\\tau$\", linetype = \"\") +\n    theme(legend.position = \"bottom\")\n\nggsave_tikz(here(\"tikz/christmas_delay_probs.tex\"), height = 1 / 2 * default_height)\n\npdf: 2",
    "crumbs": [
      "4 Models",
      "4.1 Removing reporting delays and weekday effects",
      "Figures"
    ]
  },
  {
    "objectID": "4 Models/4.1 Removing reporting delays and weekday effects/figures.html#predictions-2",
    "href": "4 Models/4.1 Removing reporting delays and weekday effects/figures.html#predictions-2",
    "title": "Figures",
    "section": "Predictions",
    "text": "Predictions",
    "crumbs": [
      "4 Models",
      "4.1 Removing reporting delays and weekday effects",
      "Figures"
    ]
  },
  {
    "objectID": "4 Models/4.1 Removing reporting delays and weekday effects/showcase_model.html",
    "href": "4 Models/4.1 Removing reporting delays and weekday effects/showcase_model.html",
    "title": "Application 1: Showcase",
    "section": "",
    "text": "from ssm4epi.models.reporting_delays import (\n    _model,\n    to_log_probs,\n    n_iterations,\n    N_meis,\n    N_mle,\n    N_posterior,\n    key,\n    percentiles_of_interest,\n)\nfrom pyprojroot.here import here\nimport jax\n\njax.config.update(\"jax_enable_x64\", True)\nimport pandas as pd\nimport jax.random as jrn\nfrom jax import numpy as jnp, vmap\nfrom isssm.laplace_approximation import laplace_approximation as LA\nfrom isssm.modified_efficient_importance_sampling import (\n    modified_efficient_importance_sampling as MEIS,\n)\nfrom isssm.estimation import mle_pgssm, initial_theta\nfrom isssm.importance_sampling import (\n    pgssm_importance_sampling,\n    ess_pct,\n    mc_integration,\n    prediction_percentiles,\n    normalize_weights,\n)\nfrom isssm.kalman import state_mode\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nmpl.rcParams[\"figure.figsize\"] = (20, 6)\ni_start = 0\nnp1 = 150\ndf = pd.read_csv(here() / \"data/processed/RKI_4day_rt.csv\")\ndates = pd.to_datetime(df.iloc[i_start : i_start + np1, 0])\ny = jnp.asarray(df.iloc[i_start : i_start + np1, 1:].to_numpy())\n\nplt.plot(dates, y)\nplt.title(f\"Incidences by delay from {dates[0]} to {dates[np1-1]}\")\nplt.show()\ntheta_manual = jnp.log(\n    # s2_log_rho, s2_W, s2_q, s2_M, s2_Wq\n    jnp.array([0.001**2, 0.1**2, 0.5**2, 0.01**2, 0.1**2])\n)\naux = (np1, 4)\nintial_result = initial_theta(y, _model, theta_manual, aux, n_iterations)\ntheta_0 = intial_result.x\nintial_result\n\n  message: Optimization terminated successfully.\n  success: True\n   status: 0\n      fun: 5.692800563209951\n        x: [-8.423e+00 -7.476e+00 -4.235e+00 -3.989e+00 -4.155e-01]\n      nit: 54\n      jac: [ 2.859e-08  9.477e-08 -5.204e-07 -2.981e-07  3.293e-07]\n hess_inv: [[ 2.131e+02  9.608e+00 ... -2.118e+01  1.941e-01]\n            [ 9.608e+00  1.806e+02 ... -1.266e+01  4.999e-02]\n            ...\n            [-2.118e+01 -1.266e+01 ...  1.930e+01  1.884e-01]\n            [ 1.941e-01  4.999e-02 ...  1.884e-01  8.456e+00]]\n     nfev: 660\n     njev: 60\nkey, subkey = jrn.split(key)\nmle_result = mle_pgssm(y, _model, theta_0, aux, n_iterations, N_mle, subkey)\ntheta_hat = mle_result.x\nmle_result\n\n  message: Optimization terminated successfully.\n  success: True\n   status: 0\n      fun: 5.691591168364818\n        x: [-8.423e+00 -7.476e+00 -4.226e+00 -3.989e+00 -4.138e-01]\n      nit: 9\n      jac: [ 1.390e-07  2.116e-07  5.007e-08  1.899e-06 -1.203e-08]\n hess_inv: [[ 1.001e+00 -4.029e-04 ...  1.199e-03 -1.249e-02]\n            [-4.029e-04  1.000e+00 ... -3.977e-04  3.226e-02]\n            ...\n            [ 1.199e-03 -3.977e-04 ...  1.005e+00  8.657e-02]\n            [-1.249e-02  3.226e-02 ...  8.657e-02  8.145e+00]]\n     nfev: 121\n     njev: 11\ns_manual = jnp.exp(theta_manual / 2)\ns_0 = jnp.exp(theta_0 / 2)\ns_mle = jnp.exp(theta_hat / 2)\n\nk = theta_manual.size\nplt.scatter(jnp.arange(k) - 0.2, s_manual, label=\"Manual\")\nplt.scatter(jnp.arange(k), s_0, label=\"Initial\")\nplt.scatter(jnp.arange(k) + 0.2, s_mle, label=\"MLE\")\nplt.xticks(jnp.arange(k), [\"$\\\\log \\\\rho$\", \"$W$\", \"$q$\", \"D\", \"$W_q$\"])\nplt.legend()\nplt.show()\nfitted_model = _model(theta_hat, aux)\nproposal_la, info_la = LA(y, fitted_model, n_iterations)\nkey, subkey = jrn.split(key)\nproposal_meis, info_meis = MEIS(\n    y, fitted_model, proposal_la.z, proposal_la.Omega, n_iterations, N_meis, subkey\n)\nkey, subkey = jrn.split(key)\nsamples, lw = pgssm_importance_sampling(\n    y, fitted_model, proposal_meis.z, proposal_meis.Omega, N_posterior, subkey\n)\ness_pct(lw)\n\nArray(26.97016876, dtype=float64)\nstate_modes_meis = vmap(state_mode, (None, 0))(fitted_model, samples)\nx_smooth = mc_integration(state_modes_meis, lw)\nx_lower, x_mid, x_upper = prediction_percentiles(\n    state_modes_meis, normalize_weights(lw), jnp.array([2.5, 50.0, 97.5]) / 100.0\n)\n\n# I_smooth = jnp.exp(x_smooth[:, 0])\nI_smooth = mc_integration(jnp.exp(state_modes_meis[:, :, 0]), lw)\nrho_smooth = jnp.exp(x_smooth[:, 1])\nD_smooth = jnp.exp(x_smooth[:, 2])\nW_smooth = jnp.exp(x_smooth[:, 3])\nlog_ratios = x_smooth[:, 9:12]\nlog_probs = to_log_probs(log_ratios)\n\nweekday_log_ratios = x_smooth[:, jnp.array([12, 18, 24])]\n\nfig, axs = plt.subplots(4, 2, figsize=(15, 10))\n\naxs = axs.flatten()\nfig.tight_layout()\n\naxs[0].set_title(\"incidences\")\naxs[0].plot(dates, I_smooth, label=\"$I_t$\")\n# axs[0].plot(dates, jnp.exp(x_lower[:, 0]), color=\"black\", linestyle=\"dashed\")\naxs[0].plot(dates, y.sum(axis=1), label=\"$Y_t$\", color=\"grey\", alpha=0.5)\naxs[0].legend()\n\naxs[1].set_title(\"growth factor\")\naxs[1].plot(dates, jnp.exp(x_lower[:, 1]), color=\"grey\", alpha=0.5)\naxs[1].plot(dates, jnp.exp(x_upper[:, 1]), color=\"grey\", alpha=0.5)\naxs[1].plot(dates, rho_smooth, label=\"$\\\\log \\\\rho_t$\")\n\naxs[2].set_title(\"weekday effect\")\naxs[2].plot(dates, W_smooth, label=\"$W_t$\")\n\naxs[3].set_title(\"delay probabilities\")\n\naxs[3].plot(dates, jnp.exp(log_probs[:, 0]), label=\"$p_{t, 1}$\")\naxs[3].plot(dates, jnp.exp(log_probs[:, 1]), label=\"$p_{t, 2}$\")\naxs[3].plot(dates, jnp.exp(log_probs[:, 2]), label=\"$p_{t, 3}$\")\naxs[3].plot(dates, jnp.exp(log_probs[:, 3]), label=\"$p_{t, 4}$\")\naxs[3].plot(dates, jnp.exp(log_probs).sum(axis=1), label=\"total p\")\naxs[3].legend()\n\naxs[4].set_title(\"Log ratios\")\naxs[4].plot(dates, log_ratios[:, 0], label=\"$q_{t, 1}$\")\naxs[4].plot(dates, log_ratios[:, 1], label=\"$q_{t, 2}$\")\naxs[4].plot(dates, log_ratios[:, 2], label=\"$q_{t, 3}$\")\nfor d in dates[::7]:\n    axs[4].axvline(d, color=\"black\", alpha=0.2)\n\naxs[5].set_title(\"Dirt\")\naxs[5].plot(dates, D_smooth)\n\naxs[6].set_title(\"Weekday effect log ratios\")\naxs[6].plot(dates, weekday_log_ratios[:, 0], label=\"$W_{t, 1}$\")\naxs[6].plot(dates, weekday_log_ratios[:, 1], label=\"$W_{t, 2}$\")\naxs[6].plot(dates, weekday_log_ratios[:, 2], label=\"$W_{t, 3}$\")\naxs[6].legend()\nplt.show()\nfrom isssm.importance_sampling import prediction\n\n\ndef f_I(x, s, y_prime):\n    return jnp.exp(x[:, 0:1])\n\n\npercentiles_of_interest = jnp.array(\n    [0.01, 0.025, *(0.05 * jnp.arange(1, 20)), 0.975, 0.99]\n)\nmean, sd, quantiles = prediction(\n    f_I, y, proposal_meis, fitted_model, 1000, key, percentiles_of_interest\n)\nplt.plot(dates, mean, label=\"mean\")\nplt.plot(dates, y.sum(axis=-1), label=\"observations\")\nplt.plot(dates, quantiles[0], linestyle=\"dashed\", color=\"black\", alpha=0.3)\nplt.plot(dates, quantiles[12], linestyle=\"dotted\", color=\"black\")\nplt.plot(dates, quantiles[-1], linestyle=\"dashed\", color=\"black\", alpha=0.3)\nplt.legend()\nplt.show()",
    "crumbs": [
      "4 Models",
      "4.1 Removing reporting delays and weekday effects",
      "Application 1: Showcase"
    ]
  },
  {
    "objectID": "4 Models/4.1 Removing reporting delays and weekday effects/showcase_model.html#storing-results",
    "href": "4 Models/4.1 Removing reporting delays and weekday effects/showcase_model.html#storing-results",
    "title": "Application 1: Showcase",
    "section": "Storing results",
    "text": "Storing results\n\n# theta\ndf_theta = pd.DataFrame.from_records(\n    jnp.vstack([theta_manual, theta_0, theta_hat]),\n    columns=[\"log rho\", \"W\", \"q\", \"M\", \"W_q\"],\n    index=[\"manual\", \"initial\", \"MLE\"],\n)\ndf_theta.to_csv(\n    here() / \"data/results/4_showcase_model/thetas.csv\", index_label=\"method\"\n)\ndf_theta\n\n\n\n\n\n\n\n\nlog rho\nW\nq\nM\nW_q\n\n\n\n\nmanual\n-13.815510557964274\n-4.605170185988091\n-1.3862943611198906\n-9.210340371976182\n-4.605170185988091\n\n\ninitial\n-8.4227449439452\n-7.476380644806974\n-4.23540895789563\n-3.9889897766936477\n-0.4154693091042875\n\n\nMLE\n-8.422721513295569\n-7.476399652760843\n-4.225735089851275\n-3.988944142986405\n-0.41375665515244603\n\n\n\n\n\n\n\n\nfrom isssm.importance_sampling import prediction\nfrom jaxtyping import Float, Array\n\n# predictions\n\n# date / name / mean / sd / percentiles\n\nkey, subkey_prediction = jrn.split(key)\n\n\ndef f_predict(x, s, y):\n    probs = jnp.exp(to_log_probs(x[:, 9:12]))\n    probs_signal = jnp.exp(to_log_probs(s[:, 1:]))\n    I = jnp.exp(x[:, 0:1])\n    rho = jnp.exp(x[:, 1:2])\n    M = jnp.exp(x[:, 2:3])\n    W = jnp.exp(x[:, 3:4])\n    runn_W = jnp.convolve(jnp.exp(x[:, 3]), jnp.ones(7) / 7, mode=\"same\")[:, None]\n    Wq = jnp.exp(x[:, jnp.array([12, 18, 24])])\n    return jnp.concatenate(\n        [I, rho, M, W, runn_W, probs, probs_signal, Wq],\n        -1,\n    )\n\n\ndef stacked_prediction(f):\n    mean, sd, quantiles = prediction(\n        f,\n        y,\n        proposal_meis,\n        fitted_model,\n        N_posterior,\n        subkey_prediction,\n        percentiles_of_interest,\n    )\n    return jnp.vstack((mean[None], sd[None], quantiles))\n\n\njnp.save(\n    here() / \"data/results/4_showcase_model/predictions.npy\",\n    stacked_prediction(f_predict),\n)",
    "crumbs": [
      "4 Models",
      "4.1 Removing reporting delays and weekday effects",
      "Application 1: Showcase"
    ]
  },
  {
    "objectID": "4 Models/4.1 Removing reporting delays and weekday effects/christmas_model.html",
    "href": "4 Models/4.1 Removing reporting delays and weekday effects/christmas_model.html",
    "title": "Application 2: Christmas model",
    "section": "",
    "text": "from ssm4epi.models.reporting_delays import (\n    account_for_nans,\n    _model,\n    to_log_probs,\n    n_iterations,\n    N_meis,\n    N_mle,\n    N_posterior,\n    key,\n    percentiles_of_interest,\n)\nfrom pyprojroot.here import here\nimport jax\n\njax.config.update(\"jax_enable_x64\", True)\nimport pandas as pd\nimport jax.random as jrn\nfrom jax import numpy as jnp, vmap\nfrom isssm.laplace_approximation import laplace_approximation as LA\nfrom isssm.modified_efficient_importance_sampling import (\n    modified_efficient_importance_sampling as MEIS,\n)\nfrom isssm.estimation import mle_pgssm, initial_theta\nfrom isssm.importance_sampling import (\n    pgssm_importance_sampling,\n    ess_pct,\n    mc_integration,\n    prediction_percentiles,\n    normalize_weights,\n)\nfrom isssm.kalman import state_mode\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nmpl.rcParams[\"figure.figsize\"] = (20, 6)\ni_start = 179\nnp1 = 151\ndf = pd.read_csv(here() / \"data/processed/RKI_4day_rt.csv\")\ndates = pd.to_datetime(df.iloc[i_start : i_start + np1, 0])\nchristmas_start, christmas_end = 80, 109\nchristmas_ilocs = jnp.arange(christmas_start, christmas_end + 1).astype(int)\n\ny = jnp.asarray(df.iloc[i_start : i_start + np1, 1:].to_numpy())\n\nplt.plot(dates, y)\nplt.title(f\"Incidences by delay from {dates.iloc[0]} to {dates.iloc[-1]}\")\nplt.axvspan(\n    dates.iloc[christmas_start],\n    dates.iloc[christmas_end],\n    color=\"grey\",\n    alpha=0.5,\n)\nplt.show()\ntheta_fitted_miss = pd.read_csv(\n    here() / \"data/results/4_christmas_model/thetas_miss.csv\"\n)\ntheta_fitted = pd.read_csv(here() / \"data/results/4_christmas_model/thetas.csv\")\nrefit = False\nif not refit:\n    theta0_miss = jnp.asarray(\n        theta_fitted_miss.iloc[1, 1:].to_numpy(), dtype=jnp.float64\n    )\n    theta_hat_miss = jnp.asarray(\n        theta_fitted_miss.iloc[2, 1:].to_numpy(), dtype=jnp.float64\n    )\n    theta0 = jnp.asarray(theta_fitted.iloc[1, 1:].to_numpy(), dtype=jnp.float64)\n    theta_hat = jnp.asarray(theta_fitted.iloc[2, 1:].to_numpy(), dtype=jnp.float64)",
    "crumbs": [
      "4 Models",
      "4.1 Removing reporting delays and weekday effects",
      "Application 2: Christmas model"
    ]
  },
  {
    "objectID": "4 Models/4.1 Removing reporting delays and weekday effects/christmas_model.html#missing-model",
    "href": "4 Models/4.1 Removing reporting delays and weekday effects/christmas_model.html#missing-model",
    "title": "Application 2: Christmas model",
    "section": "missing model",
    "text": "missing model\n\n# cast to float because nan is a float\ny_nans = y.astype(jnp.float64).at[christmas_ilocs].set(jnp.nan)\nmissing_inds = jnp.isnan(y_nans)\n\ntheta_manual = jnp.log(\n    # s2_log_rho, s2_W, s2_q, s2_M, s2_Wq\n    jnp.array([0.015**2, 0.024**2, 0.12**2, 0.14**2, 0.81**2])\n)\naux = (np1, 4)\n_, y_miss = account_for_nans(_model(theta_manual, aux), y_nans, missing_inds)\n_model_miss = lambda theta, aux: account_for_nans(\n    _model(theta, aux), y_nans, missing_inds\n)[0]\nif refit:\n    theta0_miss_result = initial_theta(\n        y_miss, _model_miss, theta_manual, aux, n_iterations\n    )\n    theta0_miss = theta0_miss_result.x\n    theta0_miss_result\n\n\nif refit:\n    key, subkey = jrn.split(key)\n    mle_miss_result = mle_pgssm(\n        y_miss, _model_miss, theta0_miss, aux, n_iterations, N_mle, subkey\n    )\n    theta_hat_miss = mle_miss_result.x\n    mle_miss_result\n\n\ns_manual = jnp.exp(theta_manual / 2)\ns_0 = jnp.exp(theta0_miss / 2)\ns_mle = jnp.exp(theta_hat_miss / 2)\n\nk = theta_manual.size\nplt.scatter(jnp.arange(k) - 0.2, s_manual, label=\"Manual\")\nplt.scatter(jnp.arange(k), s_0, label=\"Initial\")\nplt.scatter(jnp.arange(k) + 0.2, s_mle, label=\"MLE\")\nplt.xticks(jnp.arange(k), [\"$\\\\log \\\\rho$\", \"$W$\", \"$q$\", \"D\", \"$W_q$\"])\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nfitted_model_miss = _model_miss(theta_hat_miss, aux)\nproposal_la_miss, _ = LA(y_miss, fitted_model_miss, n_iterations)\nkey, subkey = jrn.split(key)\nproposal_meis_miss, _ = MEIS(\n    y_miss,\n    fitted_model_miss,\n    proposal_la_miss.z,\n    proposal_la_miss.Omega,\n    n_iterations,\n    N_meis,\n    subkey,\n)\nkey, subkey = jrn.split(key)\nsamples_miss, lw_miss = pgssm_importance_sampling(\n    y_miss,\n    fitted_model_miss,\n    proposal_meis_miss.z,\n    proposal_meis_miss.Omega,\n    N_posterior,\n    subkey,\n)\ness_pct(lw_miss)\n\nArray(75.17543521, dtype=float64)\n\n\n\nstate_modes_meis = vmap(state_mode, (None, 0))(fitted_model_miss, samples_miss)\nx_smooth = mc_integration(state_modes_meis, lw_miss)\nx_lower, x_mid, x_upper = prediction_percentiles(\n    state_modes_meis, normalize_weights(lw_miss), jnp.array([2.5, 50.0, 97.5]) / 100.0\n)\n\n# I_smooth = jnp.exp(x_smooth[:, 0])\nI_smooth = mc_integration(jnp.exp(state_modes_meis[:, :, 0]), lw_miss)\nrho_smooth = jnp.exp(x_smooth[:, 1])\nD_smooth = jnp.exp(x_smooth[:, 2])\nW_smooth = jnp.exp(x_smooth[:, 3])\nlog_ratios = x_smooth[:, 9:12]\nlog_probs = to_log_probs(log_ratios)\n\nweekday_log_ratios = x_smooth[:, jnp.array([12, 18, 24])]\n\nfig, axs = plt.subplots(4, 2, figsize=(15, 10))\n\naxs = axs.flatten()\nfig.tight_layout()\n\naxs[0].set_title(\"incidences\")\naxs[0].plot(dates, I_smooth, label=\"$I_t$\")\n# axs[0].plot(dates, jnp.exp(x_lower[:, 0]), color=\"black\", linestyle=\"dashed\")\naxs[0].plot(dates, y.sum(axis=1), label=\"$Y_t$\", color=\"grey\", alpha=0.5)\naxs[0].legend()\n\naxs[1].set_title(\"growth factor\")\naxs[1].plot(dates, jnp.exp(x_lower[:, 1]), color=\"grey\", alpha=0.5)\naxs[1].plot(dates, jnp.exp(x_upper[:, 1]), color=\"grey\", alpha=0.5)\naxs[1].plot(dates, rho_smooth, label=\"$\\\\log \\\\rho_t$\")\n\naxs[2].set_title(\"weekday effect\")\naxs[2].plot(dates, W_smooth, label=\"$W_t$\")\n\naxs[3].set_title(\"delay probabilities\")\n\naxs[3].plot(dates, jnp.exp(log_probs[:, 0]), label=\"$p_{t, 1}$\")\naxs[3].plot(dates, jnp.exp(log_probs[:, 1]), label=\"$p_{t, 2}$\")\naxs[3].plot(dates, jnp.exp(log_probs[:, 2]), label=\"$p_{t, 3}$\")\naxs[3].plot(dates, jnp.exp(log_probs[:, 3]), label=\"$p_{t, 4}$\")\naxs[3].plot(dates, jnp.exp(log_probs).sum(axis=1), label=\"total p\")\naxs[3].legend()\n\naxs[4].set_title(\"Log ratios\")\naxs[4].plot(dates, log_ratios[:, 0], label=\"$q_{t, 1}$\")\naxs[4].plot(dates, log_ratios[:, 1], label=\"$q_{t, 2}$\")\naxs[4].plot(dates, log_ratios[:, 2], label=\"$q_{t, 3}$\")\nfor d in dates[::7]:\n    axs[4].axvline(d, color=\"black\", alpha=0.2)\n\naxs[5].set_title(\"Dirt\")\naxs[5].plot(dates, D_smooth)\n\naxs[6].set_title(\"Weekday effect log ratios\")\naxs[6].plot(dates, weekday_log_ratios[:, 0], label=\"$W_{t, 1}$\")\naxs[6].plot(dates, weekday_log_ratios[:, 1], label=\"$W_{t, 2}$\")\naxs[6].plot(dates, weekday_log_ratios[:, 2], label=\"$W_{t, 3}$\")\naxs[6].legend()\nplt.show()",
    "crumbs": [
      "4 Models",
      "4.1 Removing reporting delays and weekday effects",
      "Application 2: Christmas model"
    ]
  },
  {
    "objectID": "4 Models/4.1 Removing reporting delays and weekday effects/christmas_model.html#non-missing-model",
    "href": "4 Models/4.1 Removing reporting delays and weekday effects/christmas_model.html#non-missing-model",
    "title": "Application 2: Christmas model",
    "section": "non-missing model",
    "text": "non-missing model\n\n# cast to float because nan is a float\ntheta_manual = jnp.log(\n    # s2_log_rho, s2_W, s2_q, s2_M, s2_Wq\n    jnp.array([0.015**2, 0.024**2, 0.12**2, 0.14**2, 0.81**2])\n)\naux = (np1, 4)\nif refit:\n    theta0_result = initial_theta(y, _model, theta_manual, aux, n_iterations)\n    theta0 = theta0_result.x\n    theta0_result\n\n\nif refit:\n    key, subkey = jrn.split(key)\n    mle_result = mle_pgssm(y, _model, theta0, aux, n_iterations, N_mle, subkey)\n    theta_hat = mle_result.x\n    mle_result\n\n\ns_manual = jnp.exp(theta_manual / 2)\ns_0 = jnp.exp(theta0 / 2)\ns_mle = jnp.exp(theta_hat / 2)\n\nk = theta_manual.size\nplt.scatter(jnp.arange(k) - 0.2, s_manual, label=\"Manual\")\nplt.scatter(jnp.arange(k), s_0, label=\"Initial\")\nplt.scatter(jnp.arange(k) + 0.2, s_mle, label=\"MLE\")\nplt.xticks(jnp.arange(k), [\"$\\\\log \\\\rho$\", \"$W$\", \"$q$\", \"D\", \"$W_q$\"])\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nfitted_model = _model(theta_hat, aux)\nproposal_la, _ = LA(y, fitted_model, n_iterations)\nkey, subkey = jrn.split(key)\nproposal_meis, _ = MEIS(\n    y, fitted_model, proposal_la.z, proposal_la.Omega, n_iterations, N_meis, subkey\n)\nkey, subkey = jrn.split(key)\nsamples, lw = pgssm_importance_sampling(\n    y, fitted_model, proposal_meis.z, proposal_meis.Omega, N_posterior, subkey\n)\ness_pct(lw)\n\nArray(52.83872881, dtype=float64)\n\n\n\nstate_modes_meis = vmap(state_mode, (None, 0))(fitted_model, samples)\nx_smooth = mc_integration(state_modes_meis, lw)\nx_lower, x_mid, x_upper = prediction_percentiles(\n    state_modes_meis, normalize_weights(lw), jnp.array([2.5, 50.0, 97.5]) / 100.0\n)\n\n# I_smooth = jnp.exp(x_smooth[:, 0])\nI_smooth = mc_integration(jnp.exp(state_modes_meis[:, :, 0]), lw)\nrho_smooth = jnp.exp(x_smooth[:, 1])\nD_smooth = jnp.exp(x_smooth[:, 2])\nW_smooth = jnp.exp(x_smooth[:, 3])\nlog_ratios = x_smooth[:, 9:12]\nlog_probs = to_log_probs(log_ratios)\n\nweekday_log_ratios = x_smooth[:, jnp.array([12, 18, 24])]\n\nfig, axs = plt.subplots(4, 2, figsize=(15, 10))\n\naxs = axs.flatten()\nfig.tight_layout()\n\naxs[0].set_title(\"incidences\")\naxs[0].plot(dates, I_smooth, label=\"$I_t$\")\n# axs[0].plot(dates, jnp.exp(x_lower[:, 0]), color=\"black\", linestyle=\"dashed\")\naxs[0].plot(dates, y.sum(axis=1), label=\"$Y_t$\", color=\"grey\", alpha=0.5)\naxs[0].legend()\n\naxs[1].set_title(\"growth factor\")\naxs[1].plot(dates, jnp.exp(x_lower[:, 1]), color=\"grey\", alpha=0.5)\naxs[1].plot(dates, jnp.exp(x_upper[:, 1]), color=\"grey\", alpha=0.5)\naxs[1].plot(dates, rho_smooth, label=\"$\\\\log \\\\rho_t$\")\n\naxs[2].set_title(\"weekday effect\")\naxs[2].plot(dates, W_smooth, label=\"$W_t$\")\n\naxs[3].set_title(\"delay probabilities\")\n\naxs[3].plot(dates, jnp.exp(log_probs[:, 0]), label=\"$p_{t, 1}$\")\naxs[3].plot(dates, jnp.exp(log_probs[:, 1]), label=\"$p_{t, 2}$\")\naxs[3].plot(dates, jnp.exp(log_probs[:, 2]), label=\"$p_{t, 3}$\")\naxs[3].plot(dates, jnp.exp(log_probs[:, 3]), label=\"$p_{t, 4}$\")\naxs[3].plot(dates, jnp.exp(log_probs).sum(axis=1), label=\"total p\")\naxs[3].legend()\n\naxs[4].set_title(\"Log ratios\")\naxs[4].plot(dates, log_ratios[:, 0], label=\"$q_{t, 1}$\")\naxs[4].plot(dates, log_ratios[:, 1], label=\"$q_{t, 2}$\")\naxs[4].plot(dates, log_ratios[:, 2], label=\"$q_{t, 3}$\")\nfor d in dates[::7]:\n    axs[4].axvline(d, color=\"black\", alpha=0.2)\n\naxs[5].set_title(\"Dirt\")\naxs[5].plot(dates, D_smooth)\n\naxs[6].set_title(\"Weekday effect log ratios\")\naxs[6].plot(dates, weekday_log_ratios[:, 0], label=\"$W_{t, 1}$\")\naxs[6].plot(dates, weekday_log_ratios[:, 1], label=\"$W_{t, 2}$\")\naxs[6].plot(dates, weekday_log_ratios[:, 2], label=\"$W_{t, 3}$\")\naxs[6].legend()\nplt.show()",
    "crumbs": [
      "4 Models",
      "4.1 Removing reporting delays and weekday effects",
      "Application 2: Christmas model"
    ]
  },
  {
    "objectID": "4 Models/4.1 Removing reporting delays and weekday effects/christmas_model.html#storing-results",
    "href": "4 Models/4.1 Removing reporting delays and weekday effects/christmas_model.html#storing-results",
    "title": "Application 2: Christmas model",
    "section": "Storing results",
    "text": "Storing results\n\n# theta\ndf_theta_miss = pd.DataFrame.from_records(\n    jnp.vstack([theta_manual, theta0_miss, theta_hat_miss]),\n    columns=[\"log rho\", \"W\", \"q\", \"M\", \"W_q\"],\n    index=[\"manual\", \"initial\", \"MLE\"],\n)\ndf_theta_miss.to_csv(\n    here() / \"data/results/4_christmas_model/thetas_miss.csv\", index_label=\"method\"\n)\ndf_theta_miss\n\n\n\n\n\n\n\n\nlog rho\nW\nq\nM\nW_q\n\n\n\n\nmanual\n-8.399410155759854\n-7.459402897268383\n-4.240527072400182\n-3.9322257127456655\n-0.421442062631305\n\n\ninitial\n-9.498473528009953\n-7.176173500070751\n-3.6539720582817656\n-6.063721127466821\n-1.95510472147738\n\n\nMLE\n-9.491112854055851\n-7.167914274896476\n-3.653098128651405\n-6.054349162377304\n-1.9545497948842407\n\n\n\n\n\n\n\n\ndf_theta = pd.DataFrame.from_records(\n    jnp.vstack([theta_manual, theta0, theta_hat]),\n    columns=[\"log rho\", \"W\", \"q\", \"M\", \"W_q\"],\n    index=[\"manual\", \"initial\", \"MLE\"],\n)\ndf_theta.to_csv(\n    here() / \"data/results/4_christmas_model/thetas.csv\", index_label=\"method\"\n)\ndf_theta\n\n\n\n\n\n\n\n\nlog rho\nW\nq\nM\nW_q\n\n\n\n\nmanual\n-8.399410155759854\n-7.459402897268383\n-4.240527072400182\n-3.9322257127456655\n-0.421442062631305\n\n\ninitial\n-8.748476043273607\n-6.915487136896888\n-1.9623090799543892\n-4.408467670632922\n-0.1990654788396088\n\n\nMLE\n-8.748555060635622\n-6.9155143502858305\n-1.9612130790596172\n-4.408284537651647\n-0.1985909535002628\n\n\n\n\n\n\n\n\nfrom isssm.importance_sampling import prediction\nfrom jaxtyping import Float, Array\n\n# predictions\n\n# date / name / mean / sd / percentiles\n\nkey, subkey_prediction = jrn.split(key)\n\n\ndef f_predict(x, s, y):\n    probs = jnp.exp(to_log_probs(x[:, 9:12]))\n    probs_signal = jnp.exp(to_log_probs(s[:, 1:]))\n    I = jnp.exp(x[:, 0:1])\n    rho = jnp.exp(x[:, 1:2])\n    M = jnp.exp(x[:, 2:3])\n    W = jnp.exp(x[:, 3:4])\n    runn_W = jnp.convolve(jnp.exp(x[:, 3]), jnp.ones(7) / 7, mode=\"same\")[:, None]\n    Wq = jnp.exp(x[:, jnp.array([12, 18, 24])])\n    y_fill_in_total = jnp.full(I.shape, y[missing_inds].sum())\n    return jnp.concatenate(\n        [I, rho, M, W, runn_W, probs, probs_signal, Wq, y_fill_in_total],\n        -1,\n    )\n\n\ndef stacked_prediction(f):\n    mean, sd, quantiles = prediction(\n        f,\n        y,\n        proposal_meis,\n        fitted_model,\n        N_posterior,\n        subkey_prediction,\n        percentiles_of_interest,\n    )\n    return jnp.vstack((mean[None], sd[None], quantiles))\n\n\ndef stacked_prediction_miss(f):\n    mean, sd, quantiles = prediction(\n        f,\n        y_miss,\n        proposal_meis_miss,\n        fitted_model_miss,\n        N_posterior,\n        subkey_prediction,\n        percentiles_of_interest,\n        fitted_model,\n    )\n    return jnp.vstack((mean[None], sd[None], quantiles))\n\n\njnp.save(\n    here() / \"data/results/4_christmas_model/predictions.npy\",\n    stacked_prediction(f_predict),\n)\njnp.save(\n    here() / \"data/results/4_christmas_model/predictions_miss.npy\",\n    stacked_prediction_miss(f_predict),\n)",
    "crumbs": [
      "4 Models",
      "4.1 Removing reporting delays and weekday effects",
      "Application 2: Christmas model"
    ]
  }
]